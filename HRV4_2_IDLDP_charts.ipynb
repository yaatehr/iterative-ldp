{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HRV4.2 IDLDP charts.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zV7Qa0IcvkOK"
      ],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZSMHAQGGetH"
      },
      "source": [
        "# Update Path and env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzfT36wQt-b0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%config IPCompleter.use_jedi = False\n",
        "import os\n",
        "ROOT_DIR = os.path.abspath('') # For colab and ipynb\n",
        "DATA_PATH = os.path.join(ROOT_DIR, 'data')\n",
        "DATA_FILE_PATH = os.path.join(DATA_PATH, 'google-search-trends.csv')\n",
        "\n",
        "#COLAB\n",
        "import sys\n",
        "sys.path.insert(0, ROOT_DIR)\n",
        "# sys.path.insert(0, \"/home/yaatehr/miniconda3/envs/idldp/lib/python3.8/site-packages/matlab\")\n",
        "# sys.path.insert(0, \"/home/yaatehr/miniconda3/condabin\")\n",
        "# sys.path.insert(0, \"/home/yaatehr/miniconda3/bin\")\n",
        "# %env PYTHONPATH={GITHUB_PATH}\n",
        "get_absolute_path = lambda x: os.path.join(ROOT_DIR,x)\n",
        "\n",
        "VERSION_NUM=\"4.2\"\n",
        "\n",
        "\n",
        "#for LOCAL Runtimes paste the following into your terminal. make sure to update the path...\n",
        "# export PYTHONPATH=$PYTHONPATH:~/programs/xprize2/covid-xprize/\n",
        "\n",
        "# Uncomment to use the example predictor\n",
        "# ! cd /content/covid-xprize/covid_xprize/examples/predictors/lstm/\n",
        "# ! mkdir models\n",
        "# ! cp /tests/fixtures/trained_model_weights_for_tests.h5 models/trained_model_weights.h5\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febL-fh0la54"
      },
      "source": [
        "# Relaxation eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci0rBGh-lFGj"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTi0bLK9tIlx"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import operator\n",
        "import collections\n",
        "\n",
        "# my_dict = [{'a':0, 'b':1, 'c':5}, {'b':3, 'c':2}, {'b':1, 'c':1}]\n",
        "# sum_key_value = dict(functools.reduce(lambda a, b: a.update(b) or a, my_dict, collections.Counter()))\n",
        "# print(my_dict)\n",
        "# print(sum_key_value)\n",
        "\n",
        "def get_top_percentile(distribution, percentile=.5):\n",
        "  index_list = []\n",
        "  i = 0\n",
        "  current_percentile = 0\n",
        "  # assert probability_normalize(distribution) == distribution,\"testing the probability norm\"\n",
        "  args = np.argsort(distribution)[::-1]\n",
        "  while current_percentile < percentile:\n",
        "    current_percentile += distribution[args[i]]\n",
        "    index_list.append(args[i])\n",
        "    i+= 1\n",
        "  return index_list\n",
        "\n",
        "def get_percentile_indices(distribution, percentiles):\n",
        "  \"\"\"\n",
        "  note percentiles are actually quantiles (0,1)\n",
        "  \"\"\"\n",
        "  percentiles = copy.copy(percentiles)\n",
        "  if not len(percentiles):\n",
        "    assert False\n",
        "\n",
        "  args = np.argsort(distribution)[::-1] #indices in ascending order\n",
        "  sorted_dist = np.sort(distribution)[::-1]\n",
        "  sorted_dist = np.cumsum(sorted_dist)\n",
        "  unique_split_points = np.unique(sorted_dist)\n",
        "  min_split, max_split = unique_split_points[0], unique_split_points[-1]\n",
        "  if percentiles[0] < min_split: # note this only fixes edge cases, cases in teh middle will still fail if some buckets have no elements.\n",
        "    print(f\"updating the min split from {percentiles[0]} to {min_split}\")\n",
        "    percentiles[0] = min_split\n",
        "  if percentiles[-1] > max_split:\n",
        "    print(f\"updating the max split from {percentiles[-1]} to {max_split}\")\n",
        "    percentiles[-1] = max_split\n",
        "  # def get_next_ind(percentile, sorted_dist):\n",
        "  #   try:\n",
        "  #     return next(x for x, val in enumerate(sorted_dist) if val > percentile)\n",
        "  #   except StopIteration e:\n",
        "  #     print(\n",
        "  #     return None\n",
        "  try:\n",
        "    split_indices = [ next(x for x, val in enumerate(sorted_dist) if val > percentile) for percentile in percentiles]\n",
        "  except:\n",
        "    # We assume we hit a 1 split point\n",
        "    try:\n",
        "      valid_dist = sorted_dist < 1\n",
        "      v = np.split(sorted_dist, np.sum(valid_dist) or 1 )\n",
        "      valid_dist, degenerate_dist = v[0], v[1]\n",
        "      split_indices = [next(x for x, val in enumerate(valid_dist) if val > percentile) for percentile in percentiles[:-1]]\n",
        "      split_indices.append(len(valid_dist))\n",
        "    except:\n",
        "      print(\"GET PERCENTILE INDICES THROWING, \")\n",
        "      print(\"percentiles stats\")\n",
        "      print(distribution)\n",
        "      print(sorted_dist)\n",
        "      print(percentiles)\n",
        "      valid_dist = sorted_dist < 1\n",
        "      print(\"splitting at: \", np.sum(valid_dist) or 1 )\n",
        "      v = np.split(sorted_dist, np.sum(valid_dist) or 1 )\n",
        "      valid_dist, degenerate_dist = v[0], v[1]\n",
        "      split_indices = [next(x for x, val in enumerate(valid_dist) if val > percentile) for percentile in percentiles[:-1]]\n",
        "      split_indices.append(len(valid_dist))\n",
        "      print(split_indices)\n",
        "  return np.split(args, split_indices), np.arange(len(distribution))[args]\n",
        "\n",
        "def get_random_element_split(distribution, num_splits, seed=0):\n",
        "  if seed:\n",
        "    np.random.seed(seed)\n",
        "  args = np.arange(len(distribution))\n",
        "  np.random.shuffle(args)\n",
        "  # print(args)\n",
        "  # print(num_splits)\n",
        "  return np.split(args, num_splits)\n",
        "\n",
        "def get_rank_order_splits(distribution, percentiles, scores):\n",
        "  _, current_score = get_percentile_indices(distribution, percentiles)\n",
        "  scores += current_score\n",
        "  args = np.argsort(scores)[::-1]\n",
        "  sorted_scores_dist = probability_normalize(np.sort(scores)[::-1])\n",
        "  sorted_scores_dist = np.cumsum(sorted_scores_dist)\n",
        "  split_indices = [next(x for x, val in enumerate(sorted_scores_dist) if val > percentile) for percentile in percentiles]\n",
        "  return np.split(args, split_indices), scores\n",
        "\n",
        "\n",
        "#fast Hadamard Transform\n",
        "def FWHT_A(k, dist):\n",
        "    if k == 1:\n",
        "        return dist\n",
        "    dist1 = dist[0 : k//2]\n",
        "    dist2 = dist[k//2 : k]\n",
        "    trans1 = FWHT_A(k//2, dist1)\n",
        "    trans2 = FWHT_A(k//2, dist2)\n",
        "    trans = np.concatenate((trans1+ trans2, trans1 - trans2))\n",
        "    return trans\n",
        "\n",
        "#simplex projection\n",
        "def project_probability_simplex(p_estimate):\n",
        "    \n",
        "    k = len(p_estimate)  # Infer the size of the alphabet.\n",
        "    p_estimate_sorted = np.sort(p_estimate)\n",
        "    p_estimate_sorted[:] = p_estimate_sorted[::-1]\n",
        "    p_sorted_cumsum = np.cumsum(p_estimate_sorted)\n",
        "    i = 1\n",
        "    while i < k:\n",
        "        if p_estimate_sorted[i] + (1.0 / (i + 1)) * (1 - p_sorted_cumsum[i]) < 0:\n",
        "            break\n",
        "        i += 1\n",
        "    lmd = (1.0 / i) * (1 - p_sorted_cumsum[i - 1])\n",
        "    return np.maximum(p_estimate + lmd, 0)\n",
        "\n",
        "#clip and normalize\n",
        "def probability_normalize(dist):\n",
        "    dist = np.maximum(dist,0) #map it to be positive\n",
        "    norm = np.sum(dist)\n",
        "    dist = np.true_divide(dist,norm) #ensure the l_1 norm is one\n",
        "    return dist\n",
        "\n",
        "#generate a random permutation matrix\n",
        "def Random_Permutation(k):\n",
        "    permute = np.random.permutation(k)\n",
        "    reverse = np.zeros(k)\n",
        "    for i in range(k):\n",
        "        reverse[int(permute[i])] = i\n",
        "    return permute,reverse\n",
        "\n",
        "#pre-calculate Hadamard Matrix\n",
        "def Hadarmard_init(k):\n",
        "    H = [None] * k\n",
        "    for row in range(k):\n",
        "        H[row] = [True] * k\n",
        "        \n",
        "# Initialize Hadamard matrix of order n.\n",
        "    i1 = 1\n",
        "    while i1 < k:\n",
        "        for i2 in range(i1):\n",
        "            for i3 in range(i1):\n",
        "                H[i2+i1][i3]    = H[i2][i3]\n",
        "                H[i2][i3+i1]    = H[i2][i3]\n",
        "                H[i2+i1][i3+i1] = not H[i2][i3]\n",
        "        i1 += i1\n",
        "    return H\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#functions to generate distributions\n",
        "def generate_geometric_distribution(k,lbd):\n",
        "    elements = range(0,k)\n",
        "    prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n",
        "    return prob\n",
        "\n",
        "def generate_uniform_distribution(k):\n",
        "    raw_distribution = [1] * k\n",
        "    sum_raw = sum(raw_distribution)\n",
        "    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n",
        "    return prob\n",
        "\n",
        "def generate_two_steps_distribution(k):\n",
        "    raw_distribution = [1] * int(k/2) + [3] * int(k/2)\n",
        "    sum_raw = sum(raw_distribution)\n",
        "    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n",
        "    return prob\n",
        "\n",
        "def generate_Zipf_distribution(k,lbd):\n",
        "    raw_distribution = [1/(float(i)**(lbd)) for i in range(1,k+1)]\n",
        "    sum_raw = sum(raw_distribution)\n",
        "    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n",
        "    return prob\n",
        "\n",
        "def generate_Dirichlet_distribution(k,lbd):  \n",
        "    raw_distribution = [0] * k\n",
        "    for i in range(0,k):\n",
        "        raw_distribution[i] = np.random.gamma(1,1)\n",
        "    sum_raw = sum(raw_distribution)\n",
        "    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n",
        "    return prob\n",
        "\n",
        "def kwarg_dummy(k, eps, rep, epochs, step_sz, init, dist, encode_acc = 1, encode_mode = 0, topk=.1, point_growth_rate=1, relaxation_mode=0, basegroup_size=-1, eps_relaxation=1, epoch_subdivisions=None):\n",
        "  return locals()\n",
        "\n",
        "def kwarg_dummy2(k=None, eps=None, epochs=None,  init=None, encode_acc = 1, encode_mode = 0,\\\n",
        "                          tiers=2, relaxation_mode=0, growth_mode=0, score_mode=0,\\\n",
        "                          eps_relaxation=1, epoch_subdivisions=None, split_percentiles=None, baseline_arg_map=None):\n",
        "  return locals()\n",
        "\n",
        "def get_max_min(df, col):\n",
        "  return df[col].max(), df[col].min()\n",
        "\n",
        "def counter_to_dist(count_table):\n",
        "  norm = sum(count_table.values())\n",
        "  dist = np.array(list(count_table.values()))/norm\n",
        "  # print(f\"verifying counts: counter: {count_table} \\n norm: {norm}\\n dist: {dist}\")\n",
        "  return dist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_distributions():\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=4)\n",
        "  prob_list = {\n",
        "      # 'Uniform' : prob1,\n",
        "      'Two_steps' : prob2,\n",
        "      'Zipf' : prob3,\n",
        "      'Dirchlet' : prob4,\n",
        "      'Geometric' : prob5, \n",
        "  }\n",
        "  i = 0\n",
        "  for indices, ax in np.ndenumerate(ax):\n",
        "    prob = list(prob_list.values())[i]\n",
        "    prob_dist_name = list(prob_list.keys())[i]\n",
        "    print(ax)\n",
        "    ax.plot(prob)\n",
        "    ax.set_title(prob_dist_name,  fontdict={'fontsize': 18, 'fontweight': 'medium'})\n",
        "    if i == 1 or i == 3:\n",
        "      ax.set_yscale(\"log\")\n",
        "    i+= 1\n",
        "  fig.set_size_inches(16.5, 2)\n",
        "\n",
        "  fig.savefig(\"true_distributions_k_1000.png\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D7zLjUNlKkZ"
      },
      "source": [
        "## Hadamard Responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCEPf2X4s4CK"
      },
      "source": [
        "# Copyright 2017 Department of Electrical and Computer Engineering, Cornell University. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "# This is a package for locally private data transmission. \n",
        "\n",
        "\n",
        "#%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# from functions import *\n",
        "\n",
        "\n",
        "#the Hadamard randamized responce when \\epsilon < 1\n",
        "class Hadamard_Rand_high_priv:\n",
        "    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n",
        "        #set encode_acc = 1 to enable fast encoding by intializing hadamard matrix\n",
        "        self.insz = absz #input alphabet size k\n",
        "        self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n",
        "        self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n",
        "        self.exp = math.exp(pri_para) #privacy parameter\n",
        "        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n",
        "        #self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n",
        "        \n",
        "        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n",
        "        if encode_acc == 1:\n",
        "            self.H = Hadarmard_init(self.outsz) # initialize Hadarmard matrix\n",
        "            \n",
        "    def parity_check(self,x,y): #check if the hadamard entry is one (return 0 for if the entry is 1)\n",
        "        z = x&y #bit and\n",
        "        z_bit = bin(z)[2:].zfill(self.outbit)\n",
        "        check = 0\n",
        "        for i in range(0,self.outbit): #check = \\sum_i (x_i y_i) （mod 2）\n",
        "            check = check^int(z_bit[i]) \n",
        "        return check\n",
        "                                  \n",
        "    def encode_symbol(self,in_symbol):  # encode a single symbol into a privatized version\n",
        "        bitin = bin(int(in_symbol)+1)[2:].zfill(self.outbit) #map the input symbol \n",
        "        # x to x+1 since we are not using the first column of the matrix\n",
        "        out1 = random.randint(0,math.pow(2,self.outbit)-1) #get a random number in\n",
        "        # the output alphabet as one potential output\n",
        "        bitout1 = bin(out1)[2:].zfill(self.outbit)\n",
        "        for i in range(0,self.outbit): #flip the bit of out1 corresponding to the left\n",
        "            #  most bit in (in_symbol+1) which is one to get the other potential output\n",
        "            if int(bitin[i]) == 1:\n",
        "                out2 = out1 ^ (pow(2,self.outbit - i -1))\n",
        "                break   \n",
        "        #bitout2 = bin(out2)[2:].zfill(self.outbit)\n",
        "        \n",
        "        if self.ifencode_acc == 1:\n",
        "            check = 1 - self.H[int(in_symbol)+1][out1]\n",
        "        else:\n",
        "            check = 0\n",
        "            for i in range(0,self.outbit): # check if the Hadamard entry at position \n",
        "                # (in_symbol+1, out1) is one or not\n",
        "                check = check^(int(bitout1[i])&int(bitin[i]))\n",
        "\n",
        "        ra = random.random()\n",
        "        if check == 0: # if it is one output out1 with prob (1-pri_para)\n",
        "            if ra > self.pri_para:\n",
        "                return out1\n",
        "            else:\n",
        "                return out2\n",
        "        else: # else output out2 with prob (1-pri_para)\n",
        "            if ra > self.pri_para:\n",
        "                return out2\n",
        "            else:\n",
        "                return out1       \n",
        "     \n",
        "    def encode_string(self,in_list):  # encode string into a privatized string\n",
        "        out_list = [self.encode_symbol(x) for x in in_list] # encode each symbol in the string\n",
        "        return out_list\n",
        "    \n",
        "    def decode_string(self, out_list, iffast = 1, normalization = 0, verbose=False): # get the... \n",
        "        # privatized string and learn the original distribution\n",
        "        #normalization options: 0: clip and normalize(default)\n",
        "        #                       1: simplex projection\n",
        "        #                       else: no nomalization\n",
        "        \n",
        "        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n",
        "        #        1 no fast hadamard tansform  time O(n + k^2)\n",
        "        \n",
        "        l = len(out_list) \n",
        "        count,edges = np.histogram(out_list,range(self.outsz+1))\n",
        "        dist = count/float(l)\n",
        "        \n",
        "        if iffast == 1: #if we use fast hadamard transform\n",
        "            dist_mid = FWHT_A(self.outsz, dist) #do fast hadamard transform... \n",
        "            # to the frequency vector\n",
        "            dist_S = (dist_mid[1:self.insz+1] + 1)/float(2) #get the frequencies of C_i\n",
        "        else: #if we don't use fast hadamard transform\n",
        "            num = [0]*self.insz\n",
        "            for x in range(0,self.outsz):\n",
        "            #print x\n",
        "                for i in range(1, self.insz+1): #if output x is in C_i(locations in...\n",
        "                    #  row i which is 1), add it to the count of C_i\n",
        "                    if self.parity_check(i,x) == 0:\n",
        "                        num[i-1] = num[i-1] + count[x]\n",
        "            dist_S = np.array([float(x)/float(l) for x in num]) #get the frequencies of C_i\n",
        "        if verbose: \n",
        "            print(\"decoded frequency of C_i before normalizations: \", dist_S)    \n",
        "        \n",
        "        dist = (2*dist_S*(1+self.exp)-(1+self.exp))/float(self.exp-1) #calculate the...\n",
        "        # corresponding estimate for p_i\n",
        "        if verbose:\n",
        "            print(\"decoded p_i estimate before normalizations: \", dist)    \n",
        "\n",
        "        if normalization == 0: \n",
        "            dist = probability_normalize(dist) #clip and normalize\n",
        "        if normalization == 1:\n",
        "            dist = project_probability_simplex(dist) #simplex projection\n",
        "        \n",
        "        return dist\n",
        "\n",
        "\n",
        "#The Hadamard randomized response for all regimes (Modified Version)\n",
        "\n",
        "class Hadamard_Rand_general:\n",
        "    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n",
        "        self.insz = absz #input alphabet size k\n",
        "        #self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n",
        "        #self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n",
        "        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n",
        "        self.exp = math.exp(pri_para) #privacy parameter\n",
        "        #self.initbit = int(math.floor(math.log(self.exp,2))) # number of bits indicating the block number \n",
        "        self.initbit = int(math.floor(math.log(min(2*absz,self.exp),2))) # number of bits indicating the block number\n",
        "        self.part = int(math.pow(2,self.initbit)) #total number of blocks B\n",
        "        self.tailbit = int(math.ceil(math.log(float(self.insz)/float(self.part)+1,2))) #number of bits indicating the location within a block\n",
        "        self.partsz = int(math.pow(2,self.tailbit)) # size of each block b\n",
        "        self.num_one = int(self.partsz/float(2))\n",
        "        self.outbit = self.tailbit + self.initbit #total number of output bits\n",
        "        self.partnum = int(math.ceil(float(self.insz)/float(self.partsz - 1)))\n",
        "        self.outsz = int(self.partsz*self.partnum) # output alphabet size K\n",
        "        self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n",
        "        \n",
        "        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n",
        "        if encode_acc == 1:\n",
        "            self.H = Hadarmard_init(self.partsz) # initialize Hadarmard matrix\n",
        "        \n",
        "    def entry_check(self,x,y): #check if the reduced hadamard entry is one (return 0 for 1)\n",
        "        x_bit = bin(x)[2:].zfill(self.outbit)\n",
        "        y_bit = bin(y)[2:].zfill(self.outbit)\n",
        "        for i in range(0,self.initbit): # check if they are in the same block, if not, return -1\n",
        "            if x_bit[i] != y_bit[i]:\n",
        "                return True\n",
        "        check = False\n",
        "        for i in range(self.initbit, self.outbit): #check whether the entry is one within the block\n",
        "            check = check^(int(x_bit[i]) & int(y_bit[i]))\n",
        "        return check\n",
        "                                  \n",
        "            \n",
        "    def encode_symbol_rough(self,in_symbol):  # encode a single symbol into a privatized version \n",
        "        # we use coupling argument to do this\n",
        "        part_index = int(in_symbol)//(self.partsz-1)\n",
        "        part_pos = int(in_symbol)%(self.partsz-1)+1\n",
        "        in_column = (part_index << self.tailbit) + part_pos #map the input x to the xth column with weight b/2\n",
        "        #in_column = part_index * self.partsz + part_pos\n",
        "        out1 = np.random.randint(0,self.outsz) #get a random number out1 in the output alphabet as a potential output\n",
        "        ra = random.random()\n",
        "        if ra < (2*self.part)/(2*self.part-1+self.exp): #with certain prob, output the same symbol as from uniform distribution\n",
        "            return out1\n",
        "        else:\n",
        "            out_pos = out1 & (self.partsz - 1)\n",
        "            #out_pos = out1 % self.partsz\n",
        "            out1 =  out_pos + (part_index << self.tailbit) # map out1 to the same block as in_column while maintain the location within the block\n",
        "            #out1 = out_pos + part_index*self.partsz\n",
        "            if self.ifencode_acc == 1:\n",
        "                check = self.H[part_pos][out_pos]\n",
        "            else:\n",
        "                check = 1 - self.entry_check(in_column,out1)\n",
        "\n",
        "            if check == 0: #if location (in_column, out1) is one, output out1\n",
        "                return out1\n",
        "            else: #else flip bit at the left most location where bit representation of in_column is one \n",
        "                #bitin = bin(int(in_column))[2:].zfill(self.outbit)\n",
        "                check = 1\n",
        "                for i in range(self.outbit - self.initbit): \n",
        "                    if in_column%2 == 1:\n",
        "                        #out1 = out1 ^ (pow(2,self.outbit - i -1))\n",
        "                        out1 = out1 ^ check\n",
        "                        break\n",
        "                    in_column = in_column >> 1\n",
        "                    check = check << 1\n",
        "                return out1\n",
        "            \n",
        "    #delete the first row of each block\n",
        "    def encode_symbol(self, in_symbol):\n",
        "        while(1):\n",
        "            out = self.encode_symbol_rough(in_symbol)\n",
        "            if out%self.partsz != 0:\n",
        "                return out\n",
        "    \n",
        "    def encode_string(self,in_list):  # encode string into a privatized string\n",
        "        out_list = [self.encode_symbol(self.permute[x]) for x in in_list]\n",
        "        return out_list\n",
        "    \n",
        "    \n",
        "    def decode_string(self, out_list, iffast = 1, normalization = 0): # get the privatized string and learn the original distribution\n",
        "        #normalization options: 0: clip and normalize(default)\n",
        "        #                       1: simplex projection\n",
        "        #                       else: no nomalization\n",
        "        \n",
        "        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n",
        "        #        1 no fast hadamard tansform  time O(n + k^2)\n",
        "        \n",
        "        l = len(out_list)\n",
        "        count,edges = np.histogram(out_list,range(self.outsz+1))\n",
        "        freq = count/float(l)\n",
        "        \n",
        "        \n",
        "        if iffast == 1:\n",
        "            #parts = self.insz//(self.partsz-1) \n",
        "            freq_S = np.zeros(self.outsz)\n",
        "            freq_block = np.zeros(self.partnum)\n",
        "            for i in range(0, self.partnum):\n",
        "                Trans = FWHT_A(self.partsz, freq[i*self.partsz: (i+1)*self.partsz])\n",
        "                freq_block[i] = Trans[0]\n",
        "                freq_S[i*(self.partsz-1): (i+1)*(self.partsz-1)] = ( - Trans[1:self.partsz] + Trans[0])/float(2)         \n",
        "            dist_S = freq_S[0:self.insz]\n",
        "            \n",
        "        else:\n",
        "            freq_block = np.zeros(self.part) # count the number of appearances of each block\n",
        "            for i in range(0,self.part): \n",
        "                #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n",
        "                for j in range(0,self.partsz):\n",
        "                    freq_block[i] = freq_block[i] + freq[i*self.partsz+j]\n",
        "            #freq_block = np.true_divide(count_block,l) # calculate the frequency of each block\n",
        "            #dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block                    \n",
        "            for i in range(0, self.insz): \n",
        "                pi = int(i)//(self.partsz-1)\n",
        "                ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n",
        "                for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n",
        "                    if self.entry_check(ti,x) == 0:\n",
        "                        dist_S[i] = dist_S[i] + freq[x]\n",
        "                        \n",
        "        lbd = float(self.outsz - self.partnum)/float(self.num_one)\n",
        "        c1 = lbd-1+self.exp\n",
        "        \n",
        "        dist_block = np.true_divide(c1*(freq_block)- 2 + 1/float(self.num_one),self.exp-1) # calculate the estimated original prob of each block\n",
        "        \n",
        "        c2 = self.exp - 1\n",
        "        #dist = [float(2*c1*dist_S[i] - c2*dist_block[i//(self.partsz-1)] - 2)/float(c3) for i in range(0,self.insz) ]\n",
        "        dist = [float(2*c1*dist_S[i] - c2*dist_block[i//(self.partsz-1)] - 2)/float(c2) for i in range(0,self.insz) ]\n",
        "        \n",
        "        if normalization == 0: \n",
        "            dist = probability_normalize(dist) #clip and normalize\n",
        "        if normalization == 1:\n",
        "            dist = project_probability_simplex(dist) #simplex projection\n",
        "        \n",
        "        #reverse the permuation\n",
        "        dist1 = np.zeros(self.insz)\n",
        "        for i in range(self.insz):\n",
        "            dist1[int(self.reverse[i])] = dist[i]\n",
        "        return dist1\n",
        "    \n",
        "    \n",
        "    def decode_string_old(self, out_list): # get the privatized string and learn the original distribution\n",
        "        \n",
        "        l = len(out_list)\n",
        "        dist_S = np.zeros(self.insz)\n",
        "        count,edges = np.histogram(out_list,range(self.outsz+1))\n",
        "        freq = count/float(l)\n",
        "        \n",
        "        freq_block = np.zeros(self.part) # count the number of appearances of each block\n",
        "        for i in range(0,self.part): \n",
        "            #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n",
        "            for j in range(0,self.partsz):\n",
        "                freq_block[i] = freq_block[i] + freq[i*self.partsz+j]\n",
        "        \n",
        "        \n",
        "        #freq_block = np.true_divide(count_block,l) # calculate the frequency of each block\n",
        "        dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n",
        "                    \n",
        "        for i in range(0, self.insz): \n",
        "            pi = int(i)//(self.partsz-1)\n",
        "            ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n",
        "            for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n",
        "                if self.entry_check(ti,x) == 0:\n",
        "                    dist_S[i] = dist_S[i] + freq[x]\n",
        "\n",
        "        #dist_S = np.zeros(self.insz)\n",
        "        #dist_S = np.true_divide(num,l) #calculate the frequency of each C_i\n",
        "        dist_inter = np.true_divide(2*(dist_S*(2*self.part-1+self.exp)-1),self.exp-1) # calculate intermediate prob\n",
        "        dist = [dist_inter[i] - dist_block[i//(self.partsz-1)] for i in range(0,self.insz)] # calculate the estimated prob for each symbol\n",
        "        dist = np.maximum(dist,0) #map it to be positive\n",
        "        norm = np.sum(dist)\n",
        "        dist = np.true_divide(dist,norm) #ensure the l_1 norm is one\n",
        "        return dist\n",
        "    \n",
        "    #def decode_string_normalize(self, out_list): #normalized outputs using clip and normalize\n",
        "    #    dist = self.decode_string_permute(out_list)\n",
        "    #    dist = probability_normalize(dist)\n",
        "    #    return dist\n",
        "    \n",
        "    #def decode_string_project(self, out_list): #projected outputs\n",
        "    #    dist = self.decode_string_permute(out_list)\n",
        "    #    dist = project_probability_simplex(dist)\n",
        "    #    return dist\n",
        "    \n",
        "    #def decode_string_permute(self, out_list): # get the privatized string and learn the original distribution\n",
        "    #    dist1 = self.decode_string_fast(out_list)\n",
        "    #    dist = np.zeros(self.insz)\n",
        "    #    for i in range(self.insz):\n",
        "    #        dist[int(self.reverse[i])] = dist1[i]\n",
        "    #    return dist\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#The Hadamard randomized response for all regimes (original version)\n",
        "class Hadamard_Rand_general_original:\n",
        "    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n",
        "        #set encode_acc = 1 to enable fast encoding by intializing hadamard matrix\n",
        "        \n",
        "        self.insz = absz #input alphabet size k\n",
        "        #self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n",
        "        #self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n",
        "        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n",
        "        self.exp = math.exp(pri_para) #privacy parameter\n",
        "        #self.initbit = int(math.floor(math.log(self.exp,2))) # number of bits indicating the block number \n",
        "        self.initbit = int(math.floor(math.log(min(2*absz,self.exp),2))) # number of bits indicating the block number\n",
        "        self.part = int(math.pow(2,self.initbit)) #number of blocks B\n",
        "        self.tailbit = int(math.ceil(math.log(float(self.insz)/float(self.part)+1,2))) #number of bits indicating the location within a block\n",
        "        self.partsz = int(math.pow(2,self.tailbit)) # size of each block b\n",
        "        self.outbit = self.tailbit + self.initbit #total number of output bits\n",
        "        self.outsz = int(math.pow(2,self.outbit)) # output alphabet size K\n",
        "        self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n",
        "        \n",
        "        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n",
        "        if encode_acc == 1:\n",
        "            self.H = Hadarmard_init(self.partsz) # initialize Hadarmard matrix\n",
        "        \n",
        "        \n",
        "    def entry_check(self,x,y): #check if the reduced hadamard entry is one (return 0 for 1)\n",
        "        x_bit = bin(x)[2:].zfill(self.outbit)\n",
        "        y_bit = bin(y)[2:].zfill(self.outbit)\n",
        "        for i in range(0,self.initbit): # check if they are in the same block, if not, return -1\n",
        "            if x_bit[i] != y_bit[i]:\n",
        "                return True\n",
        "        check = False\n",
        "        for i in range(self.initbit, self.outbit): #check whether the entry is one within the block\n",
        "            check = check^(int(x_bit[i]) & int(y_bit[i]))\n",
        "        return check\n",
        "                                  \n",
        "    \n",
        "    def encode_symbol(self,in_symbol):  # encode a single symbol into a privatized version \n",
        "        # we use coupling argument to do this\n",
        "        part_index = int(in_symbol)//(self.partsz-1)\n",
        "        part_pos = int(in_symbol)%(self.partsz-1)+1\n",
        "        in_column = (part_index << self.tailbit) + part_pos #map the input x to the xth column with weight b/2\n",
        "        #in_column = part_index * self.partsz + part_pos\n",
        "        out1 = np.random.randint(0,self.outsz) #get a random number out1 in the output alphabet as a potential output\n",
        "        ra = random.random()\n",
        "        if ra < (2*self.part)/(2*self.part-1+self.exp): #with certain prob, output the same symbol as from uniform distribution\n",
        "            return out1\n",
        "        else:\n",
        "            out_pos = out1 & (self.partsz - 1)\n",
        "            #out_pos = out1 % self.partsz\n",
        "            out1 =  out_pos + (part_index << self.tailbit) # map out1 to the same block as in_column while maintain the location within the block\n",
        "            #out1 = out_pos + part_index*self.partsz\n",
        "            if self.ifencode_acc == 1:\n",
        "                check = 1 - self.H[part_pos][out_pos]\n",
        "            else:\n",
        "                check = self.entry_check(in_column,out1)\n",
        "\n",
        "            if check == 0: #if location (in_column, out1) is one, output out1\n",
        "                return out1\n",
        "            else: #else flip bit at the left most location where bit representation of in_column is one \n",
        "                #bitin = bin(int(in_column))[2:].zfill(self.outbit)\n",
        "                check = 1\n",
        "                for i in range(self.outbit - self.initbit): \n",
        "                    if in_column%2 == 1:\n",
        "                        #out1 = out1 ^ (pow(2,self.outbit - i -1))\n",
        "                        out1 = out1 ^ check\n",
        "                        break\n",
        "                    in_column = in_column >> 1\n",
        "                    check = check << 1\n",
        "                return out1\n",
        "    \n",
        "    def encode_string(self,in_list): #permute before encoding\n",
        "        out_list = [self.encode_symbol(self.permute[x]) for x in in_list]\n",
        "        return out_list        \n",
        "    \n",
        "    \n",
        "    def decode_string(self, out_list,iffast = 1, normalization = 0): # get the privatized string and learn the original distribution\n",
        "        #normalization options: 0: clip and normalize(default)\n",
        "        #                       1: simplex projection\n",
        "        #                       else: no nomalization\n",
        "        \n",
        "        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n",
        "        #        1 no fast hadamard tansform  time O(n + kB), B is the block size\n",
        "        \n",
        "        l = len(out_list)\n",
        "        count,edges = np.histogram(out_list,range(self.outsz+1))\n",
        "        freq = count/float(l)\n",
        "        \n",
        "        if iffast == 1:\n",
        "            parts = self.insz//(self.partsz-1) \n",
        "            freq_S = np.zeros((parts+1)*self.partsz)\n",
        "            freq_block = np.zeros((parts+1)*self.partsz)\n",
        "        \n",
        "            for i in range(0, parts+1):\n",
        "                Trans = FWHT_A(self.partsz, freq[i*self.partsz: (i+1)*self.partsz])\n",
        "                freq_block[i] = Trans[0]\n",
        "                freq_S[i*(self.partsz-1): (i+1)*(self.partsz-1)] = (Trans[1:self.partsz] + Trans[0])/float(2) \n",
        "            dist_S = freq_S[0:self.insz]\n",
        "        \n",
        "            dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n",
        "        \n",
        "        else:\n",
        "            freq_block = np.zeros(self.part) # count the number of appearances of each block\n",
        "            for i in range(0,self.part): \n",
        "                #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n",
        "                for j in range(0,self.partsz):\n",
        "                    freq_block[i] = freq_block[i] + freq[i*self.partsz+j]     \n",
        "                    \n",
        "            dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n",
        "            for i in range(0, self.insz): \n",
        "                pi = int(i)//(self.partsz-1)\n",
        "                ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n",
        "                for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n",
        "                    if self.entry_check(ti,x) == 0:\n",
        "                        dist_S[i] = dist_S[i] + freq[x]\n",
        "        \n",
        "        dist_inter = np.true_divide(2*(dist_S*(2*self.part-1+self.exp)-1),self.exp-1) # calculate intermediate prob\n",
        "        dist = [dist_inter[i] - dist_block[i//(self.partsz-1)] for i in range(0,self.insz)] # calculate the estimated prob for each symbol\n",
        "        \n",
        "        if normalization == 0: \n",
        "            dist = probability_normalize(dist) #clip and normalize\n",
        "        if normalization == 1:\n",
        "            dist = project_probability_simplex(dist) #simplex projection\n",
        "        \n",
        "        #reverse the permuation\n",
        "        dist1 = np.zeros(self.insz)\n",
        "        for i in range(self.insz):\n",
        "            dist1[int(self.reverse[i])] = dist[i]\n",
        "        return dist1\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0iguqDKa-M6"
      },
      "source": [
        "## Ground Truth Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkuCW2e137nq"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbzpokFARvwR"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "softmax = lambda x : np.exp(x)/sum(np.exp(x))\n",
        "\n",
        "DEFAULT_EPOCH_GROWTH_RATE = 0\n",
        "DEFAULT_SUBDIVISION_GROUTH_RATE=0\n",
        "\n",
        "def ground_truth_comp(df, d2, col):\n",
        "  \"\"\"\n",
        "  take in the decoded df, ground truth df, and target col\n",
        "  return dict of key: pred - ground truth\n",
        "  \"\"\"\n",
        "  # df = pd.DataFrame({\"a\": [1, 1, 3, 4], \"b\": [5,6,7,7]})\n",
        "  # d2 = pd.DataFrame({\"a\": [1, 1, 3, 4, 5, 6, 3], \"b\": [5,6,7,7,0,0,0]})\n",
        "  df_val_counts = df[col].value_counts(sort=False)\n",
        "  # ground_truth_counts = ground_truth_hist[col]\n",
        "  ground_truth_counts = d2[col].value_counts(sort=False)\n",
        "  eval_map = defaultdict(lambda : 0, zip( df_val_counts.keys(), df_val_counts.values))\n",
        "  ground_truth_val, ground_truth_key = ground_truth_counts.values, ground_truth_counts.keys()\n",
        "  # print(df[\"a\"].value_counts(sort=False).keys())\n",
        "  comparison_map = defaultdict(lambda: 0, zip(ground_truth_key, ground_truth_val))\n",
        "  differences = {k: (eval_map[k] or 0) - x for k, x in  comparison_map.items()}\n",
        "  # print(differences)\n",
        "  return differences\n",
        "\n",
        "def get_growth_rate(df, subdivisions, mode=0):\n",
        "  if mode==0: #uniform\n",
        "    return  [int(len(df) * i/(subdivisions)) for i in range(1, subdivisions)]\n",
        "  elif mode == 1: #logrhymic?? tf is this? sketchy exponential\n",
        "    return  (len(df) *np.power(2, [ i/epoch_subdivisions + 1 for i in range(1, epoch_subdivisions)])).astype(int)\n",
        "  elif mode == 2:\n",
        "    return  (len(df) *np.cumsum(softmax(np.power(2, [ i for i in range(1, epoch_subdivisions)])))).astype(int)\n",
        "\n",
        "  else:\n",
        "    assert False, \"growth mode not implemented\"\n",
        "\n",
        "# def get_epoch_df(df, epochs, epoch): #TODO Fix this it goes backwards and ends on full starts on full\n",
        "#   return df[:int(len(df)*(epoch)/epochs)] if epoch < epochs else df\n",
        "def generate_percent_err(baseline_err, input_err):\n",
        "    if np.isnan(( 1 - (input_err/baseline_err[:len(input_err)])) * 100).any():\n",
        "        print(f\"baseline_err: {baseline_err}\")\n",
        "        print(f\"input_err: {input_err}\")\n",
        "        print( ( 1 - (input_err/baseline_err[:len(input_err)])) * 100)\n",
        "    return (( 1 - (input_err/baseline_err[:len(input_err)])) * 100).flatten()\n",
        "\n",
        "\n",
        "def gen_side_by_side_boxplots(scatter_results, indices, scatter_titles, figure_title):\n",
        "  \"\"\"\n",
        "  scatter_results: \n",
        "  indices: the x axis labels (number of items in the given test\n",
        "  scatter_titles: raw (l1) err, raw baseline err, raw l2 err, raw l2 err (sahould we use RMSE instead ofg the \n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots(nrows=len(scatter_results)//2, ncols=2)\n",
        "  fig.suptitle(figure_title, fontsize=16)\n",
        "  fig.set_size_inches(10, 20)\n",
        "  ax = np.ravel(ax, order=\"F\")\n",
        "\n",
        "\n",
        "  upper_y_limit = 0\n",
        "  lower_y_limit = float(\"inf\")\n",
        "  limits  = [(lower_y_limit, upper_y_limit)]*3\n",
        "  for i, data in enumerate(scatter_results):\n",
        "    lower_y_limit, upper_y_limit = limits[i %3]\n",
        "    # bp = ax[i].boxplot(data) \n",
        "    # print(data)\n",
        "    # print(f\"this is {i} for data ^\")\n",
        "    max_y = np.max(data)\n",
        "    min_y = np.min(data)\n",
        "    upper_y_limit = max_y if max_y >= upper_y_limit else upper_y_limit\n",
        "    lower_y_limit = min_y if min_y <= lower_y_limit else lower_y_limit\n",
        "    ax[i].boxplot(data) \n",
        "    # ax[i].set_title(scatter_titles[i]) # this many out of total number of points\n",
        "    ax[i].set_xlabel(\"Number of Samples\")\n",
        "    ax[i].set_ylabel(f\"Raw {scatter_titles[i]}\")\n",
        "    # ax[i].set_ylim((-50, 50))\n",
        "    ax[i].set_xticklabels(indices)\n",
        "    limits[i % 3]= (lower_y_limit, upper_y_limit)\n",
        "\n",
        "  slack = [(upper_y_limit - lower_y_limit)*.05 for lower_y_limit, upper_y_limit in limits]\n",
        "  limits = [(lower_y_limit - slack[i], upper_y_limit + slack[i]) for i, (lower_y_limit, upper_y_limit) in enumerate(limits)]\n",
        "\n",
        "  for i, a in enumerate(ax): # set each compared axis to be the same scale\n",
        "    a.set_ylim(limits[i% 3])\n",
        "    a.yaxis.grid(True, which='both')\n",
        "\n",
        "  ax = np.reshape(ax, (len(scatter_results)//2, 2))\n",
        "  fig.show()\n",
        "  plt.pause(0.1)\n",
        "\n",
        "\n",
        "def gen_aggregate_column_boxplot(input_metrics, baseline_metric_list, indices, baseline_names, interest_cols, figure_title):\n",
        "  \"\"\"\n",
        "  input_metrics: list of metrics of shape (n_iterations, len(indices)) that should match the order of interest cols (and be same length)\n",
        "  baseline_metric_list: list of list metrics of same format as input metrics, one for each baseline\n",
        "  indices: the x axis labels (number of items in the given test\n",
        "  baseline_names: list of strings for baseline names\n",
        "  interest_cols: list of metric type names (ie raw (l1) err, l2 err, rmse)\n",
        "  figure title:  string listing experiment conditions nad params. \n",
        "  \"\"\"\n",
        "\n",
        "  colors = ['lightblue', 'lightgreen', 'tan', 'pink', 'cyan']\n",
        "\n",
        "  r = len(input_metrics)\n",
        "  n = 1 + len(baseline_metric_list)\n",
        "  fig, ax = plt.subplots(nrows=r)\n",
        "  fig.suptitle(figure_title, fontsize=16)\n",
        "  fig.set_size_inches(10, 20)\n",
        "\n",
        "  upper_y_limit = 0\n",
        "  lower_y_limit = float(\"inf\")\n",
        "  limits  = [(lower_y_limit, upper_y_limit)]*r\n",
        "  all_bps = []\n",
        "  widths = 1/(n+1)\n",
        "  for i, data in enumerate([input_metrics, *baseline_metric_list]):\n",
        "\n",
        "    # print(data)\n",
        "    # print(data.shape)\n",
        "    # print(indices.shape)\n",
        "    bps, new_lim = gen_column_boxplot(data,indices, interest_cols, ax, np.arange(len(indices)) + i/n, limits, widths, colors[i])\n",
        "    all_bps.extend(bps)\n",
        "    limits = new_lim\n",
        "\n",
        "  slack = [(upper_y_limit - lower_y_limit)*.05 for lower_y_limit, upper_y_limit in limits]\n",
        "  limits = [(lower_y_limit - slack[i], upper_y_limit + slack[i]) for i, (lower_y_limit, upper_y_limit) in enumerate(limits)]\n",
        "\n",
        "  def slice_per(source, step):\n",
        "      return [source[i::step] for i in range(step)]\n",
        "  bp_ax_list = slice_per(all_bps, r)\n",
        "  # print(\"LIMITS: \")\n",
        "  # print(limits)\n",
        "  for i, a in enumerate(ax): # set each compared axis to be the same scale\n",
        "    a.set_ylim(limits[i% r])\n",
        "    a.yaxis.grid(True, which='both')\n",
        "    a.legend([bp[\"boxes\"][0] for bp in bp_ax_list[i]], [\"experimental\"] + baseline_names, loc='upper right')\n",
        "    a.set_xlabel(\"Number of Samples\")\n",
        "    a.set_ylabel(f\"Raw {interest_cols[i]}\")\n",
        "    a.set_xticks(np.arange(len(indices)) + .3)\n",
        "    a.set_xticklabels(indices)\n",
        "    \n",
        "\n",
        "  fig.show()\n",
        "  plt.pause(0.1)\n",
        "\n",
        "\n",
        "\n",
        "def gen_column_boxplot(metrics, indices, interest_cols, ax, right_shift, limits, width, color):\n",
        "  \"\"\"\n",
        "  metrics: list of metrics of shape (n_iterations, len(indices)) that should match the order of interest cols (and be same length)\n",
        "  indices: the x axis labels (number of items in the given test\n",
        "  interest_cols: list of metric type names (ie raw (l1) err, l2 err, rmse)\n",
        "  ax: a list of axis to plot on 9one for each interest col\n",
        "  right_shift: to prevent boxx and whiskers from overlapping.\n",
        "  \"\"\"\n",
        "\n",
        "  bps = []\n",
        "  new_lims = []\n",
        "  for i, data in enumerate(metrics):\n",
        "    # print(data.shape)\n",
        "    lower_y_limit, upper_y_limit = limits[i]\n",
        "    max_y = np.max(data)\n",
        "    min_y = np.min(data)\n",
        "    upper_y_limit = max_y if max_y >= upper_y_limit else upper_y_limit\n",
        "    lower_y_limit = min_y if min_y <= lower_y_limit else lower_y_limit\n",
        "    new_lims.append((lower_y_limit, upper_y_limit))\n",
        "\n",
        "    bp = ax[i].boxplot(data, positions = right_shift, notch=True, patch_artist=True, \n",
        "                  boxprops=dict(\n",
        "                      color=color,\n",
        "                      facecolor=color\n",
        "                  ),\n",
        "                  widths = width\n",
        "                  )\n",
        "    bps.append(bp)\n",
        "    medians = np.median(data, axis=0)\n",
        "    ax[i].plot(right_shift, medians, color=color)\n",
        "\n",
        "\n",
        "  return bps, new_lims\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_plot_for_metrics(inputs, baselines, indices, plot_labels, figure_title=\"temp plot\", filename=\"tempplt.png\", save_images=True, output_counts=False, fig=None, ax=None):\n",
        "\n",
        "    if fig == None:\n",
        "      fig, ax = plt.subplots(nrows=len(inputs), ncols=1) \n",
        "    \n",
        "    fig.set_size_inches(8.5, 10)\n",
        "    fig.suptitle(figure_title, fontsize=16)\n",
        "    for i, metric in enumerate(inputs):\n",
        "        baseline_metric = baselines[i]\n",
        "        percent_errors = generate_percent_err(baseline_metric, metric)\n",
        "        \n",
        "        print(\"v\"*50)\n",
        "        print(percent_errors)\n",
        "        # print(step_sz)\n",
        "        widths = np.insert(indices[:-1],0,0)\n",
        "        widths = indices - widths\n",
        "        print(indices)\n",
        "        try:\n",
        "          indices = indices.to_list()\n",
        "        except:\n",
        "          pass\n",
        "        print(len(indices))\n",
        "        print(len(percent_errors))\n",
        "        print(\"^\"*30)\n",
        "        barlist = ax[i].bar(indices, percent_errors, width=widths*0.75) # this many out of total number of points\n",
        "        for j, percent in enumerate(percent_errors):\n",
        "          if percent < 0:\n",
        "            barlist[j].set_color('r')\n",
        "          else:\n",
        "            barlist[j].set_color('g')\n",
        "        ax[i].set_title(plot_labels[i]) # this many out of total number of points\n",
        "        ax[i].set_xlabel(\"Number of Samples\")\n",
        "        ax[i].set_ylabel(\"Avg Percent Improvement (0 is baseline perf)\")\n",
        "        ax[i].set_ylim((-50, 50))\n",
        "        ax[i].set_xticks(indices)\n",
        "        rects = ax[i].patches\n",
        "        print(metric - baseline_metric)\n",
        "        bar_labels = [f\"{float(i):.1f}\" for i in (abs(metric - baseline_metric).flatten() if output_counts else percent_errors)]\n",
        "        for rect, label in zip(rects, bar_labels):\n",
        "            height = rect.get_height()\n",
        "            x_value = rect.get_x() + rect.get_width() / 2\n",
        "\n",
        "            # Number of points between bar and label. Change to your liking.\n",
        "            space = 5\n",
        "            # Vertical alignment for positive values\n",
        "            va = 'bottom'\n",
        "\n",
        "            # If value of bar is negative: Place label below bar\n",
        "            if height < 0:\n",
        "                # Invert space to place label below\n",
        "                space *= -1\n",
        "                # Vertically align label at top\n",
        "                va = 'top'\n",
        "            ax[i].annotate(label, (x_value, height), xytext=(0, space), textcoords=\"offset points\", ha='center', va=va)\n",
        "        # ax[i].legend()\n",
        "    fig.show()\n",
        "    if save_images:\n",
        "        plt.savefig(filename+ \".png\")\n",
        "    plt.pause(0.1)\n",
        "\n",
        "\n",
        "def generate_plot_for_baselines(inputs, indices, plot_labels, figure_title=\"temp plot\", filename=\"tempplt.png\"):\n",
        "    print(inputs)\n",
        "    fig, ax = plt.subplots(nrows=len(inputs[0]), ncols=1) \n",
        "    fig.set_size_inches(8.5, 10)\n",
        "    fig.suptitle(figure_title, fontsize=16)\n",
        "    colors = ['r', 'y', 'g']\n",
        "    legend = ['HR', \"RSHR\", \"PSHR\"]\n",
        "    for k, metric_group in enumerate(inputs):\n",
        "        # print(indices)\n",
        "        # print(\"v\"*50)\n",
        "        # print(percent_errors)\n",
        "        # print(step_sz)\n",
        "        widths = np.insert(indices[:-1],0,0)\n",
        "        widths = indices - widths\n",
        "        # print(indices)\n",
        "        # print(widths)\n",
        "        try:\n",
        "          indices = indices.to_list()\n",
        "        except:\n",
        "          pass\n",
        "        # print(len(indices))\n",
        "        # print(len(percent_errors))\n",
        "        # print(\"^\"*30)\n",
        "        for i, metric in enumerate(metric_group):\n",
        "          # print((indices +widths*.25*k).astype(int), metric, (widths*0.25).astype(int))\n",
        "          # barlist = ax[i].bar(indices +widths*.25*k, metric.flatten(), width=widths*0.25, color=colors[k]) \n",
        "          barlist = ax[i].plot(indices, metric.flatten(), color=colors[k], label=legend[k]) \n",
        "          # for j, percent in enumerate(metric.flatten()):\n",
        "          #   if percent < 0:\n",
        "          #     barlist[j].set_color('r')\n",
        "          #   else:\n",
        "          #     barlist[j].set_color('g')\n",
        "          ax[i].set_title(plot_labels[i]) # this many out of total number of points\n",
        "          ax[i].set_xlabel(\"Number of Samples\")\n",
        "          ax[i].set_ylabel(\"Error\")\n",
        "          # ax[i].set_ylim((-50, 50))\n",
        "          ax[i].set_xticks(indices)\n",
        "          ax[i].legend()\n",
        "          # rects = ax[i].patches\n",
        "          # print(metric - baseline_metric)\n",
        "          # bar_labels = [f\"{float(i)*100:.1f}\"  if len(inputs) == 2 else f\"{float(i):.1f}\" for i in (metric).flatten()]\n",
        "          # for rect, label in zip(rects, bar_labels):\n",
        "          #     height = rect.get_height()\n",
        "          #     x_value = rect.get_x() + rect.get_width() / 2\n",
        "\n",
        "          #     # Number of points between bar and label. Change to your liking.\n",
        "          #     space = 5\n",
        "          #     # Vertical alignment for positive values\n",
        "          #     va = 'bottom'\n",
        "\n",
        "          #     # If value of bar is negative: Place label below bar\n",
        "          #     if height < 0:\n",
        "          #         # Invert space to place label below\n",
        "          #         space *= -1\n",
        "          #         # Vertically align label at top\n",
        "          #         va = 'top'\n",
        "          #     ax[i].annotate(label, (x_value, height), xytext=(0, space), textcoords=\"offset points\", ha='center', va=va)\n",
        "        # ax[i].legend()\n",
        "    fig.show()\n",
        "    plt.savefig(filename+ \".png\")\n",
        "    plt.pause(0.1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzZzN-A3_SR"
      },
      "source": [
        "### eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boFd4KCra9Q7"
      },
      "source": [
        "import timeit\n",
        "import scipy.io as io\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as io\n",
        "from typing import List, Dict, Any\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from RR_RAPPOR import RAPPOR\n",
        "from IDLDP.pg1 import IDLDP, LDP_MECHANISMS\n",
        "\n",
        "\n",
        "# LDP_MECHANISMS = [ \"hadamard\", \"oue_basic\", \"rappor_basic\", \"idldp_opt0\", \"rappor_idldp_opt1\", \"oue_idldp_opt_0\"]\n",
        "\n",
        "def generate_dist(output_distribution_list, in_list_sizes, element_map, alphabet_size,  verbose=True):\n",
        "    merged_pred = np.zeros(alphabet_size)\n",
        "    for i, output_list in enumerate(output_distribution_list):\n",
        "      if verbose  :\n",
        "          print(f\"distribution number {i}\")\n",
        "          print(f\"output_distributions: {output_list}\")\n",
        "          print(f\"element_map: {element_map[str(i)]}\")\n",
        "          print(f\"in list size: {in_list_sizes[i]}\")\n",
        "      merged_pred[element_map[str(i)]] += np.nan_to_num(output_list)*in_list_sizes[i]\n",
        "\n",
        "    merged_pred = probability_normalize(merged_pred)\n",
        "    return merged_pred\n",
        "\n",
        "def evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size, k=None, eps=None, \\\n",
        "                          epochs=None,  init=None, encode_acc = 1, encode_mode = 0,\\\n",
        "                          tiers=2, relaxation_mode=0, score_mode=0, growth_mode=0, \\\n",
        "                          privacy_budget=None, epoch_subdivisions=None, split_percentiles=None,\\\n",
        "                          num_repititions=10, baseline_arg_map=None, save_images=True, output_counts = True, seed=0,\n",
        "                          idldp_config = None, ldp_mechanism=\"hadamard\"):\n",
        "    # fix alphabet size and privacy levl, get the error plot with respect to sample size\n",
        "    function_kwargs = locals()\n",
        "    k = alphabet_size\n",
        "    verbose = False\n",
        "    # output_counts = False # set to true for distribution over counts instead of counts\n",
        "    # print(f\"evaluate_ground_truth kwargs: {function_kwargs}\")\n",
        "    #Args:\n",
        "    # k : alphabet size,  eps: privacy level, rep: repetition times to compute a point\n",
        "    # epochs: number of points to compute\n",
        "    \n",
        "    # encode_acc : control whether to use fast encoding for hadamard responce\n",
        "    #           recommended and default: 1 use fast encoding when k < 10000\n",
        "    #                                    if memory use is high, disable this\n",
        "    \n",
        "    # mode: control encoding method for rappor and subset selection\n",
        "    #       0 for standard, which is fast but memory intensive\n",
        "    #       1 for light, which is relatively slow but not memory intensive\n",
        "    #       2 for compress, where we compress the output of rappor and subsetselection into locations of ones.\n",
        "    #       recommended and default: 0 when k <= 5000 n <= 1000000\n",
        "    #                               if memory use is high, use light mode\n",
        "    #       you can also create other modes by modifying the code\n",
        "    # print('Alphabet size:', k)\n",
        "    # print('Privacy level:', eps)\n",
        "    if seed:\n",
        "      np.random.seed(seed)\n",
        "    baseline_flag = False\n",
        "    if not epoch_subdivisions:\n",
        "        epoch_subdivisions = epochs\n",
        "    elif epoch_subdivisions == -1:\n",
        "      epoch_subdivisions = 1\n",
        "      baseline_flag = True\n",
        "      if verbose:\n",
        "        print(\"IN THE BASELINE EVALUATION\\n\")\n",
        "    assert relaxation_mode in [0,1],\" only 2 relaxation modes implemented\"\n",
        "    # indices = range(1, epoch_subdivisions + 1)\n",
        "    indices = []\n",
        "\n",
        "\n",
        "    assert ldp_mechanism in LDP_MECHANISMS, f'ldp mechanism must be one of {LDP_MECHANISMS} but got {ldp_mechanism}'\n",
        "    if LDP_MECHANISMS.index(ldp_mechanism) > 0: #if we in an IDLDP config\n",
        "      assert idldp_config is not None, \"must have a valid idldp config\"\n",
        "\n",
        "    #TODO init the privacy budget if none?\n",
        "    if privacy_budget is None:\n",
        "      privacy_budget = [1] + [1.2]*(tiers-1)\n",
        "    assert len(privacy_budget) == tiers, f\"privacy budget len is {len(privacy_budget)} but must match tiers ({tiers})\"\n",
        "\n",
        "    tot_mae = []\n",
        "    tot_counts = []\n",
        "    tot_count_mae = []\n",
        "    tot_mse = []\n",
        "    tot_count_mse = []\n",
        "    tot_count_rmse = []\n",
        "    tot_time = []\n",
        "\n",
        "    epoch_growth_rate = get_growth_rate(df, epochs, mode=DEFAULT_EPOCH_GROWTH_RATE)\n",
        "    # print(\"=\"*50)\n",
        "    # print(\"EPOCH GROWTH RATE\")\n",
        "    # print(epoch_growth_rate)\n",
        "    for epoch in range(epochs): # remember that this is for each set of final point sizes\n",
        "        if verbose:\n",
        "          print(f\"epoch number {epoch}\")\n",
        "        subdiv_growth_rate = None\n",
        "        epoch_df = df[:epoch_growth_rate[epoch]] if epoch < epochs - 1 else df\n",
        "        indices.append(len(epoch_df))\n",
        "        if epoch_subdivisions > 1:\n",
        "        # epoch_df = epoch_df.reset_index()\n",
        "          subdiv_growth_rate = get_growth_rate(epoch_df, epoch_subdivisions, mode=growth_mode)\n",
        "          # print(f\"epoch subdivision split points: {subdiv_growth_rate}\")\n",
        "          epoch_subdivision_dfs = np.split(epoch_df, subdiv_growth_rate)\n",
        "          assert len(epoch_subdivision_dfs) == epoch_subdivisions, \" should have the same numebr fo segments as subdivisions\"\n",
        "        elif epoch_subdivisions == 1:\n",
        "          epoch_subdivision_dfs = [epoch_df]\n",
        "        else:\n",
        "          assert False, \" epoch subdivision should be greater than 0\"\n",
        "        \n",
        "        # TODO shuffle rows? subsection list, np.split, make that the in list....\n",
        "        # print(\"=\"*50)\n",
        "        # print(\"subdivisions\")\n",
        "        # print(subdiv_growth_rate if subdiv_growth_rate is not None else \"all\")\n",
        "        # # print(\"=\"*50)\n",
        "        # print(len(epoch_subdivision_dfs))\n",
        "\n",
        "        # chunks = np.split(range(k), tiers)\n",
        "        # element_map = {str(i): chunks[i] for i in range(tiers)}\n",
        "        element_map = {str(i): list(range(k)) if i == 0 else [] for i in range(tiers)}\n",
        "\n",
        "        # to store l1 errors for each method\n",
        "        l1_1 = [0]*epoch_subdivisions\n",
        "        l1_2 = [0]*epoch_subdivisions\n",
        "        l1_3 = [0]*epoch_subdivisions\n",
        "        l1_4 = [0]*epoch_subdivisions\n",
        "        \n",
        "        # to store l2 errors for each method\n",
        "        l2_1 = [0]*epoch_subdivisions\n",
        "        l2_2 = [0]*epoch_subdivisions\n",
        "        l2_3 = [0]*epoch_subdivisions\n",
        "        l2_4 = [0]*epoch_subdivisions\n",
        "        l2_5 = [0]*epoch_subdivisions\n",
        "\n",
        "        # to store decodign time for each method\n",
        "        t1_1 = [0]*epoch_subdivisions\n",
        "        t1_2 = [0]*epoch_subdivisions\n",
        "        t1_3 = [0]*epoch_subdivisions\n",
        "        t1_4 = [0]*epoch_subdivisions\n",
        "        past_sizes = []\n",
        "\n",
        "        for iter in range(epoch_subdivisions): # iterate with in the final point size over rthe number of points within\n",
        "        # doesn't have to match the number of epochs but currently does\n",
        "            if verbose:\n",
        "              print(f\"Computing epoch subdivision {iter}\")\n",
        "            in_list = epoch_subdivision_dfs[iter][col].to_numpy()\n",
        "            n = len(in_list)\n",
        "            past_sizes.append(n)\n",
        "            split_percentile_scores = np.zeros(alphabet_size)\n",
        "\n",
        "            if baseline_flag and tiers == 2: # bootstrap the ele tiers appropriately\n",
        "              raw_counter = Counter(in_list)\n",
        "              counter_with_zeros = {key: raw_counter[key] or 0 for key in element_map[\"0\"]}\n",
        "              prob_est_topk_4 = counter_to_dist(counter_with_zeros)\n",
        "\n",
        "              if score_mode == 0:\n",
        "                ranked_distribution_splits = get_random_element_split(prob_est_topk_4, [len(prob_est_topk_4)//2], seed=seed)\n",
        "              elif score_mode == 1:\n",
        "                ranked_distribution_splits, split_percentile_scores = get_percentile_indices(prob_est_topk_4, split_percentiles)\n",
        "              else:\n",
        "                assert False, \"score mode not implemented for Baseline\"\n",
        "\n",
        "            # print(f\"in list for iter {iter} with n {n} \\n \")\n",
        "            # print(past_sizes)\n",
        "            # print(np.sum(past_sizes))\n",
        "            # print(f\"out of {len(epoch_df)}\")\n",
        "\n",
        "            #get subsectino of indices. \n",
        "            count4 = 0\n",
        "            t4 = 0\n",
        "\n",
        "            ele_domain_sizes = [len(element_map[v]) for v in element_map.keys()]\n",
        "            if verbose:\n",
        "              print(\"ele tiers \", ele_domain_sizes)\n",
        "            enc = 1 if encode_acc == 1 else 0\n",
        "            count_table = Counter()\n",
        "\n",
        "            index_list = [np.concatenate([np.where(in_list == x) for x in domain_elements], axis = 1).flatten() for domain_elements in element_map.values() if len(domain_elements)]\n",
        "            in_list_sizes = [len(in_list) for in_list in index_list]\n",
        "\n",
        "            if LDP_MECHANISMS.index(ldp_mechanism) == 0:\n",
        "              # non_topk_sample_ind = np.array(range(n))\n",
        "              ldp_instance_list = [Hadamard_Rand_high_priv(q, eps*privacy_budget[i], enc) for i, q in enumerate(ele_domain_sizes) if q != 0]\n",
        "              encoding_list = [ldp_instance_list[i].encode_string(in_list[domain_elements]) for i, domain_elements in enumerate(index_list)]\n",
        "              #first generate actual subset of elements we want to split out from this sampling\n",
        "            else:\n",
        "              #we want to modify the ind_to_tier map based on the first round respoonse\n",
        "              #for the first round response just use OUE_basic/rappor_basic? seems unfair to rappor but im sure people using rappor have areason for using it not OUE.\n",
        "              #TODO\n",
        "              if LDP_MECHANISMS.index(ldp_mechanism) < 3:\n",
        "                ldp_instance_list = [RAPPOR(q, eps*privacy_budget[i], config=idldp_config) for i, q in enumerate(ele_domain_sizes) if q != 0]\n",
        "                encoding_list = [ldp_instance_list[i].encode_string(in_list[domain_elements]) for i, domain_elements in enumerate(index_list)]\n",
        "              else:\n",
        "                new_ind_to_tier = {}\n",
        "                for key, vals in element_map.items():\n",
        "                  new_ind_to_tier.update({v: key for v in vals})\n",
        "\n",
        "                idldp_config[\"ind_to_tier\"] = new_ind_to_tier if iter != 0 else None\n",
        "                ldp_instance_list = [RAPPOR(alphabet_size, eps, config=idldp_config)]\n",
        "                encoding_list = [ldp_instance_list[0].encode_string(in_list)] #for IDLDP methods just encode the entire string\n",
        "\n",
        "            #TODO pass all the params in and the ntest this tomorrow YAATEHR\n",
        "\n",
        "\n",
        "            start_time = timeit.default_timer()\n",
        "            output_distribution_list = []\n",
        "            if relaxation_mode == -1 or iter == 0:\n",
        "              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) for i, encoding in enumerate(encoding_list)]\n",
        "            elif relaxation_mode == 0:\n",
        "              raw_counter = Counter(in_list[index_list[0]])\n",
        "              counter_with_zeros = {key: raw_counter[key] or 0 for key in element_map[\"0\"]}\n",
        "              for i, key in enumerate(counter_with_zeros.keys()):\n",
        "                  assert key == element_map[\"0\"][i], \"key and element key should match\"\n",
        "              prob_est_topk_4 = counter_to_dist(counter_with_zeros)\n",
        "              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) if i != 0 else prob_est_topk_4 for i, encoding in enumerate(encoding_list)]\n",
        "\n",
        "            elif relaxation_mode == 1:\n",
        "              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) for i, encoding in enumerate(encoding_list)]\n",
        "            else:\n",
        "              assert False, \"should have a new defined relaxation mode\"\n",
        "\n",
        "            # print(in_list_sizes)\n",
        "            prob_est = generate_dist(output_distribution_list, in_list_sizes, element_map, alphabet_size, verbose=verbose)\n",
        "            # print(prob_est)\n",
        "            if not baseline_flag: #Compute ranked distribution splits with logic for making sure percentiles add up to 1\n",
        "            #Note that whne we are in the baselines we just compute the ranks once\n",
        "              if len(split_percentiles) == 1 and len(index_list) == 1:\n",
        "                if verbose:\n",
        "                  print(\"FIRST BRNACH OF PERCENTILE MDOS\")\n",
        "                current_split_percentiles = split_percentiles\n",
        "              elif len(split_percentiles) > len(index_list) -1:\n",
        "                if verbose:\n",
        "                  print(len(split_percentiles), len(index_list))\n",
        "                  print(in_list_sizes)\n",
        "                  print(ldp_instance_list)\n",
        "                sheered_percentiles = split_percentiles[len(index_list) -1:]\n",
        "                slack_percentile = np.sum(sheered_percentiles) / ( len(index_list) -1 )\n",
        "                print(\"updating slack percentile \", slack_percentile)\n",
        "                assert slack_percentile > 0 and slack_percentile < 1, \"slakc percentile should be a fraction\"\n",
        "                current_split_percentiles = split_percentiles[len(index_list)-1:] + slack_percentile\n",
        "                #TODO TEST TEH DISTRIBUTION SPLITS\n",
        "              elif len(split_percentiles) == len(index_list) -1: # ie bootstrap iteration\n",
        "                if verbose:\n",
        "                  print(\"LAST BRANCH OF PERCENTILE MODS\")\n",
        "                current_split_percentiles = split_percentiles\n",
        "              else:\n",
        "                assert False, \"Will end up throwing away sketched results\"\n",
        "\n",
        "              if score_mode == 0:\n",
        "                ranked_distribution_splits = get_random_element_split(prob_est, len(index_list), seed=seed)\n",
        "              elif score_mode == 1:\n",
        "                ranked_distribution_splits, split_percentile_scores = get_percentile_indices(prob_est, current_split_percentiles)\n",
        "              elif score_mode == 2:\n",
        "                ranked_distribution_splits, split_percentile_scores = get_rank_order_splits(prob_est, current_split_percentiles, split_percentile_scores)\n",
        "              else:\n",
        "                assert False, \"score mode not implemented\"\n",
        "\n",
        "              element_map = {str(i): ranked_distribution_splits[i] if i < len(ranked_distribution_splits) else [] for i in range(tiers)}\n",
        "\n",
        "\n",
        "            t4 = t4 + timeit.default_timer() - start_time\n",
        "            count4 = prob_est*n if output_counts else prob_est\n",
        "            if verbose:\n",
        "              print(f\"for iter {iter} the counts sum to {np.sum(prob_est*n)}\")\n",
        "            l1_4[iter] = count4\n",
        "            t1_4[iter] = t4\n",
        "        # print(f\"counts per iterations: {l1_4}\")\n",
        "        if output_counts:\n",
        "          tot_counts.append(np.sum(l1_4, axis=0))\n",
        "        else:\n",
        "          tot_counts.append(np.average(l1_4, axis=0, weights=[len(x) for x in epoch_subdivision_dfs]))\n",
        "        # print(tot_counts)\n",
        "        tot_time.append(np.sum(t1_4))\n",
        "    if verbose:\n",
        "      print(\"eval computation loop over\\n\\n\")\n",
        "    # COmpute all errors from counts\n",
        "    ground_truth_counts = ground_truth_hist[col]\n",
        "    ground_truth_val, ground_truth_key = ground_truth_counts.values, ground_truth_counts.keys()\n",
        "    total_number_of_elements = np.sum(ground_truth_val)\n",
        "\n",
        "    for i, eval_counts in enumerate(tot_counts):\n",
        "      if verbose:\n",
        "        print(\"enumerating thorugh tot_counts (eval counts)\")\n",
        "        print(len(tot_counts))\n",
        "        print(len(epoch_subdivision_dfs))\n",
        "      eval_map = defaultdict(lambda: 0, zip(range(alphabet_size), eval_counts))\n",
        "      comparison_map = defaultdict(lambda: 0, zip(ground_truth_key, (ground_truth_val if output_counts else ground_truth_val/total_number_of_elements) \\\n",
        "                                                  * (epoch_growth_rate[i]/total_number_of_elements if i < len(tot_counts)-1 and output_counts else 1)))\n",
        "      differences = {k: abs((eval_map[k] or 0) - x ) for k, x in comparison_map.items()}\n",
        "      if verbose:\n",
        "        print(f\"for iter {i} the computed counts sum to {np.sum(list(eval_map.values()))}\")\n",
        "        print(f\"for iter {i} the actual counts sum to {np.sum(list(comparison_map.values()))}\")\n",
        "        print(\"percentage of counts for ground truth: \", epoch_growth_rate[i]/total_number_of_elements if i < len(tot_counts)-1 else 1)\n",
        "        print(\"differences between eval and comparison[0]: \", list( differences.items())[0])\n",
        "        print(eval_map[0], comparison_map[0])\n",
        "        print(\"eval map[0] vs ground truth (comparison) map[0] ^\")\n",
        "      mae = np.linalg.norm(list(differences.values()), ord=1) / (epoch_growth_rate[i] if i < len(tot_counts)-1 else total_number_of_elements)\n",
        "      rmse = np.linalg.norm(list(differences.values()), ord=2) / (epoch_growth_rate[i] if i < len(tot_counts)-1 else total_number_of_elements)\n",
        "\n",
        "      mse = rmse ** 2\n",
        "      tot_mse.append(mse)\n",
        "      tot_count_rmse.append(rmse)\n",
        "      tot_mae.append(mae)\n",
        "    if verbose:\n",
        "      print(\"indices: \", indices)\n",
        "      print(\"total mae: \", tot_mae)\n",
        "    plot_label = \"hr - ILDP rollup error\"\n",
        "    # ax[0][0].plot(indices,tot_mae, label = plot_label)\n",
        "    # ax[0][0].legend()\n",
        "    # ax[0][0].set_title('L1 loss', fontdict={'fontsize': 8, 'fontweight': 'medium'})\n",
        "    \n",
        "    # ax[0][1].plot(indices,tot_mse, label = plot_label)\n",
        "    # ax[0][1].legend()\n",
        "    # ax[0][1].set_title('L2 loss', fontdict={'fontsize': 8, 'fontweight': 'medium'})\n",
        "    \n",
        "    # ax[1][0].plot(indices,tot_time, label = plot_label)\n",
        "    # ax[1][0].legend()\n",
        "    # ax[1][0].set_title('train time', fontdict={'fontsize': 8, 'fontweight': 'medium'})\n",
        "\n",
        "    # if epoch == 0:\n",
        "    #     ax[1][1].scatter(ground_truth_val, ground_truth_key)\n",
        "    #     ax[1][1].set_title('True Distribution', fontdict={'fontsize': 8, 'fontweight': 'medium'})\n",
        "    # for axis in chain(*ax):\n",
        "    #     # axis.set_yscale(\"log\")\n",
        "    #     pass\n",
        "    # plt.pause(0.1)\n",
        "\n",
        "\n",
        "    ################ BASELINE CALL #################\n",
        "\n",
        "    if baseline_arg_map:\n",
        "      baseline_data = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size, **baseline_arg_map)\n",
        "      b_data = {f\"baseline_{k}\": v for k,v in baseline_data.items()}\n",
        "    else:\n",
        "     b_data = {}\n",
        "\n",
        "\n",
        "    #save all the data into a mat file with time stamp\n",
        "    time = datetime.datetime.now().strftime(\"%m_%d_%H_%M\")\n",
        "    indices = np.array(indices)\n",
        "    data = {\n",
        "        'time' : time,\n",
        "        'absz' : k,\n",
        "        'privacy' : eps,\n",
        "        'indices' : indices, # indices of each point (number of samples)\n",
        "        'hr_error': tot_mae,\n",
        "        'hr_error_l2': tot_mse,\n",
        "        'hr_count_error_rmse': tot_count_rmse,\n",
        "        'hr_time': tot_time,\n",
        "        **b_data\n",
        "    }\n",
        "    para = 'k_{}_eps_{}_{}'.format(k,eps,privacy_budget)\n",
        "\n",
        "    baseline_metrics = []\n",
        "    input_metrics = []\n",
        "    if baseline_arg_map:\n",
        "      baseline_l1 = np.array(baseline_data[\"hr_error\"])\n",
        "      baseline_l2 = np.array(baseline_data[\"hr_error_l2\"])\n",
        "      baseline_rmse = np.array(baseline_data[\"hr_error_l2\"])**.5\n",
        "      data[\"baseline_hr_error\"] = baseline_l1\n",
        "      data[\"baseline_hr_error_l2\"] = baseline_l2\n",
        "      data[\"baseline_hr_count_error_rmse\"] = baseline_rmse\n",
        "    return data\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWjcszalWMo"
      },
      "source": [
        "#DATA Generation\n",
        "\n",
        "## Schema\n",
        "| Name | Type | Description | Example |\n",
        "| ---- | ---- | ----------- | ------- |\n",
        "| **key** | `string` | Unique string identifying the region | US_CA |\n",
        "| **country_region** | `string` | The name of the country in English. | United States |\n",
        "| **country_region_code** | `string` | The [ISO 3166-1](https://en.wikipedia.org/wiki/ISO_3166-1) code for the country. | US |\n",
        "| **sub_region_1** | `string` | The name of a region in the country. | California |\n",
        "| **sub_region_1_code** | `string` | A country-specific [ISO 3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code for the region. | US-CA |\n",
        "| **sub_region_2** | `string` | The name of a subdivision of the region above. For example, *Santa Clara County*. |  |\n",
        "| **sub_region_2_code** | `string` | For the US - The [FIPS code](https://en.wikipedia.org/wiki/FIPS_county_code) for a US county (or equivalent). | 06085 |\n",
        "| **date** | `string` | The day on which the searches took place. For weekly data, this is the first day of the 7-day weekly interval starting on Monday. For example, in the weekly data the row labeled *2020-07-13* represents the search activity for the week of July 13 to July 19, 2020, inclusive. Calendar days start and end at midnight, Pacific Standard Time. | 2020-07-13 |\n",
        "| **`${symptom name}`** | `double` `[0-100]` | Repeated for each symptom. Reflects the normalized search volume for this symptom, for the specified date and region. The field may be empty when data is not available. | 87.02 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gwBjeW7TqOd",
        "outputId": "324af2f1-f0f0-4369-b9a2-f60e1926d8fe"
      },
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /home/yaatehr/miniconda3/lib/python3.7/site-packages (0.18.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FlUuLv4ppsZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d678f2b1-e82c-4a77-99cf-16f033d285ed"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import urllib.request\n",
        "from fuzzywuzzy import fuzz, process\n",
        "\n",
        "def find_related_cols(columns, keywords):\n",
        "  output = list()\n",
        "  # processor = lambda x: \" \".join(x.split(\"_\"))\n",
        "  for keyword in keywords:\n",
        "    choices = process.extractWithoutOrder(keyword, columns, score_cutoff=80)\n",
        "    # print(f\"found the following choices for {keyword}: {choices}\")\n",
        "    output.extend(choices)\n",
        "\n",
        "  return list(set(output))\n",
        "\n",
        "GENERIC_SYMPTOM_KEYWORDS = [\n",
        "                    \"Fever\",\n",
        "                    \"chills\",\n",
        "                    \"breath\",\n",
        "                    \"fatigue\",\n",
        "                    # \"ache\",\n",
        "                    \"taste\",\n",
        "                    \"smell\",\n",
        "                    \"throat\",\n",
        "                    \"congestion\",\n",
        "                    \"runny\",\n",
        "                    \"nausea\",\n",
        "                    \"vomiting\",\n",
        "                    \"diarrhea\",\n",
        "                    # \"muscle\"\n",
        "]\n",
        "\n",
        "\n",
        "pd.set_option('display.max_row', 500)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "DATA_URL = \"https://storage.googleapis.com/covid19-open-data/v2/google-search-trends.csv\"\n",
        "\n",
        "\n",
        "# Load CSV data directly from the URL with pandas, the options are needed to prevent\n",
        "# reading of records with key \"NA\" (Namibia) as NaN\n",
        "\n",
        "class MockArgs(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "class Dataset(object):\n",
        "  \"\"\"\n",
        "  class to load and compute dataset manipulations for symptom dgeneration\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_url, args=None):\n",
        "    self.args = args if args else MockArgs()\n",
        "\n",
        "    if not os.path.exists(DATA_FILE_PATH):\n",
        "        if not os.path.exists(os.path.dirname(DATA_FILE_PATH)):\n",
        "            os.makedirs(os.path.dirname(DATA_FILE_PATH), exist_ok=True)\n",
        "        urllib.request.urlretrieve(DATA_URL, DATA_FILE_PATH)\n",
        "\n",
        "    self.df = self._prepare_dataframe(data_url)\n",
        "\n",
        "    \n",
        "  def _prepare_dataframe(self, data_url: str) -> pd.DataFrame:\n",
        "      \"\"\"\n",
        "      Loads the dataset and cleans it up\n",
        "      :param data_url: the url containing the original data\n",
        "      :return: a Pandas DataFrame with the historical data\n",
        "      \"\"\"\n",
        "      symptom_data = pd.read_csv(\n",
        "          data_url,\n",
        "          keep_default_na=False,\n",
        "          na_values=[\"\"],\n",
        "      )\n",
        "      print(symptom_data.columns)\n",
        "      return symptom_data\n",
        "\n",
        "\n",
        "symptom_data = pd.read_csv(\n",
        "    DATA_FILE_PATH,\n",
        "    keep_default_na=False,\n",
        "    na_values=[\"\"],\n",
        ")\n",
        "symptom_data[\"date\"] = pd.to_datetime(symptom_data[\"date\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8f88a20cef80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m symptom_data = pd.read_csv(\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mDATA_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/idldp/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yaatehr/programs/hadamard_response/data/google-search-trends.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbnVkk2PycUR"
      },
      "source": [
        "COVID_SYMPTOM_COLS, scores = zip(*find_related_cols(symptom_data.columns.tolist(), GENERIC_SYMPTOM_KEYWORDS))\n",
        "COVID_SYMPTOM_COLS = list(COVID_SYMPTOM_COLS)\n",
        "COVID_SYMPTOM_COLS.remove(\"search_trends_diarrhea\")\n",
        "\n",
        "for c in COVID_SYMPTOM_COLS:\n",
        "  print(c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYzXvUjvclBV"
      },
      "source": [
        "# Parallel eval pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Hfpa9-MOrI"
      },
      "source": [
        "from itertools import chain\n",
        "import pickle\n",
        "import copy\n",
        "from IDLDP.pg1 import ldp_mechanism_helper\n",
        "\n",
        "\n",
        "def evaluate_dataset_parallel(df, target_col_groups,\n",
        "                              metadata_cols=None,\n",
        "                              score_mode =0,\n",
        "                              growth_mode=0,\n",
        "                              k=None,\n",
        "                              eps=None,\n",
        "                              split_percentiles=None,\n",
        "                              epochs=None,\n",
        "                              step_sz=None,\n",
        "                              init=None,\n",
        "                              dist=None,\n",
        "                              encode_acc = 1,\n",
        "                              encode_mode = 0,\n",
        "                              tiers=2,\n",
        "                              relaxation_mode=0,\n",
        "                              privacy_budget=None,\n",
        "                              epoch_subdivisions=None,\n",
        "                              BASELINE_VERSION = 0,\n",
        "                              num_repititions = 10,\n",
        "                              output_counts=False,\n",
        "                              save_images=False,\n",
        "                              ldp_mechanism=\"hadamard\",\n",
        "                              idldp=None,\n",
        "                              ):\n",
        "\n",
        "  #preprocess df to contain categories\n",
        "  # selecting cols by name\n",
        "  # print(f\"evaluate_dataset_parallel kwargs: {locals()}\")\n",
        "  # locals().update(kwargs)\n",
        "\n",
        "\n",
        "  arg_map = copy.deepcopy(locals())\n",
        "  arg_map.pop('df', None)\n",
        "  arg_map.pop('target_col_groups', None)\n",
        "  arg_map.pop('metadata_cols', None)\n",
        "  arg_map.pop('step_sz', None)\n",
        "  arg_map.pop('dist', None)\n",
        "  arg_map.pop('BASELINE_VERSION', None)\n",
        "  baseline_arg_map = copy.deepcopy(arg_map)\n",
        "\n",
        "\n",
        "  #For output grahps (all are computed, not all are displayed)\n",
        "  # Out of all possible: interest_cols = [\"hr_error\", \"hr_error_l2\", \"hr_count_error_rmse\"]\n",
        "  interest_cols = [\"hr_error\", \"hr_error_l2\", \"hr_count_error_rmse\"]\n",
        "\n",
        "\n",
        "\n",
        "  # HR Baseline.\n",
        "  hr_baseline = {\n",
        "      'epoch_subdivisions': -1,\n",
        "      'score_mode': 0, # DOesn't actually matter, nveer used.\n",
        "      'tiers': 1,\n",
        "      'split_percentiles': None,\n",
        "  }\n",
        "\n",
        "  #Random SPlit Hadamard Response.\n",
        "\n",
        "  random_split_baseline = {\n",
        "      'epoch_subdivisions': -1,\n",
        "      'split_percentiles': None,\n",
        "      'score_mode': 0, # random scores\n",
        "      'tiers': 2,\n",
        "  }\n",
        "\n",
        "  # Percentile split hadamard response.\n",
        "  percentile_split_baseline = {\n",
        "      'epoch_subdivisions': -1, #TODO change to the number of epoch subdivisions we have? This triggers baseline flag logic (ie we start to recompute the scores)\n",
        "      'split_percentiles': [.1],\n",
        "      'score_mode': 1,\n",
        "      'tiers': 2,\n",
        "  }\n",
        "\n",
        "  baselines = [hr_baseline, random_split_baseline, percentile_split_baseline]\n",
        "\n",
        "\n",
        "  # baseline_arg_map[\"split_percentiles\"] = None\n",
        "  # baseline_arg_map[\"tiers\"] = 1\n",
        "  # # baseline_arg_map[\"point_growth_rate\"]=0\n",
        "  # baseline_arg_map[\"score_mode\"] = 0\n",
        "  print(\"experimental_args: \", arg_map)\n",
        "  print(\"\\n\")\n",
        "  # print(\"baseline_args: \", baseline_arg_map)\n",
        "  # print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "  #INIT IDLDP\n",
        "  # if idldp is None:\n",
        "  #   idldp = IDLDP()\n",
        "\n",
        "\n",
        "  metadata_cols = [] if not metadata_cols else metadata_cols\n",
        "  g = df.columns.to_series().groupby(df.dtypes).groups\n",
        "  type_to_col = {k.name: v for k, v in g.items()}\n",
        "  string_col_codes_and_labels = {}\n",
        "  num_elements = len(df)\n",
        "  ground_truth_hist = {}\n",
        "  results= {}\n",
        "\n",
        "  if \"object\" in type_to_col.keys():\n",
        "    for col in type_to_col[\"object\"]:\n",
        "      if col in metadata_cols:\n",
        "        continue\n",
        "      cat = pd.Categorical(df[col])\n",
        "      string_col_codes_and_labels[col] = (cat.codes, cat.categories)\n",
        "      df[col] = cat.codes\n",
        "  \n",
        "  summed_results = np.zeros((6, epochs))\n",
        "  listed_results = []\n",
        "\n",
        "  experimental_results = []\n",
        "  baseline_results = []\n",
        "\n",
        "  total_alphabet_size = 0\n",
        "  config_cache = {f\"eval_{i}\": None for i in range(len(target_col_groups))}\n",
        "  if not isinstance(BASELINE_VERSION, list):\n",
        "    BASELINE_VERSION = [BASELINE_VERSION]\n",
        "  for b in BASELINE_VERSION:\n",
        "    config_cache.update({f\"baseline_{b}_{i}\": None for i in range(len(target_col_groups))})\n",
        "\n",
        "  for rep_no in range(num_repititions):\n",
        "      \n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    for i, col_group in enumerate(target_col_groups):\n",
        "      col = f\"col_group_{i}\"\n",
        "      df[col] = df[col_group].astype(str).apply(''.join, axis=1)\n",
        "      cat = pd.Categorical(df[col])\n",
        "      string_col_codes_and_labels[col] = (cat.codes, cat.categories)\n",
        "      df[col] = cat.codes\n",
        "      alphabet_size = len(df[col].unique())\n",
        "      total_alphabet_size += alphabet_size\n",
        "\n",
        "      for c in chain(col_group, [col]):\n",
        "        #make histogram\n",
        "        col_histogram = df[c].value_counts(sort=False)\n",
        "        # print(col_histogram)\n",
        "        # ground_truth_hist[c] = (col_histogram.to_numpy() / num_elements, col_historgram.index.to_list())\n",
        "        ground_truth_hist[c] = col_histogram\n",
        "      \n",
        "\n",
        "      opt_mode = ldp_mechanism_helper(ldp_mechanism)\n",
        "      split_percentiles = np.cumsum(np.array([1/tiers]*(tiers-1))) if not split_percentiles else split_percentiles\n",
        "      assert len(split_percentiles) == tiers -1, f\"split percentilse must match tiers but got {len(split_percentiles)} and {tiers -1}\"\n",
        "\n",
        "      #convert np.split points to a distribution over high to low priv elements\n",
        "\n",
        "      def split_percentiles_to_tier_splits(split_points:List[float]):\n",
        "        cum_split = np.cumsum(split_points).tolist()\n",
        "        cum_split.insert(0, 0)\n",
        "        cum_split.pop(-1)\n",
        "        temp = np.subtract(split_points, cum_split)\n",
        "        tier_splits = np.concatenate([temp, [1- np.sum(temp)]]).tolist()\n",
        "        return tier_splits\n",
        "\n",
        "\n",
        "      if opt_mode != -1 and config_cache[f\"eval_{i}\"] is None:\n",
        "        tier_split_percentages = split_percentiles_to_tier_splits(split_percentiles)\n",
        "        print(\"first config generation\")\n",
        "        idldp_config = idldp.gen_perturbation_probs(\n",
        "          epsilon=eps,\n",
        "          privacy_budget=privacy_budget, #\n",
        "          tier_split_percentages=tier_split_percentages,\n",
        "          domain_size=alphabet_size,\n",
        "          total_records= None,#Doesn't actually affect anything....\n",
        "          opt_mode=opt_mode,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "      col_group_results = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size,\n",
        "        score_mode=score_mode, k=k, eps=eps, epochs=epochs, init=init, encode_acc = encode_acc,\n",
        "        encode_mode = encode_mode, tiers=tiers, relaxation_mode=relaxation_mode,\n",
        "        privacy_budget=privacy_budget, epoch_subdivisions=epoch_subdivisions,\n",
        "        split_percentiles=split_percentiles, baseline_arg_map=None,\n",
        "        save_images=save_images, output_counts = output_counts,\n",
        "        idldp_config=copy.deepcopy(config_cache[f\"eval_{i}\"]),\n",
        "        ldp_mechanism=ldp_mechanism,\n",
        "        )\n",
        "      \n",
        "      results[col] = [col_group_results]\n",
        "\n",
        "      for b in BASELINE_VERSION:\n",
        "        baseline_arg_map = copy.deepcopy(arg_map)\n",
        "        baseline_arg_map.update(baselines[b])\n",
        "\n",
        "\n",
        "        if rep_no == 0:\n",
        "          print(\"baseline_args: \", baseline_arg_map)\n",
        "          print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "        baseline_arg_map[\"split_percentiles\"] = np.cumsum(np.array([1/baseline_arg_map[\"tiers\"]]*(baseline_arg_map[\"tiers\"]-1))) if (\"split_percentiles\" not in baseline_arg_map.keys() or baseline_arg_map[\"split_percentiles\"] is None) else baseline_arg_map[\"split_percentiles\"]\n",
        "        assert baseline_arg_map[\"tiers\"] == 1 or len(baseline_arg_map[\"split_percentiles\"]) == baseline_arg_map[\"tiers\"] -1, f\"split percentilse must match tiers but got {len(baseline_arg_map['split_percentiles'])} and {baseline_arg_map['tiers'] -1}\"\n",
        "        \n",
        "        opt_mode = ldp_mechanism_helper(baseline_arg_map[\"ldp_mechanism\"])\n",
        "\n",
        "        if opt_mode != -1 and config_cache[f\"baseline_{b}_{i}\"] is None:\n",
        "          tier_split_percentages = split_percentiles_to_tier_splits(split_percentiles)\n",
        "          idldp_config = idldp.gen_perturbation_probs(\n",
        "            epsilon=eps,\n",
        "            privacy_budget=privacy_budget, #\n",
        "            tier_split_percentages=tier_split_percentages,\n",
        "            domain_size=alphabet_size,\n",
        "            total_records = None,#Doesn't actually affect anything....\n",
        "            opt_mode=opt_mode,\n",
        "          )\n",
        "          baseline_arg_map[\"idldp_config\"] = idldp_config\n",
        "\n",
        "\n",
        "        if rep_no == 0:\n",
        "          print(\"baseline_args: \", baseline_arg_map)\n",
        "          print(\"\\n\\n\")\n",
        "\n",
        "        baseline_data = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size,\n",
        "          **baseline_arg_map)\n",
        "        results[col].append(baseline_data)\n",
        "        \n",
        "\n",
        "    for col in [f\"col_group_{i}\" for i in range(len(target_col_groups))]:\n",
        "      r = results[col] # we have a list of true, + variable baselines\n",
        "\n",
        "      experimental_results.append([r[0][interest] for interest in interest_cols])\n",
        "      baseline_results.append([[l[interest] for interest in interest_cols] for i,l in enumerate(r) if i > 0])\n",
        "\n",
        "      #to get the relative results for each baseline we will need to stack each group\n",
        "\n",
        "      # stack = np.vstack((r[c] for c in chain(interest_cols, [f\"baseline_{x}\" for x in interest_cols])))\n",
        "      # print(stack.shape)\n",
        "      # print(summed_results.shape)\n",
        "      # summed_results += stack\n",
        "      # listed_results.append(stack)\n",
        "\n",
        "  # summed_results/= num_repititions\n",
        "  # total_alphabet_size /= num_repititions\n",
        "  # listed_results = np.array(listed_results)\n",
        "  # scatter_results = [listed_results[:,i,:] for i in range(6)]\n",
        "  # print(scatter_results)\n",
        "  # print(f\"scatter results of len {len(scatter_results)} shape is: \")\n",
        "  # print(scatter_results[0].shape)\n",
        "\n",
        "  experimental_results = np.array(experimental_results, ndmin=3)\n",
        "  baseline_results = np.array(baseline_results, ndmin=4)\n",
        "  # print(experimental_results.shape)\n",
        "  # print(baseline_results.shape)\n",
        "  experimental_results = experimental_results.transpose(1,0,2)\n",
        "  baseline_results = baseline_results.transpose(1,2,0,3)\n",
        "  # print(experimental_results)\n",
        "  # print(baseline_results)\n",
        "  # baseline_results = [b.tolist() for b in baseline_results]\n",
        "\n",
        "\n",
        "  para = 'k_{}_epss_{}_{}'.format(total_alphabet_size,eps,privacy_budget)\n",
        "  time = datetime.datetime.now().strftime(\"%m_%d_%H-%M\")\n",
        "  legend = ['HR', \"RSHR\", \"PSHR\"]\n",
        "  # print(total_alphabet_size)\n",
        "  \n",
        "  RELAXATIONS = [\"Oracle\", \"Split Domain\"]\n",
        "\n",
        "\n",
        "  filename = f\"{VERSION_NUM}_{dist}_t{time}_rmode_{relaxation_mode}_smode_{score_mode}_gmode_{growth_mode}_t_{tiers}_{para}\" #'data_' + dist + '_'+ relaxation_mode + topk + para + time\n",
        "  kstr = f\"Tier {tiers}, splits {split_percentiles}, k={total_alphabet_size}\"\n",
        "  boxplot_title_string = f\"{dist}, {RELAXATIONS[relaxation_mode]} HR, \\u03B5=({eps}, {privacy_budget}), {kstr}\"\n",
        "\n",
        "  # print(results)\n",
        "\n",
        "  gen_aggregate_column_boxplot(experimental_results, baseline_results, results[col][0][\"indices\"], [legend[i] for i in BASELINE_VERSION], interest_cols, boxplot_title_string)  \n",
        "\n",
        "\n",
        "  # interest_cols.extend([f\"baseline_{x}\" for x in interest_cols])\n",
        "\n",
        "  # tot_mae, tot_mse, tot_rmse, baseline_l1, baseline_l2, baseline_rmse = np.vsplit(summed_results, list(range(1,6)))\n",
        "  # baseline_metrics = [baseline_l1, baseline_rmse]\n",
        "  # input_metrics = [tot_mae, tot_rmse]\n",
        "  # para = 'k_{}_epss_{}_{}'.format(total_alphabet_size,eps,eps*eps_relaxation)\n",
        "  # time = datetime.datetime.now().strftime(\"%m_%d_%H-%M\")\n",
        "  # legend = ['HR', \"RSHR\", \"PSHR\"]\n",
        "  # # print(total_alphabet_size)\n",
        "  \n",
        "\n",
        "  # filename = f\"3.3.3_{dist}_t{time}_rmode_{relaxation_mode}_smode_{score_mode}_gmode_{growth_mode}_t_{tiers}_{para}_BV_{BASELINE_VERSION}\" #'data_' + dist + '_'+ relaxation_mode + topk + para + time\n",
        "  # kstr = f\"Tier {tiers}, splits {split_percentiles}, {legend[BASELINE_VERSION]} baseline, k={total_alphabet_size}\"\n",
        "\n",
        "  # plot_labels = [\"L1 Error Deviations\", \"RMSE Deviations\"]\n",
        "  # RELAXATIONS = [\"Oracle\", \"Split Domain\"]\n",
        "  # #percentage errors\n",
        "  # gen_side_by_side_boxplots(scatter_results, results[col][\"indices\"], interest_cols, boxplot_title_string)\n",
        "  # percent_error_title_string = f\"%Err - {dist}, {RELAXATIONS[relaxation_mode]} HR, \\u03B5=({eps}, {eps*eps_relaxation}), {kstr}\"\n",
        "  # generate_plot_for_metrics(input_metrics, baseline_metrics, results[col][\"indices\"], plot_labels[:len(input_metrics)], figure_title=percent_error_title_string,filename=filename, save_images=save_images, output_counts = output_counts)\n",
        "  # kwargs = kwarg_dummy2(k=total_alphabet_size, eps=eps, epochs=epochs, init=init, encode_acc = encode_acc, encode_mode = encode_mode, tiers=tiers, relaxation_mode=relaxation_mode, eps_relaxation=eps_relaxation, epoch_subdivisions=epoch_subdivisions, split_percentiles=split_percentiles, baseline_arg_map=baseline_arg_map)\n",
        "  # print(filename)\n",
        "  # if save_images:\n",
        "  #     pickle.dump((input_metrics, baseline_metrics, results[col][\"indices\"], plot_labels[:len(input_metrics)], percent_error_title_string, filename, kwargs), open(f\"{filename}.pkl\", 'wb'))\n",
        "  \n",
        "  return results, ground_truth_hist, string_col_codes_and_labels\n",
        "\n",
        "# function_kwargs = {'k': 1000, 'score_mode' : 0, 'eps': 2, 'tiers': 2, 'split_percentiles': [.7], 'epochs': 10, 'encode_acc': 0, 'encode_mode': 0, 'point_growth_rate': 1, 'relaxation_mode': 1, 'basegroup_size': 0.01, 'eps_relaxation': 1, 'epoch_subdivisions': None, 'init': 1, 'step_sz': 50000}\n",
        "# evaluate_dataset_parallel(github_sickness_data, [github_sickness_data_cols], **function_kwargs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRC9ScYzvaWt"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d347BTMm--8S"
      },
      "source": [
        "### Cancer data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM5C9ZGzv5gq"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOs7UBQKyucu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "437d07c7-5c1b-452d-92ad-6de217d81d43"
      },
      "source": [
        "NUM_GENERATED_DIST_SAMPLES = 50000\n",
        "NUM_UNIQUE_ELES = 100\n",
        "number_of_columns = 1\n",
        "unique_elements = np.arange(NUM_UNIQUE_ELES)\n",
        "prob = generate_Zipf_distribution(NUM_UNIQUE_ELES, 1.0)\n",
        "\n",
        "assert len(unique_elements) == len(prob), f\"length of {len(unique_elements)} should be equal to the length of the probability dist {len(prob)}\"\n",
        "\n",
        "\n",
        "power_law_dict = {col: np.random.choice(unique_elements, NUM_GENERATED_DIST_SAMPLES, p=prob) for col in range(number_of_columns)}\n",
        "\n",
        "power_law_data = pd.DataFrame(power_law_dict)\n",
        "power_law_data_cols = list(range(number_of_columns))\n",
        "\n",
        "power_law_data.describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  0\n",
              "count  50000.000000\n",
              "mean      18.359900\n",
              "std       24.569252\n",
              "min        0.000000\n",
              "25%        1.000000\n",
              "50%        7.000000\n",
              "75%       27.000000\n",
              "max       99.000000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>18.359900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>24.569252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>27.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>99.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__iZ1H7ErbK_"
      },
      "source": [
        "#Real Data main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyU0sZjQreh0",
        "outputId": "839a16d9-f9ec-4924-cc83-1c803059ea64"
      },
      "source": [
        "# %matplotlib inline\n",
        "from IDLDP.pg1 import IDLDP\n",
        "idldp = IDLDP()\n",
        "# (covid_symptom_data, cancer_data, github_sickness_data) = pickle.load(open('datasets.pkl', 'rb'))\n",
        "dataset_dfs = [\n",
        "  # covid_symptom_data,\n",
        "  # cancer_data,\n",
        "  # github_sickness_data,\n",
        "  power_law_data,\n",
        "]\n",
        "DATASET_NAMES = [\n",
        "  # \"COVID synthetic\", \n",
        "# \"Cancer Data\", \n",
        "# \"Github Sickness all cols\",\n",
        "\"Power Law 1k 1col\",\n",
        "]\n",
        "\n",
        "n = 2 #must be graeteer than or equal to 2\n",
        "gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n",
        "dataset_col_group_settings = [\n",
        "  # [[COVID_SYMPTOM_COLS], gen_col_group_split(2, COVID_SYMPTOM_COLS), gen_col_group_split(3, COVID_SYMPTOM_COLS)],\n",
        "  # [  [[\"stage\", \"grid_index\"]], [[\"stage\"], [\"grid_index\"]], ],\n",
        "  # [[github_sickness_data_cols], gen_col_group_split(2, github_sickness_data_cols), gen_col_group_split(3, github_sickness_data_cols)],\n",
        "  [[power_law_data_cols]]\n",
        "]\n",
        "relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n",
        "\n",
        "failed_runs = []\n",
        "for relaxation_mode in [0, 1]:\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  for i, dataset_df in enumerate(dataset_dfs):\n",
        "    for j, col_group in enumerate(dataset_col_group_settings[i]):\n",
        "      for ldp_mechanism in LDP_MECHANISMS[0]:\n",
        "        for baseline_version in range(3):\n",
        "          for tiers in [2]:\n",
        "            for split_percentiles in [[.1]]:#[[i*.1] for i in range(1, 4)]:\n",
        "              for eps in [0.01]:#, 0.1, 1]:\n",
        "                # relaxation_mode = 1\n",
        "                epochs = 5\n",
        "                growth_mode = 0\n",
        "                score_mode = 1\n",
        "                privacy_budget = [1] + [1.2]*(tiers-1)\n",
        "                epoch_subdivisions = 2\n",
        "                num_repititions = 3\n",
        "                # for score_mode in range(1,3):\n",
        "                # for growth_mode in [1]:\n",
        "                for epoch_subdivisions in range(1,10, 3):\n",
        "                  # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n",
        "                      print(datetime.datetime.now())\n",
        "                      print(DATASET_NAMES[i])\n",
        "                      print(col_group)\n",
        "                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n",
        "                      'growth_mode': growth_mode,\n",
        "                      'k': 1000,\n",
        "                      'score_mode' : score_mode,\n",
        "                      'eps': eps,\n",
        "                      'tiers': tiers,\n",
        "                      'split_percentiles': split_percentiles,\n",
        "                      'epochs': epochs,\n",
        "                      'encode_acc': 0,\n",
        "                      'encode_mode': 0,\n",
        "                      'relaxation_mode': relaxation_mode,\n",
        "                      'privacy_budget': privacy_budget,\n",
        "                      'epoch_subdivisions': epoch_subdivisions,\n",
        "                      'init': 1,\n",
        "                      'step_sz': 50000,\n",
        "                      'num_repititions': num_repititions,\n",
        "                      'output_counts': False,\n",
        "                      'BASELINE_VERSION': baseline_version,\n",
        "                      'ldp_mechanism': ldp_mechanism,\n",
        "                      'idldp': idldp,\n",
        "                      }\n",
        "                      # try:\n",
        "                      print(\"passing kwargs from main looop\")\n",
        "                      evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n",
        "                      # raise Exception(\"over\")\n",
        "                    # except Exception as e:\n",
        "                      # print(e)\n",
        "                      # plt.close()\n",
        "                      # failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "                      # break\n",
        "\n",
        "# for i, dataset_df in enumerate(dataset_dfs):\n",
        "#   for j, col_group in enumerate(dataset_col_group_settings[i]):\n",
        "#     tiers = 2\n",
        "#     split_percentiles = [.5]\n",
        "#     eps_relaxation = 1\n",
        "#     eps = 2\n",
        "#     relaxation_mode = 1\n",
        "#     epochs = 5\n",
        "#     point_growth_rate = 1\n",
        "#     score_mode = 0\n",
        "#     growth_mode = 0\n",
        "#     ## TODO make sure none of the above have any affect on the outome\n",
        "#     # for tiers in [2]:\n",
        "#     #   for split_percentiles in [[.5], [.7]]:\n",
        "#     #     for eps_relaxation in [1, 4]:\n",
        "\n",
        "#     #       for score_mode in range(1,3):\n",
        "#     #         for growth_mode in range(3):\n",
        "#                 # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n",
        "#     print(datetime.datetime.now())\n",
        "#     print(DATASET_NAMES[i])\n",
        "#     print(col_group)\n",
        "#     print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "#     function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000}\n",
        "#     try:\n",
        "#       evaluate_baselines_parallel(dataset_df, col_group, **function_kwargs)\n",
        "#     except StopIteration as e:\n",
        "#       print(e)\n",
        "#       plt.close()\n",
        "#       failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/MATLAB/R2021a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKWe1WW0vgHY"
      },
      "source": [
        "### Main 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rblVi4fUqT80"
      },
      "source": [
        "%matplotlib inline\n",
        "# (covid_symptom_data, cancer_data, github_sickness_data) = pickle.load(open('datasets.pkl', 'rb'))\n",
        "dataset_dfs = [\n",
        "  # covid_symptom_data,\n",
        "  # cancer_data,\n",
        "  # github_sickness_data,\n",
        "  power_law_data,\n",
        "]\n",
        "DATASET_NAMES = [\n",
        "  # \"COVID synthetic\", \n",
        "# \"Cancer Data\", \n",
        "# \"Github Sickness all cols\",\n",
        "\"Power Law 1k 1col\",\n",
        "]\n",
        "\n",
        "n = 2 #must be graeteer than or equal to 2\n",
        "gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n",
        "dataset_col_group_settings = [\n",
        "  # [[COVID_SYMPTOM_COLS], gen_col_group_split(2, COVID_SYMPTOM_COLS), gen_col_group_split(3, COVID_SYMPTOM_COLS)],\n",
        "  # [  [[\"stage\", \"grid_index\"]], [[\"stage\"], [\"grid_index\"]], ],\n",
        "  # [[github_sickness_data_cols], gen_col_group_split(2, github_sickness_data_cols), gen_col_group_split(3, github_sickness_data_cols)],\n",
        "  [[power_law_data_cols]]\n",
        "]\n",
        "relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n",
        "\n",
        "failed_runs = []\n",
        "for relaxation_mode in [0, 1]:\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  print(\"=\"*100 + \"\\n\\n\")\n",
        "  for i, dataset_df in enumerate(dataset_dfs):\n",
        "    for j, col_group in enumerate(dataset_col_group_settings[i]):\n",
        "      for baseline_version in range(3):\n",
        "        for tiers in [2]:\n",
        "          for split_percentiles in [[.1]]:#[[i*.1] for i in range(1, 4)]:\n",
        "            for eps in [0.01]:#, 0.1, 1]:\n",
        "              # relaxation_mode = 1\n",
        "              epochs = 5\n",
        "              growth_mode = 0\n",
        "              score_mode = 1\n",
        "              eps_relaxation = 1\n",
        "              epoch_subdivisions = 2\n",
        "              num_repititions = 8\n",
        "              # for score_mode in range(1,3):\n",
        "              # for growth_mode in [1]:\n",
        "              for output_counts in [True, False]:\n",
        "                for epoch_subdivisions in range(1,10, 3):\n",
        "                  # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n",
        "                      print(datetime.datetime.now())\n",
        "                      print(DATASET_NAMES[i])\n",
        "                      print(col_group)\n",
        "                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n",
        "                      'growth_mode': growth_mode,\n",
        "                      'k': 1000,\n",
        "                      'score_mode' : score_mode,\n",
        "                      'eps': eps,\n",
        "                      'tiers': tiers,\n",
        "                      'split_percentiles': split_percentiles,\n",
        "                      'epochs': epochs,\n",
        "                      'encode_acc': 0,\n",
        "                      'encode_mode': 0,\n",
        "                      'relaxation_mode': relaxation_mode,\n",
        "                      'eps_relaxation': eps_relaxation,\n",
        "                      'epoch_subdivisions': epoch_subdivisions,\n",
        "                      'init': 1,\n",
        "                      'step_sz': 50000,\n",
        "                      'num_repititions': num_repititions,\n",
        "                      'output_counts': output_counts,\n",
        "                      'BASELINE_VERSION': baseline_version,\n",
        "\n",
        "                      }\n",
        "                      try:\n",
        "                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n",
        "                      # raise Exception(\"over\")\n",
        "                      except Exception as e:\n",
        "                        print(e)\n",
        "                        plt.close()\n",
        "                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "                        break\n",
        "\n",
        "# for i, dataset_df in enumerate(dataset_dfs):\n",
        "#   for j, col_group in enumerate(dataset_col_group_settings[i]):\n",
        "#     tiers = 2\n",
        "#     split_percentiles = [.5]\n",
        "#     eps_relaxation = 1\n",
        "#     eps = 2\n",
        "#     relaxation_mode = 1\n",
        "#     epochs = 5\n",
        "#     point_growth_rate = 1\n",
        "#     score_mode = 0\n",
        "#     growth_mode = 0\n",
        "#     ## TODO make sure none of the above have any affect on the outome\n",
        "#     # for tiers in [2]:\n",
        "#     #   for split_percentiles in [[.5], [.7]]:\n",
        "#     #     for eps_relaxation in [1, 4]:\n",
        "\n",
        "#     #       for score_mode in range(1,3):\n",
        "#     #         for growth_mode in range(3):\n",
        "#                 # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n",
        "#     print(datetime.datetime.now())\n",
        "#     print(DATASET_NAMES[i])\n",
        "#     print(col_group)\n",
        "#     print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "#     function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000}\n",
        "#     try:\n",
        "#       evaluate_baselines_parallel(dataset_df, col_group, **function_kwargs)\n",
        "#     except StopIteration as e:\n",
        "#       print(e)\n",
        "#       plt.close()\n",
        "#       failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E26g244RyOFJ"
      },
      "source": [
        "import pprint\n",
        "pprint.pprint(failed_runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV7Qa0IcvkOK"
      },
      "source": [
        "### Main 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfNfhRktOl39"
      },
      "source": [
        "for i, dataset_df in enumerate(dataset_dfs):\n",
        "  for j, col_group in enumerate(dataset_col_group_settings[i]):\n",
        "    for baseline_version in range(3):\n",
        "      for tiers in [2]:\n",
        "        for split_percentiles in [[i*.1] for i in range(1, 10)]:\n",
        "          eps = 2\n",
        "          relaxation_mode = 1\n",
        "          epochs = 5\n",
        "          growth_mode=1\n",
        "          score_mode=0\n",
        "          eps_relaxation= 1\n",
        "          # for eps_relaxation in [1, 4]:\n",
        "\n",
        "          for eps_relaxation in [1, 4]:\n",
        "                      print(datetime.datetime.now())\n",
        "                      print(DATASET_NAMES[i])\n",
        "                      print(col_group)\n",
        "                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n",
        "                      try:\n",
        "                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n",
        "                      except Exception as e:\n",
        "                        print(e)\n",
        "                        plt.close()\n",
        "                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "                        break\n",
        "          for score_mode in range(0,3):\n",
        "                      print(datetime.datetime.now())\n",
        "                      print(DATASET_NAMES[i])\n",
        "                      print(col_group)\n",
        "                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n",
        "                      try:\n",
        "                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n",
        "                      except Exception as e:\n",
        "                        print(e)\n",
        "                        plt.close()\n",
        "                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "                        break\n",
        "          for growth_mode in range(3):\n",
        "                      print(datetime.datetime.now())\n",
        "                      print(DATASET_NAMES[i])\n",
        "                      print(col_group)\n",
        "                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n",
        "                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n",
        "                      try:\n",
        "                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n",
        "                      except Exception as e:\n",
        "                        print(e)\n",
        "                        plt.close()\n",
        "                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n",
        "                        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2BrWbEmGBIK"
      },
      "source": [
        "import pprint\n",
        "pprint.pprint(failed_runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGO9hacPtluy"
      },
      "source": [
        "# Test IDLDP\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWb1n2XittGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24bc2023-7d5b-49c0-bc20-66d1f479ce3d"
      },
      "source": [
        "import math\n",
        "from RR_RAPPOR import RAPPOR\n",
        "import numpy as np\n",
        "from IDLDP.pg1 import IDLDP\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k = 1000 #absz\n",
        "n = 10000\n",
        "elements = range(0,k) #ab\n",
        "lbd = 0.8 #parameter for geometric dist\n",
        "eps = 1 # privacy_para\n",
        "prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n",
        "#prob = [1/float(k)] * k\n",
        "in_list = np.random.choice(elements, n, p=prob) #input symbols\n",
        "\n",
        "\n",
        "# idldp params\n",
        "privacy_budget = [1, 1.2, 2]\n",
        "tier_split_percentages = [.05, .05, .9]\n",
        "domain_size = k\n",
        "total_records = n\n",
        "opt_mode = 1\n",
        "idldp = IDLDP()\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(nrows=5, ncols=1) \n",
        "fig.set_size_inches(8.5, 10)\n",
        "\n",
        "\n",
        "for opt_mode in range(5):\n",
        "  print(f\"checking opt mode {opt_mode}\\n\\n\")\n",
        "  X, pred_MSE, config = idldp.gen_perturbation_probs(\n",
        "    eps,\n",
        "    privacy_budget,\n",
        "    tier_split_percentages=tier_split_percentages,\n",
        "    domain_size=k,\n",
        "    total_records = n,\n",
        "    opt_mode = opt_mode,\n",
        "  )\n",
        "\n",
        "\n",
        "  rappor = RAPPOR(k,eps, config=config)\n",
        "  out_list = rappor.id_ldp_perturb(in_list, **config)\n",
        "  outp,temp = np.histogram(out_list,range(k+1))\n",
        "  prob_est = rappor.decode_counts(outp,n) # estimate the original underlying distribution\n",
        "  print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
        "  print (\"prob_sum: \", str(sum(prob_est)))\n",
        "  prob_est = rappor.decode_counts(outp,n,1) # estimate the original underlying distribution\n",
        "  ax[opt_mode].plot(elements,prob)\n",
        "  ax[opt_mode].plot(elements,prob_est)\n",
        "  #plt.plot(prob_est)\n",
        "  print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
        "  print (\"prob_sum: \", str(sum(prob_est)))\n",
        "  # plt.pause(0.1)\n",
        "# plt.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/MATLAB/R2021a\n",
            "IDLDP INIT SUCCESSFUL, toolboxes added\n",
            "checking opt mode 0\n",
            "\n",
            "\n",
            "{'privacy_budget': [1, 1.2, 2], 'n_tiers': 3, 'tier_split_percentages': [0.05, 0.05, 0.9], 'domain_size': 1000, 'total_records': 10000, 'tier_indices': [array([ 33, 199, 799, 176, 212, 685, 735, 801, 341, 250, 619, 150, 752,\n",
            "       972,  96, 744, 477, 654, 258, 149, 974, 629, 420, 973, 929, 413,\n",
            "       701, 138,  81, 713, 830, 241, 580,  77, 171, 375, 340, 908, 444,\n",
            "       210, 498, 776, 418, 181, 778, 367, 687, 722,  41, 751]), array([893, 745, 366, 783, 300, 294, 408, 207, 284, 642, 462, 235, 883,\n",
            "       205, 501, 473,   3, 379, 958,  54, 454, 829, 200, 756, 556, 809,\n",
            "       941, 485, 784, 531, 788, 458, 221,   1, 449, 179, 329, 363, 715,\n",
            "       955, 669, 166, 641, 381, 743, 108, 553, 382, 567, 911]), array([679, 734, 612,  68, 709, 823, 402, 347, 209, 980, 351, 223, 374,\n",
            "       350, 937, 671, 796, 183, 493, 943, 607, 737, 405, 110, 768, 680,\n",
            "       919, 710, 589, 841, 934, 821, 467, 450, 581, 177, 120, 800, 436,\n",
            "       828, 320, 325, 760, 995, 716, 707, 470, 355, 116, 923, 545, 509,\n",
            "        78, 990, 298, 285, 192, 731, 291, 877, 129, 487, 993, 777, 169,\n",
            "       762, 627, 585, 932, 703, 264,  43,  42,  99, 792, 380, 564, 723,\n",
            "       571, 970, 224, 428, 266, 100, 994, 939, 814, 944, 650, 569, 924,\n",
            "        80, 953,  61, 550, 372, 711, 390, 636, 289, 536, 652, 321, 332,\n",
            "       905, 638, 659, 443, 926,  21,  56, 928, 369, 148, 188, 270,  79,\n",
            "       513, 388, 373,  15, 257, 278, 982,  14, 126, 617, 857, 769, 140,\n",
            "       354,  20, 238, 630, 646, 574, 719, 432, 640, 132, 322, 981, 518,\n",
            "       326,  89, 142, 699, 912, 534, 302,  63, 245,  35, 983, 866, 836,\n",
            "       753, 865, 337, 293, 592, 486, 500, 890, 344, 261, 448, 593, 613,\n",
            "       117, 124,  71, 525,   6, 742, 345,  70, 101, 725, 281, 688, 299,\n",
            "       339, 280, 191, 441, 563, 754, 404, 670, 672, 515, 948, 186, 740,\n",
            "        24, 482, 867, 463,  12, 476, 643, 190, 353, 548, 503, 193, 696,\n",
            "       965,  59, 601, 856, 524, 887, 596, 316, 996, 873,  64, 674, 286,\n",
            "       774, 426, 481, 226, 962,  17, 255,  40,   2, 761,  29, 880, 755,\n",
            "       910, 881, 824, 611, 506,  58, 578, 931, 530, 686, 851, 661, 276,\n",
            "       196, 139, 435, 938, 338, 227, 655, 622, 307, 872, 949, 361, 936,\n",
            "       427, 766, 992, 535, 391, 874, 541, 813, 472, 551, 330, 951,  67,\n",
            "       656, 750, 251, 539, 540, 357, 868, 987, 203, 859, 437, 267, 362,\n",
            "       429, 616, 211, 274, 842, 898, 850,  22, 933, 576, 147, 902, 712,\n",
            "       123, 312, 216, 621, 649, 105, 384, 786, 438, 492, 846, 555, 631,\n",
            "       434, 835, 219, 237, 184, 144, 162, 459,  46, 811, 327, 807, 133,\n",
            "         9, 466, 479, 151, 514, 683, 315,  11, 816, 817, 464, 410, 502,\n",
            "       561, 849, 222, 644, 297, 984, 323,  25, 583, 377, 495, 417, 810,\n",
            "       370, 922, 891, 306, 360, 730, 942,  57, 588, 804, 296, 234, 273,\n",
            "       452, 812, 400, 623,  39, 871, 523, 747, 657, 727, 651, 164, 109,\n",
            "       666, 189, 194, 957, 673,  13, 422, 342,  27, 346, 869, 909, 153,\n",
            "       587, 572, 645, 988,  88, 122, 248, 921, 802,  30, 356, 573, 508,\n",
            "       419, 820, 624, 855, 665, 143, 689, 736, 861, 691, 789, 128, 946,\n",
            "       484, 854, 764, 145, 705, 793, 182, 197,  66, 283, 614, 303, 215,\n",
            "       324, 597, 748, 935, 918, 706, 440, 961,  16, 618,  49, 885, 520,\n",
            "       785, 704, 242, 917, 848,  36, 403, 480, 991, 385,  84,   4, 648,\n",
            "       967, 394, 102, 119, 111, 167, 317,  28, 239, 488, 103, 892, 146,\n",
            "        94, 610, 637, 168, 228, 457, 290, 615, 915,  44, 952,  97, 333,\n",
            "       517, 693, 364, 522, 528, 392, 158, 634, 157, 155,  76, 794, 208,\n",
            "       997, 475, 729, 609, 698, 843, 406, 336, 599, 233, 424, 668, 262,\n",
            "       875,  45, 975, 697, 838, 815, 598, 770, 594, 678, 708, 232, 559,\n",
            "       497,  74, 832, 156, 414, 844, 489, 653, 985, 864,  34,   8,  10,\n",
            "       940, 694, 446, 159, 519, 511, 125, 595, 348, 505, 568, 504, 328,\n",
            "       243, 798, 779, 425, 529, 805, 265, 660, 839,  90, 421, 445,  60,\n",
            "       309, 728, 447,  93, 496, 460, 894, 202, 554,   5, 175, 625, 407,\n",
            "        91, 104, 667, 773, 904, 582, 565, 395, 178, 136, 430, 252, 757,\n",
            "       664, 416, 797, 692, 767, 895, 552, 963, 113, 724, 647, 229, 876,\n",
            "       927, 717, 494,  53, 112, 978, 230, 960, 277, 968, 684, 401, 837,\n",
            "       165, 442, 127, 969, 906, 246, 387, 371, 916, 930, 819, 741, 549,\n",
            "       160, 979, 833, 304, 398, 542, 107, 602, 790, 365, 682,  69, 732,\n",
            "       526, 319, 538, 999,  47, 772, 721, 562,  98, 853, 358, 527, 451,\n",
            "        82, 499, 543, 547, 560, 249, 154, 343, 899, 897, 763, 826, 474,\n",
            "       510, 490, 236, 231, 214, 714, 152,  62, 161, 739, 920,   7, 544,\n",
            "       896, 733, 986,  26, 383, 439, 822,  18, 998, 349, 950, 781, 628,\n",
            "        75,  37, 399, 626, 271, 945, 213, 352, 677, 695, 663, 675,  48,\n",
            "       453, 240, 512, 269, 282, 204, 305, 818, 546, 532, 964, 507,  73,\n",
            "       700, 720, 852,  38, 676, 301, 516, 220, 803, 780, 247, 971, 584,\n",
            "       163, 907, 263,  65, 604, 662, 253, 831,  83, 141, 847, 313, 397,\n",
            "        86, 180, 956, 566, 195, 206, 882, 471, 884, 288, 879, 718, 409,\n",
            "        87, 415, 825, 260, 456, 521, 570, 608, 198, 311, 130, 368, 386,\n",
            "       900, 863,  52,  95, 586, 966, 558, 603, 318, 600, 633,   0, 254,\n",
            "       396, 201, 217, 575, 455,  51, 244, 308,  31,  50, 771, 134, 256,\n",
            "       759, 287, 172, 726, 491, 334, 959, 174, 187, 557, 954, 870, 782,\n",
            "       292, 791,  72, 433, 577, 115, 225,  23, 106, 886, 635, 690, 393,\n",
            "       903, 738, 889, 787, 758, 331, 914, 259, 335, 295, 775, 135, 389,\n",
            "        19, 947,  55, 465, 632, 218, 681, 275, 590, 658, 376,  85, 827,\n",
            "       989, 976,  92, 749, 137, 461, 411, 483, 808, 431, 746, 423, 888,\n",
            "       478, 279, 272, 765,  32, 913, 533, 359, 840, 121, 579, 860, 639,\n",
            "       862, 469, 806, 131, 314, 901, 118, 268, 977, 185, 412, 845, 378,\n",
            "       795, 702, 605, 834, 925, 591, 114, 537, 173, 170, 878, 310, 620,\n",
            "       606, 858, 468])], 'alpha': array([ 50,  50, 900])}\n",
            "starting optimization for opt mode 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 distance:  1.2800000000000002\n",
            "prob_sum:  1.0\n",
            "l1 distance:  1.6\n",
            "prob_sum:  1.0\n",
            "checking opt mode 1\n",
            "\n",
            "\n",
            "{'privacy_budget': [1, 1.2, 2], 'n_tiers': 3, 'tier_split_percentages': [0.05, 0.05, 0.9], 'domain_size': 1000, 'total_records': 10000, 'tier_indices': [array([253, 853, 444, 286, 217, 222, 686, 269, 799, 731, 976, 703,  97,\n",
            "       600, 925, 811, 259, 378, 448, 776, 424, 144, 667, 533, 360, 338,\n",
            "       851, 818, 336, 125, 438, 176, 755,  26, 204, 834, 121, 101, 757,\n",
            "       939, 627, 524, 688,  51,  53, 866,  20, 987, 574, 307]), array([276, 201, 729, 546, 238, 908,  89, 102,  24, 572, 914, 419, 498,\n",
            "       713, 620, 734, 871, 702, 767, 705, 531, 928, 805,   0, 223, 711,\n",
            "        76, 916, 195, 323, 920, 900, 380, 228, 539, 362, 401, 883, 162,\n",
            "       495, 676, 471, 895, 591, 433, 358, 347, 758, 785, 944]), array([997, 296, 100, 864, 328, 617, 527, 795, 183, 442, 625,   1, 656,\n",
            "       320, 467, 116, 949, 172, 108, 519, 428, 434, 910, 961, 452, 451,\n",
            "       348, 454, 132, 138, 468, 862, 957,  35, 940, 714, 356, 214, 506,\n",
            "       232, 411, 329, 234, 447, 270, 938,   4, 841, 292, 543, 874, 103,\n",
            "       564, 588, 417, 792, 237, 488,  85, 587, 280,  68,  40, 355, 158,\n",
            "       440, 791, 367, 863, 652, 542, 135, 898, 421, 182, 747, 489, 459,\n",
            "       606, 552, 984, 766,  43, 581,  90,   9, 945, 257, 700, 840, 894,\n",
            "       662, 221, 491,  79, 312,  12, 892, 934,  19,  18, 521, 745, 981,\n",
            "       331, 649, 439, 960, 469, 647, 973, 787, 163, 556,  13, 857, 958,\n",
            "       242, 199, 322, 350, 975, 209, 854, 921, 426,  48, 166, 924, 847,\n",
            "       302, 880, 191, 299, 239, 330, 610, 951, 394,  82, 576, 342, 137,\n",
            "       112, 518, 515, 548, 400, 333, 835, 991, 580, 826, 654, 152,  63,\n",
            "       340, 213, 829, 266, 590, 133, 475, 952, 769, 174, 803,  77, 295,\n",
            "       258, 626, 505, 399, 512, 390, 186,  49,  87, 381, 510, 382, 309,\n",
            "       397, 289, 507, 508, 501, 819, 786, 668, 422, 461, 463, 605, 832,\n",
            "       846, 728, 633, 699, 970, 224, 985, 681, 462,  39, 996, 267, 845,\n",
            "       718, 790, 418,   8, 967, 980, 584, 437, 601, 244, 493, 992, 913,\n",
            "       429, 919, 549, 550, 190,  29, 412, 169, 208, 160, 820, 241,  94,\n",
            "       691, 646, 754, 770, 480, 470, 603, 744,  17, 796, 777, 165, 272,\n",
            "       248, 684, 275, 609, 260, 346, 233, 869, 274,   2, 674, 540, 575,\n",
            "       582, 648, 310, 935, 858,  22, 635, 481, 282, 736, 965, 555, 357,\n",
            "       345, 410, 351, 598, 373, 889, 772, 768, 164, 455, 902, 697, 860,\n",
            "       569,  34, 251, 802, 723, 530, 836, 441, 283, 943, 942, 657, 145,\n",
            "       185, 483, 153, 781, 432, 142, 936, 568, 612, 178, 117, 453, 212,\n",
            "       478, 492, 558, 704, 619, 372, 264, 497, 923, 304, 778, 800, 930,\n",
            "       124, 801, 446, 414, 931, 484, 565, 159, 250, 255,  41, 252,  21,\n",
            "       219, 187, 551, 739, 326, 637, 450, 906,  30,  57, 897, 520, 712,\n",
            "       761, 349, 852, 870, 343, 139, 673, 859, 395,  78, 911, 824, 406,\n",
            "       268, 398, 660, 534, 388, 701, 545, 285, 907, 256, 161, 559, 583,\n",
            "       631, 123, 918, 850, 106, 107, 955,  25, 523, 634, 513, 525, 827,\n",
            "       325, 771, 604, 216, 155, 278,   5, 586, 842,  58, 341, 670, 644,\n",
            "       535, 638, 104, 989, 363, 716, 708,  52, 759, 815, 563,  99, 431,\n",
            "        88, 261,  55, 838, 301, 316, 193, 993, 821, 369, 822, 773, 655,\n",
            "       196, 823, 396, 364, 246,  74, 313, 693, 966, 305, 808, 666, 669,\n",
            "       661, 303, 719,  44, 368, 887, 215, 403, 188, 263, 151, 717, 375,\n",
            "         3, 788, 749, 807, 265, 487, 321, 793, 126, 474, 407,  59, 327,\n",
            "       210, 192, 436, 730, 235, 653, 614, 571,  69, 490, 377, 748, 813,\n",
            "       982, 798, 197, 566, 319, 675, 300, 240,  96, 332, 466, 672, 570,\n",
            "       379, 170, 557, 855, 622, 709, 389, 206, 692, 843, 868, 915, 618,\n",
            "       245, 881,  61, 830, 926, 671, 528, 839, 141, 371,  28, 361, 231,\n",
            "       589, 746, 797, 599, 903, 933, 690, 122, 334, 886, 317, 482, 409,\n",
            "        38, 632, 640, 486, 812, 306, 593, 756, 140, 810, 831, 884, 385,\n",
            "       680,  67, 782, 365, 922, 752, 678, 932, 629, 456, 611, 127, 636,\n",
            "       663, 630, 177, 315, 682, 794,  98, 998, 522, 146, 877, 119, 202,\n",
            "       287, 726, 128,  36, 352, 354, 114, 888, 173, 500, 760, 873, 615,\n",
            "       577, 337, 502, 175, 460, 156, 393, 727, 205, 687, 628, 405, 715,\n",
            "       912, 947,   6, 592, 659, 969, 544, 899, 742, 971, 602,  45,  60,\n",
            "       449, 890, 972, 243,  62, 578, 789, 273, 737, 538, 763,  32, 404,\n",
            "       391, 979, 207, 978, 554, 959, 948, 585, 218, 905, 149, 751, 828,\n",
            "       879,  80, 893, 974, 597, 148, 963, 249, 154,   7, 740,  50, 946,\n",
            "        56,  95, 335, 623, 180, 547, 105,  83, 762, 254, 443, 136, 529,\n",
            "        81, 764, 109, 848, 344, 696, 194, 995, 901,  14, 279, 464, 203,\n",
            "       595, 732, 650,  54,  42, 387, 833, 844, 413,  27,  93, 607, 956,\n",
            "       227, 573,  86, 641, 271, 937, 876, 986,  70, 733,  91, 639, 383,\n",
            "       503, 861, 750, 384,  92, 707, 698, 990, 416, 499, 724, 198, 658,\n",
            "       816, 189, 954, 111, 664, 741, 181, 875, 775, 780, 856, 541, 113,\n",
            "       514, 143, 120, 695, 392, 929, 374, 896, 226, 809, 477, 694, 689,\n",
            "       738, 494, 882, 445, 941, 427, 298, 885, 608, 706, 849, 721, 134,\n",
            "        72, 211,  10, 983, 783, 784, 476, 457, 753, 430, 353, 297, 324,\n",
            "       651,  75, 817, 308, 553, 200, 435,  37, 281, 537, 262, 294, 806,\n",
            "       891,  33, 386,  73, 118, 909, 950, 837,  65, 504, 735, 472, 526,\n",
            "       621, 624, 425, 517, 765, 999, 293, 423, 562, 168, 964, 129, 722,\n",
            "       150, 167, 613, 291, 779, 376, 479, 318,  16, 473, 402, 774, 643,\n",
            "       968,  66,  46, 509, 904, 225, 311, 642,  84, 220, 994, 288, 532,\n",
            "       415, 339, 536, 130, 865, 953, 359, 277, 804, 496, 579, 878, 720,\n",
            "        15, 962, 596, 247, 743, 977, 710,  31, 927, 567,  11, 290, 677,\n",
            "       561, 131, 485, 408, 115, 171, 420, 867, 516, 184, 917, 110, 665,\n",
            "       872,  23, 616, 229, 679, 825,  47, 236, 366, 465, 147, 594, 314,\n",
            "       560, 179, 157, 814, 645, 230, 370, 725, 683, 511, 988, 284,  64,\n",
            "       685,  71, 458])], 'alpha': array([ 50,  50, 900])}\n",
            "starting optimization for opt mode 1\n",
            "[1.  1.2 2. ]\n",
            "[ 50  50 900]\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 distance:  1.2800000000000002\n",
            "prob_sum:  1.0\n",
            "l1 distance:  1.6\n",
            "prob_sum:  1.0\n",
            "checking opt mode 2\n",
            "\n",
            "\n",
            "{'privacy_budget': [1, 1.2, 2], 'n_tiers': 3, 'tier_split_percentages': [0.05, 0.05, 0.9], 'domain_size': 1000, 'total_records': 10000, 'tier_indices': [array([923, 424, 727, 769, 508,  53, 461,  24, 477, 566, 169, 601, 851,\n",
            "       401, 813, 352, 592, 391,  77, 856, 958, 443, 121, 165, 791, 669,\n",
            "       965, 772, 704, 368, 623, 603, 700, 506, 333, 836, 781, 915, 502,\n",
            "       235, 545, 562,  10, 154, 774, 746, 114, 259, 862,   6]), array([794, 753, 449, 123, 608, 755, 638, 912, 720, 597, 382, 835, 309,\n",
            "       575, 280, 648, 298, 194, 480, 284, 315, 763, 701, 207, 708, 485,\n",
            "       342, 952,  15,  91,  61, 236, 410, 773, 160,  46, 552, 156, 413,\n",
            "       129, 882, 688, 947, 703, 128, 913,  71, 620, 405, 292]), array([326, 132, 829,  96,  88, 911, 988, 460, 553, 891, 377, 784, 464,\n",
            "       826, 228, 201, 463, 839, 353,  35, 627, 871, 980,  92, 782, 271,\n",
            "       399, 246, 266, 418, 193, 997, 846, 853, 560, 795, 364, 994, 293,\n",
            "       759, 214, 384, 252,   4, 539, 796, 409, 656, 215, 765, 478, 277,\n",
            "       221, 527, 776, 164, 248, 415, 476, 375, 350, 522, 971, 843, 110,\n",
            "       453,  95, 760, 976, 492,  22, 130,  82, 681, 108, 584,   2, 673,\n",
            "       327, 684, 378, 467,  40, 275, 734, 572, 875, 963,  21,   1, 857,\n",
            "       617, 550, 944, 439, 986, 644, 127, 731, 858, 636, 256, 336, 893,\n",
            "       676, 182, 285, 109, 641, 917, 548,  31, 996, 745, 170, 118, 205,\n",
            "       421, 295, 526, 374, 568, 698, 934, 147,  85, 260, 960, 302, 749,\n",
            "       991,  75, 967, 161, 936, 692, 639,  28, 657, 950,  60, 104, 628,\n",
            "        97, 146,  57, 361, 573,  93, 273, 695, 626,  32,  25, 334, 423,\n",
            "       606, 974, 778, 120, 832, 447, 319, 186, 909, 863, 962, 985, 654,\n",
            "       340, 380, 212, 457, 804, 145, 306, 297, 233, 581, 265, 866, 546,\n",
            "       716, 373, 388, 483, 286, 301, 358, 426, 888, 969, 101, 359, 869,\n",
            "       458, 887, 653, 922, 921, 600, 547, 234,  81, 665, 501, 998, 226,\n",
            "       470, 983, 951, 396, 369,  86, 602, 938, 347, 531, 798, 245, 403,\n",
            "       504,  39, 505, 491, 282, 571, 908, 167, 881, 210, 664, 812, 685,\n",
            "       990, 808, 543, 809, 211, 890, 135, 642, 834, 838, 828,  48, 356,\n",
            "       604, 385, 803,   0, 578, 393, 733, 646,  55, 845, 687, 432, 482,\n",
            "       689, 355, 634, 873, 351, 283,  67, 386, 106, 544, 178, 806, 833,\n",
            "       142, 799, 475, 586, 209, 743, 198, 793, 874, 307, 371, 425, 126,\n",
            "        65, 697, 496, 379, 576, 625, 690, 844, 143, 972, 615, 680, 589,\n",
            "       331, 696, 594, 989, 766, 365, 208, 322, 801, 740, 619, 525, 593,\n",
            "       899,  18, 761, 831, 381, 693, 436,  42, 503, 430, 678,  17, 431,\n",
            "        38,  68, 537, 699, 825, 797, 469, 886, 523, 175, 232, 920, 827,\n",
            "       880, 919, 509, 472, 940,  83, 538, 330, 290, 148, 717, 820, 775,\n",
            "       635, 149, 456, 416, 867, 694, 354, 554, 124, 840,  79, 310,  43,\n",
            "       387, 417, 254, 451, 953, 171, 718, 137, 479, 868, 564, 274, 735,\n",
            "       742, 343, 136, 317, 877, 499, 556, 349, 984, 532,  56, 595, 529,\n",
            "       162, 559, 465, 468, 444,  29, 964, 406, 313, 677, 981, 841, 445,\n",
            "       184, 549, 767, 172, 658, 190, 954, 363, 311, 278, 515,   5, 139,\n",
            "       218, 390, 107, 939, 202, 577, 705, 312, 398, 318, 679, 757, 926,\n",
            "       848, 376, 192, 370,  72, 332,  62, 299, 655, 264, 441, 402, 666,\n",
            "       570, 337, 244, 725, 372, 719, 800, 916, 933, 459, 802, 321, 878,\n",
            "       736, 816, 647, 314, 517, 948, 611, 540, 555, 756, 661, 158, 906,\n",
            "       968, 744, 821, 959, 395, 663, 847, 852, 219, 392, 807, 294, 674,\n",
            "       224, 200, 860,  34, 131, 937, 849, 786, 904, 632, 898, 199, 383,\n",
            "         7,  73,  13, 422, 328, 250, 471, 651, 174, 929, 237, 243,  14,\n",
            "       133, 448, 702, 579, 814, 970, 721, 487, 524, 507, 291, 348, 533,\n",
            "       739, 176, 220, 785, 897, 789, 287, 907, 191, 914, 792, 751, 270,\n",
            "        49, 253, 779, 715, 865, 652, 730, 930, 941, 157, 613,  59, 710,\n",
            "       640, 446, 650, 675, 183,  74, 528, 649, 818, 512, 811, 134,  66,\n",
            "       621, 153, 316, 569, 150, 325, 645, 159, 771, 565, 433, 879, 672,\n",
            "       901, 764, 263, 876,  26, 585, 777, 262, 510, 519,  52,  84,  30,\n",
            "       662,  90, 660, 536, 790, 928, 240,  78, 580, 238, 854, 163, 979,\n",
            "       345, 204, 261, 630, 494, 119, 596, 534, 622, 817, 668, 815, 783,\n",
            "       610, 400,  89, 629, 910, 289, 152, 300, 151, 411, 927, 125, 185,\n",
            "       995, 122, 748, 738, 141,  33, 511, 276, 438, 180,  69, 455, 304,\n",
            "       943, 272, 894, 188, 497, 892, 179, 870, 754, 225,  41, 864, 885,\n",
            "       518, 498, 341, 206, 329, 488, 762, 805, 434, 977, 227, 308, 758,\n",
            "       607, 366, 437,  94,  70, 323,  27, 588, 288, 837, 987, 412, 189,\n",
            "       305, 279,  54,  99, 429, 999, 138, 557, 973, 883, 966, 115, 362,\n",
            "       686, 567, 551, 896, 946, 598, 993, 320, 732, 389, 420, 667, 599,\n",
            "        12, 168, 889, 737, 239,  37, 177, 258, 810, 166, 360, 249, 255,\n",
            "       144, 394, 452, 338, 397, 247, 830, 850, 770, 859, 473, 903, 945,\n",
            "       269, 140, 408, 267, 925, 787, 713, 855,  23, 724, 631, 957, 542,\n",
            "       427, 489, 714, 516, 102, 213, 112,  50, 618, 722, 741, 612, 335,\n",
            "       474, 367, 230, 344, 116, 428, 486, 633, 339, 842, 711, 614, 932,\n",
            "       591, 520, 495, 346, 296, 949, 241,  20, 884, 691, 268, 824, 514,\n",
            "       462, 484, 103, 590, 616, 251,  44, 706, 203, 671, 303, 723, 956,\n",
            "       541, 561, 729, 709, 419, 100, 155,  98, 683, 978, 440, 196, 195,\n",
            "       823, 257, 563, 768, 197, 982, 905, 895, 113, 454, 961,   9, 558,\n",
            "       707, 931, 682, 712, 942,  58, 624, 747, 187,  45, 513, 583, 357,\n",
            "       924, 521, 281, 752,  63,   8, 955, 435,  16, 111, 587, 181, 788,\n",
            "       992, 117, 490, 466,  11, 574, 222, 861,  47,   3, 609,  36, 229,\n",
            "       481, 450, 822, 530, 637, 780,  76, 216, 404, 414, 500, 493, 750,\n",
            "       902, 659, 872, 975, 105, 442, 935, 670, 223, 582, 324,  64,  51,\n",
            "        80, 217,  87, 231, 643, 819, 726, 535, 407,  19, 173, 918, 728,\n",
            "       242, 900, 605])], 'alpha': array([ 50,  50, 900])}\n",
            "starting optimization for opt mode 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 distance:  1.28\n",
            "prob_sum:  1.0\n",
            "l1 distance:  1.6\n",
            "prob_sum:  1.0\n",
            "checking opt mode 3\n",
            "\n",
            "\n",
            "{'privacy_budget': [1, 1.2, 2], 'n_tiers': 3, 'tier_split_percentages': [0.05, 0.05, 0.9], 'domain_size': 1000, 'total_records': 10000, 'tier_indices': [array([788, 635, 679, 714,  76, 823, 771, 644, 758, 918, 567, 625, 602,\n",
            "       850,  45, 909, 854, 232, 729, 574, 708, 967, 769, 450, 209, 182,\n",
            "       634, 672, 111, 148, 582, 653,  91, 726, 319, 149, 491, 707, 540,\n",
            "       767, 833, 805, 710, 621, 612, 245, 241, 792,   6, 658]), array([668, 160, 646, 970, 835, 896, 989,  21, 779, 898, 678,  18, 692,\n",
            "        52, 556, 479, 501, 947, 474, 251, 102, 721, 200, 882, 740, 630,\n",
            "       215, 579, 919, 588, 511, 878, 320, 910, 344, 249,   1, 419, 723,\n",
            "       834, 889,  81, 133, 203,  19, 819, 143, 912, 560, 597]), array([956, 839, 937, 690, 689, 851, 934, 840, 295, 323, 222, 219, 429,\n",
            "       371,  75, 207,  24,  83,  48, 720, 443, 734, 162, 780, 880, 414,\n",
            "       224, 983, 336, 159, 386, 270, 534, 405, 700,  88, 616,  66, 471,\n",
            "       338, 390, 442, 899, 212, 229, 824, 533, 116, 849, 968, 945, 869,\n",
            "       718, 946,  98, 691, 196, 715, 592, 403, 282, 167, 559,  60, 699,\n",
            "       786, 290, 754, 994, 401, 221, 948,  13, 928, 900, 799, 685, 628,\n",
            "       781, 146,  94,  26, 712, 502, 227, 791, 891, 140, 437, 641, 261,\n",
            "       665, 172, 861, 669, 586, 455, 847, 570, 226,  67, 235, 375, 230,\n",
            "        84, 611, 527, 225, 801, 930, 881, 785,  63, 953, 155, 879, 859,\n",
            "       294, 757, 307, 448, 139, 980, 266,  39, 661, 131, 725, 841, 991,\n",
            "       817, 965, 253, 379, 655, 870, 739,  14, 591, 728, 528, 340, 618,\n",
            "       125, 508, 228, 571, 461,   7, 703, 575, 530,  79, 254, 187, 852,\n",
            "       519, 385, 157, 169, 361,  53, 114, 463, 422, 164, 376, 744, 449,\n",
            "       810, 517, 753,  78, 814, 170, 863, 557, 205, 339, 822, 278, 274,\n",
            "       887, 431, 776, 620, 742, 593, 353, 787, 306, 942, 643, 347,  73,\n",
            "       526, 457, 622, 453, 421, 687,  41, 399, 515, 134, 793, 255, 639,\n",
            "       832, 688, 277, 124, 118, 163, 830, 719, 775, 258, 755, 867, 539,\n",
            "       648,  56, 609, 189, 961, 367, 466, 305, 291,  89,  90, 789, 727,\n",
            "       590, 318, 933, 871, 844, 193, 363, 749, 440, 877,   3, 709, 873,\n",
            "       914, 686, 998, 201, 905, 513, 240, 865, 798, 120, 211, 903, 763,\n",
            "       977, 394, 682, 484, 180, 321, 623, 214, 218, 389, 828, 656, 561,\n",
            "         4, 341, 562, 996, 387, 468, 804, 377, 119, 248, 858, 895, 936,\n",
            "         2, 997, 498, 355, 935, 963, 366, 473, 327, 836, 483, 489, 976,\n",
            "         5, 663, 382, 186, 626, 292, 284, 838, 897, 194, 902, 487,  47,\n",
            "       538,  17, 743, 495, 848, 974, 185, 372, 420, 106, 470, 233, 525,\n",
            "       615, 768, 267, 676, 171, 486, 223, 504, 667, 541, 921, 433, 764,\n",
            "        59, 736, 275, 650, 604, 210, 883, 901, 259, 702, 192, 348, 356,\n",
            "       584, 636, 494, 842, 247, 317, 195, 671, 446, 237, 128,  11, 493,\n",
            "       465, 404, 238, 145, 257, 152, 546, 265,  58, 383,  54, 594, 286,\n",
            "       179, 555, 444, 913, 312, 647,  22, 748, 920, 298, 432, 137, 165,\n",
            "       972, 281, 536, 660, 929, 964, 263, 438, 283, 369, 762, 922, 770,\n",
            "       142, 697, 154, 716, 293, 864, 392, 441, 362, 717, 821, 649, 506,\n",
            "       957, 908, 790, 314, 532, 796, 995, 322, 509, 408, 130, 837, 674,\n",
            "       573, 772, 797, 202, 469, 512, 566, 860, 141, 950, 608,  15, 988,\n",
            "       168, 992, 760, 436, 818, 184, 244, 971, 400, 462, 260, 585, 304,\n",
            "        82,  40, 331, 503,  42, 117, 816, 926, 289, 939, 551, 741, 855,\n",
            "       773, 572, 177, 614,  85, 944, 750, 730, 412, 651, 213, 256,  93,\n",
            "         9,   0, 365, 547, 885, 670, 147, 191, 724, 675, 578, 966, 892,\n",
            "         8, 101, 434, 445, 982, 166, 358, 868, 638, 510, 807, 243, 492,\n",
            "       310, 388, 558, 276, 104,  69, 711,  64, 476, 637, 297, 598, 866,\n",
            "       357, 364, 535, 384, 127, 568, 979, 601, 188, 123, 439, 411, 732,\n",
            "       490, 694,  35, 589, 459, 938, 884, 190, 239, 911,  32, 460, 960,\n",
            "       747, 595, 112, 904, 505, 395, 472, 427, 969, 103, 659, 288, 378,\n",
            "       499, 345, 631,  51, 158, 204, 198, 923, 497, 973, 520, 342, 299,\n",
            "       874, 617, 349, 645, 745,  70, 407,  87, 893, 603, 943, 428, 523,\n",
            "       330, 999, 250, 313, 151,  16, 303, 368,  31, 812, 308, 583, 410,\n",
            "       456,  86, 843,  71,  80, 802, 418, 138, 549, 216, 264,  34, 271,\n",
            "       981, 932, 272, 906, 302,  96, 577, 199, 984, 475, 704, 351, 516,\n",
            "       862, 673,  33, 652, 803, 337, 454, 565, 987, 153, 110, 890, 958,\n",
            "       373, 664,  29, 975, 610, 354, 452, 600, 613, 398,  55, 705, 391,\n",
            "       488, 132, 795, 985,  72, 808, 800, 605, 464, 554, 343,  46, 268,\n",
            "       713, 500, 397, 619, 518, 731, 681, 402, 430, 941, 783, 311, 954,\n",
            "       552, 657, 236, 537, 751, 426, 580,  23, 121, 181, 624, 765,  38,\n",
            "       927, 115, 545, 406, 738, 144, 735, 231, 809, 886, 826, 986, 380,\n",
            "       126, 542, 374, 778,  20, 393, 424, 917, 684, 478,  61, 752, 531,\n",
            "       122, 252, 825, 156, 301, 285,  37, 698, 756, 962,  99, 642, 925,\n",
            "       417, 820, 632,  43, 940, 269, 485, 197, 550, 183, 955, 553,  95,\n",
            "       813, 794, 695, 458, 246,  62, 827, 242, 806, 326,  49, 666, 782,\n",
            "       607, 564,  57, 831, 220,  44, 872, 350, 129, 481, 109, 951, 876,\n",
            "       761, 352, 359, 496, 280, 334, 978, 208, 480, 856, 477, 737, 654,\n",
            "       507, 409, 324, 857, 273, 279, 113, 435, 100, 381, 759, 329,  12,\n",
            "       105, 587, 217, 423, 683, 952, 916,  92, 777, 316, 300,  74, 529,\n",
            "       333, 662,  50, 287, 563, 370, 234, 627, 784, 993, 596, 888, 425,\n",
            "       990, 599, 262, 173, 677, 696, 733, 706, 328,  27, 346, 581, 548,\n",
            "       746,  77, 467, 829, 811, 633, 360,  36, 325, 107, 315, 524, 959,\n",
            "       931, 701, 845, 335, 175, 206, 521, 415, 447,  25, 176, 569, 416,\n",
            "       544, 413, 135, 924,  30, 949, 178, 576, 853, 894, 915, 680,  97,\n",
            "       774, 766,  68, 396, 332, 522, 815, 482, 161, 309, 150, 629, 722,\n",
            "       640, 174,  28, 296, 543, 108, 606,  10, 451, 514, 136, 875, 846,\n",
            "        65, 693, 907])], 'alpha': array([ 50,  50, 900])}\n",
            "starting optimization for opt mode 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 distance:  1.28\n",
            "prob_sum:  1.0\n",
            "l1 distance:  1.6\n",
            "prob_sum:  1.0\n",
            "checking opt mode 4\n",
            "\n",
            "\n",
            "{'privacy_budget': [1, 1.2, 2], 'n_tiers': 3, 'tier_split_percentages': [0.05, 0.05, 0.9], 'domain_size': 1000, 'total_records': 10000, 'tier_indices': [array([393, 720, 313, 691, 606, 610, 806, 371, 478, 271, 959, 937, 897,\n",
            "       552, 480, 168, 138, 441, 199, 825, 630, 977, 548, 264, 635, 814,\n",
            "       628, 793, 967,  27, 274,  12, 500,  56, 534, 789, 196, 592, 640,\n",
            "       618, 812, 312, 957, 555, 859, 869, 389, 518, 470, 788]), array([128,  67, 886, 430, 546, 140, 589, 801, 612, 250, 892, 791, 493,\n",
            "       368,  17, 494, 536, 447, 713, 339, 376, 288, 442, 116, 216, 355,\n",
            "       651, 718, 292, 839, 509, 782,  92, 994, 310,  97, 827, 182, 248,\n",
            "       311, 456, 421, 187, 972, 659,  59, 143, 514, 451, 375]), array([889, 337,  39, 559, 181, 760, 978, 286, 725, 736, 673, 158, 797,\n",
            "       818, 475, 623,  66, 273, 266, 888, 190, 111, 492, 783, 347, 202,\n",
            "       561, 855, 118,  38,  30, 155, 523, 294, 445, 213, 124, 672, 291,\n",
            "       461, 794, 350, 666, 970, 648,  79, 563, 195, 748, 858, 674, 176,\n",
            "       306, 654, 334, 980, 684, 454, 372, 705, 737, 751, 908, 817, 297,\n",
            "       925, 988, 683, 175, 639, 834, 468, 944, 309, 487, 418,  90, 921,\n",
            "       655, 450,   3, 807, 831, 240, 729, 538, 798,  41, 501, 282, 521,\n",
            "       950, 169, 416, 349, 457, 413, 533, 154, 685, 519, 222, 912, 943,\n",
            "       974, 675, 724, 766, 593, 318, 952, 934, 381, 126, 961, 792, 787,\n",
            "       226, 159,   8, 290, 868, 595, 909, 270, 829, 557, 391, 167, 745,\n",
            "       301, 211, 667, 707, 656, 601, 890, 824,   2, 906, 572, 423, 496,\n",
            "       756,  96, 916, 776, 620, 896, 767, 427, 594, 922, 775, 965, 320,\n",
            "       161, 464, 883, 969, 277, 846, 531, 723, 842, 383, 229, 551, 322,\n",
            "       722, 733,  73, 249, 262, 251,  11, 830, 472,  70, 754, 810, 924,\n",
            "       717,  13, 363, 577, 762, 336, 885, 598, 529, 741, 866, 979, 732,\n",
            "       508, 284, 759,  28, 621, 838, 779, 491, 865, 268, 329,  88, 466,\n",
            "       321, 225, 332, 881, 852, 687, 419, 578, 394, 483, 411, 365, 549,\n",
            "       267, 245, 663, 582, 911, 832, 790, 596, 101,  26, 963, 289, 304,\n",
            "       510, 727, 236, 435, 947, 219, 706, 497,  42, 283, 474, 956, 765,\n",
            "       165, 697,  24, 343, 875, 485, 844,  77, 599, 206, 545, 119, 188,\n",
            "       778, 489, 449, 637, 197, 177, 805, 476, 424, 903, 433, 515, 201,\n",
            "       774,  89,  54, 481, 652, 927, 499,  84, 813, 527, 414, 701, 619,\n",
            "       604, 192, 973, 379, 296, 360, 590, 632, 410, 253, 870, 634, 826,\n",
            "       893,  85, 436, 220, 279, 327, 374, 342, 439, 409, 811, 328,  15,\n",
            "       385, 622,  48, 344, 254, 884, 569, 686, 406, 340, 122, 757, 307,\n",
            "       335, 319, 232, 390,  32, 420, 617, 584, 597, 123, 919, 512, 698,\n",
            "       170, 679, 841, 613, 928, 223, 132, 141, 567, 105, 102, 781, 804,\n",
            "       498, 581,  34, 215, 120,  18, 134, 404, 218, 992, 864, 516,  94,\n",
            "       184, 405,  40, 380, 995, 511, 867, 400, 853, 854, 108, 103, 522,\n",
            "       671,  45, 308, 358, 104, 446, 953, 962, 395, 849, 968,  76, 857,\n",
            "       929, 665, 605, 964, 504, 265, 299, 930, 370, 278, 217, 716, 800,\n",
            "       275, 479, 315,  33, 272, 183, 649,  75, 234, 160,  81,  82, 877,\n",
            "       631, 526, 999, 872, 704, 524, 166, 285, 690,  71, 719, 576, 862,\n",
            "       543, 488, 600, 560, 984, 746, 566, 443, 513, 539, 880, 802, 661,\n",
            "       575,  47,  20, 367, 145, 281, 641, 997, 752,   7, 377, 646, 696,\n",
            "       770, 330, 914,  55, 636,  69,  16, 361, 747, 230, 351,  23, 295,\n",
            "       458, 700, 261,  83, 238, 658, 408, 750, 388, 643, 749, 755, 144,\n",
            "       873, 742,  25,  80, 703, 840, 764, 444, 611, 348,  98, 110, 777,\n",
            "       920, 833,  36, 207,   4, 204,  86, 146, 845,  62, 205,  78, 437,\n",
            "       985, 127, 819, 848, 735, 678,  61, 544, 714, 113, 354, 353, 629,\n",
            "       112, 151, 302, 357, 772, 432, 150, 125, 847, 948, 153, 193, 186,\n",
            "       417, 210, 926, 198, 986, 191, 579, 650, 172, 954, 346, 412, 542,\n",
            "       699, 624,  68, 136,  91, 910, 625, 808, 871, 115,  21, 326, 996,\n",
            "       715, 316, 664, 212, 677, 998, 525, 983, 976, 300, 137, 918, 702,\n",
            "       856, 558, 938, 415, 891, 694, 241, 535, 669, 130, 305, 333, 331,\n",
            "       942, 989,  65, 486, 662, 373, 587, 366, 364, 987, 660, 507,  37,\n",
            "       768, 387, 554,  10, 939, 887, 171, 861, 936, 591, 851, 532, 256,\n",
            "       574, 556, 568, 657, 362, 180, 738, 121, 941,  14, 680, 455, 356,\n",
            "       149, 259,  35, 482, 224, 462, 467,  22, 676, 324, 904, 422, 951,\n",
            "       260, 771, 843, 644, 571, 341, 991, 668, 550, 517, 401, 731, 898,\n",
            "       189, 109, 615, 392,  19, 688, 602, 917, 860, 940, 570, 971, 287,\n",
            "       932, 298, 129, 975, 520,  52, 200, 647, 396, 879, 460, 131, 325,\n",
            "       882, 876, 900, 440, 163, 739, 382, 565, 178, 233, 709, 107, 465,\n",
            "       816, 448, 607, 609,  31, 915, 407, 894, 540,  93, 428, 753, 721,\n",
            "       403, 293, 689, 359, 203,   9,   0,  63, 173, 477, 815, 227, 562,\n",
            "       784, 453, 459, 185, 642, 452, 323,  99, 803,  60, 503, 244,  44,\n",
            "       247, 573, 139, 495,  64, 913, 243, 179, 785, 863, 473, 795, 820,\n",
            "         1, 693, 257, 993,  72, 905, 133, 850, 258, 586, 228, 246, 148,\n",
            "       345, 505,   6, 835, 780, 828, 710, 156, 633, 902, 907, 209, 490,\n",
            "       960, 106, 899, 434, 114, 469, 935, 726, 711, 425, 874,  51, 164,\n",
            "       438, 580, 626, 564,  46, 878, 174, 627, 945,  57, 695, 734,  87,\n",
            "       758, 740, 239, 530, 958,  58, 763, 402, 728, 769, 982, 117, 255,\n",
            "       603, 352, 135, 484, 990, 502, 338, 553, 645, 823, 303, 537, 242,\n",
            "        95, 966, 821, 528, 682, 237, 949, 398, 463, 147, 809, 836, 208,\n",
            "       100, 588, 786,  49, 653,  53, 471, 955, 616, 378, 397, 692, 276,\n",
            "       269, 743, 981, 235,   5, 901, 933, 384, 426, 946, 670,  29, 263,\n",
            "       399, 730, 157, 796, 429, 931, 923, 822, 547, 252, 317, 608, 638,\n",
            "       799, 386, 614, 314, 761, 837, 142, 681, 583, 773, 162, 541, 280,\n",
            "        74, 194, 221, 152, 712,  43, 214, 744, 895,  50, 231, 369, 585,\n",
            "       506, 431, 708])], 'alpha': array([ 50,  50, 900])}\n",
            "starting optimization for opt mode 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 distance:  1.28\n",
            "prob_sum:  1.0\n",
            "l1 distance:  1.6\n",
            "prob_sum:  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 612x720 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYz62KhWttuP"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}