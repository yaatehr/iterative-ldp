{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython import get_ipython\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Update Path and env variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('load_ext', 'autoreload')\n","get_ipython().run_line_magic('autoreload', '2')\n","get_ipython().run_line_magic('config', 'IPCompleter.use_jedi = False')\n","# %load_ext ipycache\n","\n","import os\n","ROOT_DIR = os.path.abspath('') # For colab and ipynb\n","DATA_PATH = os.path.join(ROOT_DIR, 'data')\n","DATA_FILE_PATH = os.path.join(DATA_PATH, 'google-search-trends.csv')\n","\n","#COLAB\n","import sys\n","sys.path.insert(0, ROOT_DIR)\n","# sys.path.insert(0, \"/home/yaatehr/miniconda3/envs/idldp/lib/python3.8/site-packages/matlab\")\n","# sys.path.insert(0, \"/home/yaatehr/miniconda3/condabin\")\n","# sys.path.insert(0, \"/home/yaatehr/miniconda3/bin\")\n","# %env PYTHONPATH={GITHUB_PATH}\n","get_absolute_path = lambda x: os.path.join(ROOT_DIR,x)\n","\n","VERSION_NUM=\"22ObfuscateHighScore2-topk.0-hadamard\"\n","\n","\n","#for LOCAL Runtimes paste the following into your terminal. make sure to update the path...\n","# export PYTHONPATH=$PYTHONPATH:~/programs/xprize2/covid-xprize/\n","\n","# Uncomment to use the example predictor\n","# ! cd /content/covid-xprize/covid_xprize/examples/predictors/lstm/\n","# ! mkdir models\n","# ! cp /tests/fixtures/trained_model_weights_for_tests.h5 models/trained_model_weights.h5\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Relaxation eval"]},{"cell_type":"markdown","metadata":{},"source":[" ## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import numpy as np\n","import math\n","from collections import defaultdict\n","import functools\n","import operator\n","import collections\n","\n","# my_dict = [{'a':0, 'b':1, 'c':5}, {'b':3, 'c':2}, {'b':1, 'c':1}]\n","# sum_key_value = dict(functools.reduce(lambda a, b: a.update(b) or a, my_dict, collections.Counter()))\n","# print(my_dict)\n","# print(sum_key_value)\n","\n","def get_top_percentile(distribution, percentile=.5):\n","  index_list = []\n","  i = 0\n","  current_percentile = 0\n","  # assert probability_normalize(distribution) == distribution,\"testing the probability norm\"\n","  args = np.argsort(distribution)[::-1]\n","  while current_percentile < percentile:\n","    current_percentile += distribution[args[i]]\n","    index_list.append(args[i])\n","    i+= 1\n","  return index_list\n","\n","def get_percentile_indices(distribution, percentiles, topk_mode=True, floor_num=2):\n","  \"\"\"\n","  note percentiles are actually quantiles (0,1)\n","  \"\"\"\n","  percentiles = copy.copy(percentiles)\n","  if not len(percentiles):\n","    assert False\n","  # print(percentiles)\n","  # print(len(distribution))\n","\n","  args = np.argsort(distribution)[::-1] #indices in highest element first order\n","  if topk_mode:\n","    num_eles_per_tier = np.ceil(np.copy(percentiles)*len(args))\n","    less_than_3 = num_eles_per_tier < floor_num\n","    num_eles_per_tier[less_than_3] = floor_num\n","    total = np.sum(num_eles_per_tier)\n","    slack = len(args) - total\n","    if slack < 0:\n","      num_eles_per_tier[-1] += slack\n","    num_eles_per_tier = np.cumsum(num_eles_per_tier)\n","    items = np.split(args, num_eles_per_tier.astype(int))\n","    # print(items)\n","    return items, np.arange(len(distribution))[args]\n","\n","  sorted_dist = np.sort(distribution)[::-1]\n","  sorted_dist = np.cumsum(sorted_dist)\n","  unique_split_points = np.unique(sorted_dist)\n","  min_split, max_split = unique_split_points[0], unique_split_points[-1]\n","  if percentiles[0] < min_split: # note this only fixes edge cases, cases in teh middle will still fail if some buckets have no elements.\n","    # print(f\"updating the min split from {percentiles[0]} to {min_split}\")\n","    percentiles[0] = min_split\n","  if percentiles[-1] > max_split:\n","    # print(f\"updating the max split from {percentiles[-1]} to {max_split}\")\n","    percentiles[-1] = max_split\n","  # def get_next_ind(percentile, sorted_dist):\n","  #   try:\n","  #     return next(x for x, val in enumerate(sorted_dist) if val > percentile)\n","  #   except StopIteration e:\n","  #     print(\n","  #     return None\n","  try:\n","    split_indices = [ next(x for x, val in enumerate(sorted_dist) if val > percentile) for percentile in percentiles]\n","  except:\n","    # We assume we hit a 1 split point\n","    try:\n","      valid_dist = sorted_dist < 1\n","      v = np.split(sorted_dist, np.sum(valid_dist) or 1 )\n","      valid_dist, degenerate_dist = v[0], v[1]\n","      split_indices = [next(x for x, val in enumerate(valid_dist) if val > percentile) for percentile in percentiles[:-1]]\n","      split_indices.append(len(valid_dist))\n","    except:\n","      print(\"GET PERCENTILE INDICES THROWING, \")\n","      print(\"percentiles stats\")\n","      print(distribution)\n","      print(sorted_dist)\n","      print(percentiles)\n","      valid_dist = sorted_dist < 1\n","      print(\"splitting at: \", np.sum(valid_dist) or 1 )\n","      v = np.split(sorted_dist, np.sum(valid_dist) or 1 )\n","      valid_dist, degenerate_dist = v[0], v[1]\n","      split_indices = [next(x for x, val in enumerate(valid_dist) if val > percentile) for percentile in percentiles[:-1]]\n","      split_indices.append(len(valid_dist))\n","      print(split_indices)\n","  return np.split(args, split_indices), np.arange(len(distribution))[args]\n","\n","\n","\n","def init_privacy_budget(tiers, obfuscate_heavy_hitters=True, max_relaxation=0.2):\n","  privacy_budget = np.ones(tiers)\n","  if tiers > 1:\n","    mod = np.linspace(0, max_relaxation, num=tiers, endpoint=True)\n","    if not obfuscate_heavy_hitters:\n","      mod = np.flip(mod)\n","    privacy_budget += mod\n","  privacy_budget = privacy_budget.tolist()\n","  assert len(privacy_budget) == tiers, f\"privacy budget len is {len(privacy_budget)} but must match tiers ({tiers})\"\n","\n","  return privacy_budget\n","\n","def get_random_element_split(distribution, percentiles, seed=0, floor_num=2):\n","  if seed:\n","    np.random.seed(seed)\n","  args = np.arange(len(distribution))\n","  np.random.shuffle(args)\n","  # print(args)\n","  # print(num_splits)\n","\n","  num_eles_per_tier = np.ceil(np.copy(percentiles)*len(args))\n","  less_than_3 = num_eles_per_tier < floor_num\n","  num_eles_per_tier[less_than_3] = floor_num\n","  total = np.sum(num_eles_per_tier)\n","  slack = len(args) - total\n","  if slack < 0:\n","    num_eles_per_tier[-1] += slack\n","  num_eles_per_tier = np.cumsum(num_eles_per_tier)\n","\n","  return np.split(args, num_eles_per_tier.astype(int))\n","\n","def get_rank_order_splits(distribution, percentiles, scores, topk_mode=True, floor_num=2, weighted=True, weights=None):\n","  _, current_score = get_percentile_indices(distribution, percentiles, topk_mode=topk_mode, floor_num=floor_num)\n","  if weighted:\n","    if len(weights) ==1:\n","      current_score = current_score\n","      #  = np.ones(len(scores))/len(scores) # uniform weights\n","    # scores += current_score\n","    else:\n","      current_score = np.average(np.stack(scores+ [current_score], axis=0), axis=0, weights=np.array(weights)**.7) # makes earlier elements slightly stronger contribs....\n","  else:\n","    current_score = scores[-1] + current_score\n","  args = np.argsort(current_score)[::-1] #highest vals virst (but we reutned arange (argsort) which means they get numeric score of their index)\n","  if topk_mode:\n","    num_eles_per_tier = np.ceil(np.copy(percentiles)*len(args))\n","    less_than_3 = num_eles_per_tier < floor_num\n","    num_eles_per_tier[less_than_3] = floor_num\n","    total = np.sum(num_eles_per_tier)\n","    slack = len(args) - total\n","    if slack < 0:\n","      num_eles_per_tier[-1] += slack\n","    num_eles_per_tier = np.cumsum(num_eles_per_tier)\n","    items = np.split(args, num_eles_per_tier.astype(int))\n","    # print(items)\n","    return items, current_score\n","  sorted_scores_dist = probability_normalize(np.sort(current_score)[::-1])\n","  sorted_scores_dist = np.cumsum(sorted_scores_dist)\n","  split_indices = [next(x for x, val in enumerate(sorted_scores_dist) if val > percentile) for percentile in percentiles]\n","  # print(split_indices)\n","  return np.split(args, split_indices), current_score\n","\n","\n","#fast Hadamard Transform\n","def FWHT_A(k, dist):\n","    if k == 1:\n","        return dist\n","    dist1 = dist[0 : k//2]\n","    dist2 = dist[k//2 : k]\n","    trans1 = FWHT_A(k//2, dist1)\n","    trans2 = FWHT_A(k//2, dist2)\n","    trans = np.concatenate((trans1+ trans2, trans1 - trans2))\n","    return trans\n","\n","#simplex projection\n","def project_probability_simplex(p_estimate):\n","    \n","    k = len(p_estimate)  # Infer the size of the alphabet.\n","    p_estimate_sorted = np.sort(p_estimate)\n","    p_estimate_sorted[:] = p_estimate_sorted[::-1]\n","    p_sorted_cumsum = np.cumsum(p_estimate_sorted)\n","    i = 1\n","    while i < k:\n","        if p_estimate_sorted[i] + (1.0 / (i + 1)) * (1 - p_sorted_cumsum[i]) < 0:\n","            break\n","        i += 1\n","    lmd = (1.0 / i) * (1 - p_sorted_cumsum[i - 1])\n","    return np.maximum(p_estimate + lmd, 0)\n","\n","#clip and normalize\n","def probability_normalize(dist, epsilon=1e-10):\n","    dist = np.maximum(dist,epsilon) #map it to be positive\n","    norm = np.sum(dist)\n","    dist = np.true_divide(dist,norm) #ensure the l_1 norm is one\n","    return dist\n","\n","#generate a random permutation matrix\n","def Random_Permutation(k):\n","    permute = np.random.permutation(k)\n","    reverse = np.zeros(k)\n","    for i in range(k):\n","        reverse[int(permute[i])] = i\n","    return permute,reverse\n","\n","#pre-calculate Hadamard Matrix\n","def Hadarmard_init(k):\n","    H = [None] * k\n","    for row in range(k):\n","        H[row] = [True] * k\n","        \n","# Initialize Hadamard matrix of order n.\n","    i1 = 1\n","    while i1 < k:\n","        for i2 in range(i1):\n","            for i3 in range(i1):\n","                H[i2+i1][i3]    = H[i2][i3]\n","                H[i2][i3+i1]    = H[i2][i3]\n","                H[i2+i1][i3+i1] = not H[i2][i3]\n","        i1 += i1\n","    return H\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","#functions to generate distributions\n","def generate_geometric_distribution(k,lbd):\n","    elements = range(0,k)\n","    prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n","    return prob\n","\n","def generate_uniform_distribution(k):\n","    raw_distribution = [1] * k\n","    sum_raw = sum(raw_distribution)\n","    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n","    return prob\n","\n","def generate_two_steps_distribution(k):\n","    raw_distribution = [1] * int(k/2) + [3] * int(k/2)\n","    sum_raw = sum(raw_distribution)\n","    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n","    return prob\n","\n","def generate_Zipf_distribution(k,lbd):\n","    raw_distribution = [1/(float(i)**(lbd)) for i in range(1,k+1)]\n","    sum_raw = sum(raw_distribution)\n","    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n","    return prob\n","\n","def generate_Dirichlet_distribution(k,lbd):  \n","    raw_distribution = [0] * k\n","    for i in range(0,k):\n","        raw_distribution[i] = np.random.gamma(1,1)\n","    sum_raw = sum(raw_distribution)\n","    prob = [float(y)/float(sum_raw) for y in raw_distribution]\n","    return prob\n","\n","def kwarg_dummy(k, eps, rep, epochs, step_sz, init, dist, encode_acc = 1, encode_mode = 0, topk=.1, point_growth_rate=1, relaxation_mode=0, basegroup_size=-1, eps_relaxation=1, epoch_subdivisions=None):\n","  return locals()\n","\n","def kwarg_dummy2(k=None, eps=None, epochs=None,  init=None, encode_acc = 1, encode_mode = 0,                          tiers=2, relaxation_mode=0, growth_mode=0, score_mode=0,                          eps_relaxation=1, epoch_subdivisions=None, split_percentiles=None, baseline_arg_map=None):\n","  return locals()\n","\n","def get_max_min(df, col):\n","  return df[col].max(), df[col].min()\n","\n","def counter_to_dist(count_table):\n","  norm = sum(count_table.values())\n","  dist = np.array(list(count_table.values()))/norm\n","  # print(f\"verifying counts: counter: {count_table} \\n norm: {norm}\\n dist: {dist}\")\n","  return dist\n","\n","\n","\n","\n","def plot_distributions():\n","  fig, ax = plt.subplots(nrows=1, ncols=4)\n","  prob_list = {\n","      # 'Uniform' : prob1,\n","      'Two_steps' : prob2,\n","      'Zipf' : prob3,\n","      'Dirchlet' : prob4,\n","      'Geometric' : prob5, \n","  }\n","  i = 0\n","  for indices, ax in np.ndenumerate(ax):\n","    prob = list(prob_list.values())[i]\n","    prob_dist_name = list(prob_list.keys())[i]\n","    print(ax)\n","    ax.plot(prob)\n","    ax.set_title(prob_dist_name,  fontdict={'fontsize': 18, 'fontweight': 'medium'})\n","    if i == 1 or i == 3:\n","      ax.set_yscale(\"log\")\n","    i+= 1\n","  fig.set_size_inches(16.5, 2)\n","\n","  fig.savefig(\"true_distributions_k_1000.png\")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Hadamard Responses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Copyright 2017 Department of Electrical and Computer Engineering, Cornell University. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\n","# This is a package for locally private data transmission. \n","\n","\n","#%matplotlib inline\n","import math\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","# from functions import *\n","\n","\n","#the Hadamard randamized responce when \\epsilon < 1\n","class Hadamard_Rand_high_priv:\n","    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n","        #set encode_acc = 1 to enable fast encoding by intializing hadamard matrix\n","        self.insz = absz #input alphabet size k\n","        self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n","        self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n","        self.exp = math.exp(pri_para) #privacy parameter\n","        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n","        #self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n","        \n","        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n","        if encode_acc == 1:\n","            self.H = Hadarmard_init(self.outsz) # initialize Hadarmard matrix\n","            \n","    def parity_check(self,x,y): #check if the hadamard entry is one (return 0 for if the entry is 1)\n","        z = x&y #bit and\n","        z_bit = bin(z)[2:].zfill(self.outbit)\n","        check = 0\n","        for i in range(0,self.outbit): #check = \\sum_i (x_i y_i) （mod 2）\n","            check = check^int(z_bit[i]) \n","        return check\n","                                  \n","    def encode_symbol(self,in_symbol):  # encode a single symbol into a privatized version\n","        bitin = bin(int(in_symbol)+1)[2:].zfill(self.outbit) #map the input symbol \n","        # x to x+1 since we are not using the first column of the matrix\n","        out1 = random.randint(0,math.pow(2,self.outbit)-1) #get a random number in\n","        # the output alphabet as one potential output\n","        bitout1 = bin(out1)[2:].zfill(self.outbit)\n","        for i in range(0,self.outbit): #flip the bit of out1 corresponding to the left\n","            #  most bit in (in_symbol+1) which is one to get the other potential output\n","            if int(bitin[i]) == 1:\n","                out2 = out1 ^ (pow(2,self.outbit - i -1))\n","                break   \n","        #bitout2 = bin(out2)[2:].zfill(self.outbit)\n","        \n","        if self.ifencode_acc == 1:\n","            check = 1 - self.H[int(in_symbol)+1][out1]\n","        else:\n","            check = 0\n","            for i in range(0,self.outbit): # check if the Hadamard entry at position \n","                # (in_symbol+1, out1) is one or not\n","                check = check^(int(bitout1[i])&int(bitin[i]))\n","\n","        ra = random.random()\n","        if check == 0: # if it is one output out1 with prob (1-pri_para)\n","            if ra > self.pri_para:\n","                return out1\n","            else:\n","                return out2\n","        else: # else output out2 with prob (1-pri_para)\n","            if ra > self.pri_para:\n","                return out2\n","            else:\n","                return out1       \n","     \n","    def encode_string(self,in_list):  # encode string into a privatized string\n","        out_list = [self.encode_symbol(x) for x in in_list] # encode each symbol in the string\n","        return out_list\n","    \n","    def decode_string(self, out_list, iffast = 1, normalization = 0, verbose=False): # get the... \n","        # privatized string and learn the original distribution\n","        #normalization options: 0: clip and normalize(default)\n","        #                       1: simplex projection\n","        #                       else: no nomalization\n","        \n","        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n","        #        1 no fast hadamard tansform  time O(n + k^2)\n","        \n","        l = len(out_list) \n","        count,edges = np.histogram(out_list,range(self.outsz+1))\n","        dist = count/float(l)\n","        \n","        if iffast == 1: #if we use fast hadamard transform\n","            dist_mid = FWHT_A(self.outsz, dist) #do fast hadamard transform... \n","            # to the frequency vector\n","            dist_S = (dist_mid[1:self.insz+1] + 1)/float(2) #get the frequencies of C_i\n","        else: #if we don't use fast hadamard transform\n","            num = [0]*self.insz\n","            for x in range(0,self.outsz):\n","            #print x\n","                for i in range(1, self.insz+1): #if output x is in C_i(locations in...\n","                    #  row i which is 1), add it to the count of C_i\n","                    if self.parity_check(i,x) == 0:\n","                        num[i-1] = num[i-1] + count[x]\n","            dist_S = np.array([float(x)/float(l) for x in num]) #get the frequencies of C_i\n","        if verbose: \n","            print(\"decoded frequency of C_i before normalizations: \", dist_S)    \n","        \n","        dist = (2*dist_S*(1+self.exp)-(1+self.exp))/float(self.exp-1) #calculate the...\n","        # corresponding estimate for p_i\n","        if verbose:\n","            print(\"decoded p_i estimate before normalizations: \", dist)    \n","\n","        if normalization == 0: \n","            dist = probability_normalize(dist) #clip and normalize\n","        if normalization == 1:\n","            dist = project_probability_simplex(dist) #simplex projection\n","        \n","        return dist\n","\n","\n","#The Hadamard randomized response for all regimes (Modified Version)\n","\n","class Hadamard_Rand_general:\n","    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n","        self.insz = absz #input alphabet size k\n","        #self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n","        #self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n","        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n","        self.exp = math.exp(pri_para) #privacy parameter\n","        #self.initbit = int(math.floor(math.log(self.exp,2))) # number of bits indicating the block number \n","        self.initbit = int(math.floor(math.log(min(2*absz,self.exp),2))) # number of bits indicating the block number\n","        self.part = int(math.pow(2,self.initbit)) #total number of blocks B\n","        self.tailbit = int(math.ceil(math.log(float(self.insz)/float(self.part)+1,2))) #number of bits indicating the location within a block\n","        self.partsz = int(math.pow(2,self.tailbit)) # size of each block b\n","        self.num_one = int(self.partsz/float(2))\n","        self.outbit = self.tailbit + self.initbit #total number of output bits\n","        self.partnum = int(math.ceil(float(self.insz)/float(self.partsz - 1)))\n","        self.outsz = int(self.partsz*self.partnum) # output alphabet size K\n","        self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n","        \n","        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n","        if encode_acc == 1:\n","            self.H = Hadarmard_init(self.partsz) # initialize Hadarmard matrix\n","        \n","    def entry_check(self,x,y): #check if the reduced hadamard entry is one (return 0 for 1)\n","        x_bit = bin(x)[2:].zfill(self.outbit)\n","        y_bit = bin(y)[2:].zfill(self.outbit)\n","        for i in range(0,self.initbit): # check if they are in the same block, if not, return -1\n","            if x_bit[i] != y_bit[i]:\n","                return True\n","        check = False\n","        for i in range(self.initbit, self.outbit): #check whether the entry is one within the block\n","            check = check^(int(x_bit[i]) & int(y_bit[i]))\n","        return check\n","                                  \n","            \n","    def encode_symbol_rough(self,in_symbol):  # encode a single symbol into a privatized version \n","        # we use coupling argument to do this\n","        part_index = int(in_symbol)//(self.partsz-1)\n","        part_pos = int(in_symbol)%(self.partsz-1)+1\n","        in_column = (part_index << self.tailbit) + part_pos #map the input x to the xth column with weight b/2\n","        #in_column = part_index * self.partsz + part_pos\n","        out1 = np.random.randint(0,self.outsz) #get a random number out1 in the output alphabet as a potential output\n","        ra = random.random()\n","        if ra < (2*self.part)/(2*self.part-1+self.exp): #with certain prob, output the same symbol as from uniform distribution\n","            return out1\n","        else:\n","            out_pos = out1 & (self.partsz - 1)\n","            #out_pos = out1 % self.partsz\n","            out1 =  out_pos + (part_index << self.tailbit) # map out1 to the same block as in_column while maintain the location within the block\n","            #out1 = out_pos + part_index*self.partsz\n","            if self.ifencode_acc == 1:\n","                check = self.H[part_pos][out_pos]\n","            else:\n","                check = 1 - self.entry_check(in_column,out1)\n","\n","            if check == 0: #if location (in_column, out1) is one, output out1\n","                return out1\n","            else: #else flip bit at the left most location where bit representation of in_column is one \n","                #bitin = bin(int(in_column))[2:].zfill(self.outbit)\n","                check = 1\n","                for i in range(self.outbit - self.initbit): \n","                    if in_column%2 == 1:\n","                        #out1 = out1 ^ (pow(2,self.outbit - i -1))\n","                        out1 = out1 ^ check\n","                        break\n","                    in_column = in_column >> 1\n","                    check = check << 1\n","                return out1\n","            \n","    #delete the first row of each block\n","    def encode_symbol(self, in_symbol):\n","        while(1):\n","            out = self.encode_symbol_rough(in_symbol)\n","            if out%self.partsz != 0:\n","                return out\n","    \n","    def encode_string(self,in_list):  # encode string into a privatized string\n","        out_list = [self.encode_symbol(self.permute[x]) for x in in_list]\n","        return out_list\n","    \n","    \n","    def decode_string(self, out_list, iffast = 1, normalization = 0): # get the privatized string and learn the original distribution\n","        #normalization options: 0: clip and normalize(default)\n","        #                       1: simplex projection\n","        #                       else: no nomalization\n","        \n","        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n","        #        1 no fast hadamard tansform  time O(n + k^2)\n","        \n","        l = len(out_list)\n","        count,edges = np.histogram(out_list,range(self.outsz+1))\n","        freq = count/float(l)\n","        \n","        \n","        if iffast == 1:\n","            #parts = self.insz//(self.partsz-1) \n","            freq_S = np.zeros(self.outsz)\n","            freq_block = np.zeros(self.partnum)\n","            for i in range(0, self.partnum):\n","                Trans = FWHT_A(self.partsz, freq[i*self.partsz: (i+1)*self.partsz])\n","                freq_block[i] = Trans[0]\n","                freq_S[i*(self.partsz-1): (i+1)*(self.partsz-1)] = ( - Trans[1:self.partsz] + Trans[0])/float(2)         \n","            dist_S = freq_S[0:self.insz]\n","            \n","        else:\n","            freq_block = np.zeros(self.part) # count the number of appearances of each block\n","            for i in range(0,self.part): \n","                #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n","                for j in range(0,self.partsz):\n","                    freq_block[i] = freq_block[i] + freq[i*self.partsz+j]\n","            #freq_block = np.true_divide(count_block,l) # calculate the frequency of each block\n","            #dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block                    \n","            for i in range(0, self.insz): \n","                pi = int(i)//(self.partsz-1)\n","                ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n","                for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n","                    if self.entry_check(ti,x) == 0:\n","                        dist_S[i] = dist_S[i] + freq[x]\n","                        \n","        lbd = float(self.outsz - self.partnum)/float(self.num_one)\n","        c1 = lbd-1+self.exp\n","        \n","        dist_block = np.true_divide(c1*(freq_block)- 2 + 1/float(self.num_one),self.exp-1) # calculate the estimated original prob of each block\n","        \n","        c2 = self.exp - 1\n","        #dist = [float(2*c1*dist_S[i] - c2*dist_block[i//(self.partsz-1)] - 2)/float(c3) for i in range(0,self.insz) ]\n","        dist = [float(2*c1*dist_S[i] - c2*dist_block[i//(self.partsz-1)] - 2)/float(c2) for i in range(0,self.insz) ]\n","        \n","        if normalization == 0: \n","            dist = probability_normalize(dist) #clip and normalize\n","        if normalization == 1:\n","            dist = project_probability_simplex(dist) #simplex projection\n","        \n","        #reverse the permuation\n","        dist1 = np.zeros(self.insz)\n","        for i in range(self.insz):\n","            dist1[int(self.reverse[i])] = dist[i]\n","        return dist1\n","    \n","    \n","    def decode_string_old(self, out_list): # get the privatized string and learn the original distribution\n","        \n","        l = len(out_list)\n","        dist_S = np.zeros(self.insz)\n","        count,edges = np.histogram(out_list,range(self.outsz+1))\n","        freq = count/float(l)\n","        \n","        freq_block = np.zeros(self.part) # count the number of appearances of each block\n","        for i in range(0,self.part): \n","            #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n","            for j in range(0,self.partsz):\n","                freq_block[i] = freq_block[i] + freq[i*self.partsz+j]\n","        \n","        \n","        #freq_block = np.true_divide(count_block,l) # calculate the frequency of each block\n","        dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n","                    \n","        for i in range(0, self.insz): \n","            pi = int(i)//(self.partsz-1)\n","            ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n","            for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n","                if self.entry_check(ti,x) == 0:\n","                    dist_S[i] = dist_S[i] + freq[x]\n","\n","        #dist_S = np.zeros(self.insz)\n","        #dist_S = np.true_divide(num,l) #calculate the frequency of each C_i\n","        dist_inter = np.true_divide(2*(dist_S*(2*self.part-1+self.exp)-1),self.exp-1) # calculate intermediate prob\n","        dist = [dist_inter[i] - dist_block[i//(self.partsz-1)] for i in range(0,self.insz)] # calculate the estimated prob for each symbol\n","        dist = np.maximum(dist,0) #map it to be positive\n","        norm = np.sum(dist)\n","        dist = np.true_divide(dist,norm) #ensure the l_1 norm is one\n","        return dist\n","    \n","    #def decode_string_normalize(self, out_list): #normalized outputs using clip and normalize\n","    #    dist = self.decode_string_permute(out_list)\n","    #    dist = probability_normalize(dist)\n","    #    return dist\n","    \n","    #def decode_string_project(self, out_list): #projected outputs\n","    #    dist = self.decode_string_permute(out_list)\n","    #    dist = project_probability_simplex(dist)\n","    #    return dist\n","    \n","    #def decode_string_permute(self, out_list): # get the privatized string and learn the original distribution\n","    #    dist1 = self.decode_string_fast(out_list)\n","    #    dist = np.zeros(self.insz)\n","    #    for i in range(self.insz):\n","    #        dist[int(self.reverse[i])] = dist1[i]\n","    #    return dist\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","#The Hadamard randomized response for all regimes (original version)\n","class Hadamard_Rand_general_original:\n","    def __init__(self, absz, pri_para, encode_acc = 0): # absz: alphabet size, pri_para: privacy parameter\n","        #set encode_acc = 1 to enable fast encoding by intializing hadamard matrix\n","        \n","        self.insz = absz #input alphabet size k\n","        #self.outsz = int(math.pow(2,math.ceil(math.log(absz+1,2)))) #output alphabet size: smallest exponent of 2 which is bigger than k\n","        #self.outbit = int(math.ceil(math.log(absz+1,2))) #output bit length\n","        self.pri_para = 1/(1+math.exp(pri_para)) #flipping probability to maintain local privacy\n","        self.exp = math.exp(pri_para) #privacy parameter\n","        #self.initbit = int(math.floor(math.log(self.exp,2))) # number of bits indicating the block number \n","        self.initbit = int(math.floor(math.log(min(2*absz,self.exp),2))) # number of bits indicating the block number\n","        self.part = int(math.pow(2,self.initbit)) #number of blocks B\n","        self.tailbit = int(math.ceil(math.log(float(self.insz)/float(self.part)+1,2))) #number of bits indicating the location within a block\n","        self.partsz = int(math.pow(2,self.tailbit)) # size of each block b\n","        self.outbit = self.tailbit + self.initbit #total number of output bits\n","        self.outsz = int(math.pow(2,self.outbit)) # output alphabet size K\n","        self.permute, self.reverse = Random_Permutation(absz) #Initialize random permutation\n","        \n","        self.ifencode_acc = encode_acc #whether to accelerate encoding process\n","        if encode_acc == 1:\n","            self.H = Hadarmard_init(self.partsz) # initialize Hadarmard matrix\n","        \n","        \n","    def entry_check(self,x,y): #check if the reduced hadamard entry is one (return 0 for 1)\n","        x_bit = bin(x)[2:].zfill(self.outbit)\n","        y_bit = bin(y)[2:].zfill(self.outbit)\n","        for i in range(0,self.initbit): # check if they are in the same block, if not, return -1\n","            if x_bit[i] != y_bit[i]:\n","                return True\n","        check = False\n","        for i in range(self.initbit, self.outbit): #check whether the entry is one within the block\n","            check = check^(int(x_bit[i]) & int(y_bit[i]))\n","        return check\n","                                  \n","    \n","    def encode_symbol(self,in_symbol):  # encode a single symbol into a privatized version \n","        # we use coupling argument to do this\n","        part_index = int(in_symbol)//(self.partsz-1)\n","        part_pos = int(in_symbol)%(self.partsz-1)+1\n","        in_column = (part_index << self.tailbit) + part_pos #map the input x to the xth column with weight b/2\n","        #in_column = part_index * self.partsz + part_pos\n","        out1 = np.random.randint(0,self.outsz) #get a random number out1 in the output alphabet as a potential output\n","        ra = random.random()\n","        if ra < (2*self.part)/(2*self.part-1+self.exp): #with certain prob, output the same symbol as from uniform distribution\n","            return out1\n","        else:\n","            out_pos = out1 & (self.partsz - 1)\n","            #out_pos = out1 % self.partsz\n","            out1 =  out_pos + (part_index << self.tailbit) # map out1 to the same block as in_column while maintain the location within the block\n","            #out1 = out_pos + part_index*self.partsz\n","            if self.ifencode_acc == 1:\n","                check = 1 - self.H[part_pos][out_pos]\n","            else:\n","                check = self.entry_check(in_column,out1)\n","\n","            if check == 0: #if location (in_column, out1) is one, output out1\n","                return out1\n","            else: #else flip bit at the left most location where bit representation of in_column is one \n","                #bitin = bin(int(in_column))[2:].zfill(self.outbit)\n","                check = 1\n","                for i in range(self.outbit - self.initbit): \n","                    if in_column%2 == 1:\n","                        #out1 = out1 ^ (pow(2,self.outbit - i -1))\n","                        out1 = out1 ^ check\n","                        break\n","                    in_column = in_column >> 1\n","                    check = check << 1\n","                return out1\n","    \n","    def encode_string(self,in_list): #permute before encoding\n","        out_list = [self.encode_symbol(self.permute[x]) for x in in_list]\n","        return out_list        \n","    \n","    \n","    def decode_string(self, out_list,iffast = 1, normalization = 0): # get the privatized string and learn the original distribution\n","        #normalization options: 0: clip and normalize(default)\n","        #                       1: simplex projection\n","        #                       else: no nomalization\n","        \n","        #iffast: 0 use fast hadamard transform time O(n  + k\\log k)\n","        #        1 no fast hadamard tansform  time O(n + kB), B is the block size\n","        \n","        l = len(out_list)\n","        count,edges = np.histogram(out_list,range(self.outsz+1))\n","        freq = count/float(l)\n","        \n","        if iffast == 1:\n","            parts = self.insz//(self.partsz-1) \n","            freq_S = np.zeros((parts+1)*self.partsz)\n","            freq_block = np.zeros((parts+1)*self.partsz)\n","        \n","            for i in range(0, parts+1):\n","                Trans = FWHT_A(self.partsz, freq[i*self.partsz: (i+1)*self.partsz])\n","                freq_block[i] = Trans[0]\n","                freq_S[i*(self.partsz-1): (i+1)*(self.partsz-1)] = (Trans[1:self.partsz] + Trans[0])/float(2) \n","            dist_S = freq_S[0:self.insz]\n","        \n","            dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n","        \n","        else:\n","            freq_block = np.zeros(self.part) # count the number of appearances of each block\n","            for i in range(0,self.part): \n","                #count_block[i] = np.sum(count[i*self.partsz : (i+1)*self.partsz - 1])\n","                for j in range(0,self.partsz):\n","                    freq_block[i] = freq_block[i] + freq[i*self.partsz+j]     \n","                    \n","            dist_block = np.true_divide((2*self.part-1+self.exp)*(freq_block)-2,self.exp-1) # calculate the estimated original prob of each block\n","            for i in range(0, self.insz): \n","                pi = int(i)//(self.partsz-1)\n","                ti = pi*self.partsz + int(i)%(self.partsz-1)+1\n","                for x in range(pi*self.partsz, (pi+1)*self.partsz): # count the number of appearances of each C_i\n","                    if self.entry_check(ti,x) == 0:\n","                        dist_S[i] = dist_S[i] + freq[x]\n","        \n","        dist_inter = np.true_divide(2*(dist_S*(2*self.part-1+self.exp)-1),self.exp-1) # calculate intermediate prob\n","        dist = [dist_inter[i] - dist_block[i//(self.partsz-1)] for i in range(0,self.insz)] # calculate the estimated prob for each symbol\n","        \n","        if normalization == 0: \n","            dist = probability_normalize(dist) #clip and normalize\n","        if normalization == 1:\n","            dist = project_probability_simplex(dist) #simplex projection\n","        \n","        #reverse the permuation\n","        dist1 = np.zeros(self.insz)\n","        for i in range(self.insz):\n","            dist1[int(self.reverse[i])] = dist[i]\n","        return dist1\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Ground Truth Evaluation"]},{"cell_type":"markdown","metadata":{},"source":[" ### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","BBOX_ANCHOR = 1.47\n","def calibrate_plt(plt):\n","  SMALL_SIZE = 14\n","  MEDIUM_SIZE = 18\n","  BIGGER_SIZE = 30\n","\n","  plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","  plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n","  plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","  plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","  plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","  plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n","  # plt.rc('legend', loc='lower left')    # legend fontsize\n","  # plt.rc('legend', bbox_to_anchor=(1.05,1))    # legend fontsize\n","  plt.rc('legend', framealpha=.1)    # legend fontsize\n","  plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","calibrate_plt(plt)\n","\n","softmax = lambda x : np.exp(x)/sum(np.exp(x))\n","\n","DEFAULT_EPOCH_GROWTH_RATE = 0\n","DEFAULT_SUBDIVISION_GROUTH_RATE=0\n","\n","def ground_truth_comp(df, d2, col):\n","  \"\"\"\n","  take in the decoded df, ground truth df, and target col\n","  return dict of key: pred - ground truth\n","  \"\"\"\n","  # df = pd.DataFrame({\"a\": [1, 1, 3, 4], \"b\": [5,6,7,7]})\n","  # d2 = pd.DataFrame({\"a\": [1, 1, 3, 4, 5, 6, 3], \"b\": [5,6,7,7,0,0,0]})\n","  df_val_counts = df[col].value_counts(sort=False)\n","  # ground_truth_counts = ground_truth_hist[col]\n","  ground_truth_counts = d2[col].value_counts(sort=False)\n","  eval_map = defaultdict(lambda : 0, zip( df_val_counts.keys(), df_val_counts.values))\n","  ground_truth_val, ground_truth_key = ground_truth_counts.values, ground_truth_counts.keys()\n","  # print(df[\"a\"].value_counts(sort=False).keys())\n","  comparison_map = defaultdict(lambda: 0, zip(ground_truth_key, ground_truth_val))\n","  differences = {k: (eval_map[k] or 0) - x for k, x in  comparison_map.items()}\n","  # print(differences)\n","  return differences\n","\n","def get_growth_rate(df, subdivisions, mode=0):\n","  if mode==0: #uniform\n","    return  [int(len(df) * i/(subdivisions)) for i in range(1, subdivisions)]\n","  elif mode == 1: #linear\n","    return  (len(df) *np.linspace(.05,1,num=subdivisions)).astype(int)[:-1]\n","  elif mode == 2:\n","    # print(\"EXPONENTIAL G\")\n","    return  (len(df) *np.linspace(0.25, 1.0, num=subdivisions)**2).astype(int)[:-1]\n","\n","  else:\n","    assert False, \"growth mode not implemented\"\n","\n","# def get_epoch_df(df, epochs, epoch): #TODO Fix this it goes backwards and ends on full starts on full\n","#   return df[:int(len(df)*(epoch)/epochs)] if epoch < epochs else df\n","def generate_percent_err(baseline_err, input_err):\n","    if np.isnan(( 1 - (input_err/baseline_err[:len(input_err)])) * 100).any():\n","        print(f\"baseline_err: {baseline_err}\")\n","        print(f\"input_err: {input_err}\")\n","        print( ( 1 - (input_err/baseline_err[:len(input_err)])) * 100)\n","    return (( 1 - (input_err/baseline_err[:len(input_err)])) * 100).flatten()\n","\n","\n","\n","def snake_to_readable(word):\n","  # import re\n","  def comp_word(w, t):\n","    return w == t or w == \"baseline_\" + t\n","  if comp_word(word, \"prob_est\"):\n","    return \"Frequency Estimation Density\"\n","  if comp_word(word, \"variance\"):\n","    return \"Emp. Error Variance\"\n","  if comp_word(word, 'hr_error'):\n","    return \"Distribution RMSE\"\n","  if comp_word(word, 'hr_count_error_rmse'):\n","    return \"Count RMSE (Normalized)\"\n","  o = ''.join((x.capitalize() + \" \" or ' ') if x !=\"inv\" else (x.capitalize() + \". \" or ' ') for x in word.split('_'))\n","  o = o.replace('Rmse', 'RMSE')\n","  return o\n","\n","def metric_to_unit_mapping(word):\n","  # import re\n","  normalized_rmse_metric_list = ['partition_weighted_rmse', 'weighted_rmse', 'inv_weighted_rmse', 'prob_est', 'partition_inv_weighted_rmse']\n","\n","  def comp_word(w, t):\n","    return w == t or w == \"baseline_\" + t\n","  if comp_word(word, \"prob_est\"):\n","    return \"Frequency Estimation Density\"\n","  if comp_word(word, \"kl_divergence\"):\n","    return \"Relative Entropy\"\n","  if any([comp_word(word, w) for w in normalized_rmse_metric_list]):\n","    return \"Distribution RMSE\"\n","  if comp_word(word, \"variance\"):\n","    return \"Emp. Error Variance\"\n","  if comp_word(word, 'hr_error'):\n","    return \"Distribution RMSE\"\n","  if comp_word(word, 'hr_count_error_rmse'):\n","    return \"Count RMSE (Normalized)\"\n","  # if comp_word(word, \"chi_square_distance\"\n","  if comp_word(word, 'time'):\n","    return 'Duration (seconds)'\n","  return ''.join((x.capitalize() + \" \" or ' ') if x !=\"inv\" else (x.capitalize() + \". \" or ' ') for x in word.split('_'))\n","\n","\n","def generate_legend_baseline(\n","    baseline_readable: str,\n","    ):\n","    legend = [f'Zero Shot {baseline_readable}', f\"ZS Random Prior {baseline_readable}\", f\"ZS Oracle Prior {baseline_readable}\", \"Zero Shot OUE\", \"Zero Shot Hadamard\", f\"Iter. Random Prior {baseline_readable}\", f\"Iter. Oracle Prior {baseline_readable}\"]\n","\n","    return legend\n","\n","def finalize_legend(legend):\n","  legend +=  [(\"Iter. \" if (ldp_mechanism_helper(x) >2 or ldp_mechanism_helper(x) == -1) else \"\")  + snake_to_readable(x) + \" (Our Method)\" for x in LDP_MECHANISMS]\n","  return legend\n","\n","\n","\n","def gen_partition_aggregate_column_boxplot(input_metrics, baseline_metric_list, index_list, baseline_names, interest_cols, num_points_per_epoch, figure_title, filename=\"temp\", save_images=False):\n","  \"\"\"\n","  input_metrics: list of metrics of shape (n_iterations, len(indices)) that should match the order of interest cols (and be same length)\n","  baseline_metric_list: list of list metrics of same format as input metrics, one for each baseline\n","  index_list: list of the x axis labels (number of items in the given test)\n","  baseline_names: list of strings for baseline names\n","  interest_cols: list of metric type names (ie raw (l1) err, l2 err, rmse)\n","  figure title:  string listing experiment conditions nad params. \n","  \"\"\"\n","\n","  colors = ['crimson', 'coral', 'gold', 'darkcyan', 'dodgerblue', 'slategrey']\n","  #indicates that the experimental has a single subdiv. skip\n","  #index list needs to be variable length lists for the number of points n in each chart\n","\n","  index_locs = [i for i, percentile_list in enumerate(index_list) if len(percentile_list) > 1]\n","  baseline_metric_list = np.array(baseline_metric_list)[[i - 1 for i in index_locs[1:]]].tolist()\n","  index_list = np.array(index_list)[index_locs].tolist()\n","  # print(index_list)\n","  print(baseline_names)\n","  baseline_names = np.array(baseline_names)[index_locs].tolist()\n","  index_labels = copy.copy(index_list)\n","  index_list = [inds / inds[-1] for inds in index_list]\n","\n","  if len(index_list) == 0 or 0 not in index_locs: #filter out list here (for all input lists, baseline names, metrics, etc)\n","    print(\"Skipping single partition chart\")\n","    return\n","  \n","  # n is not fixed. It actually changes per baseline / experiment.\n","  r = 1 + len(baseline_metric_list) # should actually be the number of non lenth 1 charts\n","  fig, ax = plt.subplots(nrows=r)\n","  fig.suptitle(figure_title, fontsize=15)\n","  fig.set_size_inches(15, 10*r)\n","  print(r)\n","  print(baseline_names)\n","  # print(\"the shaep of the input metrics is....: \")\n","  # print(input_metrics.shape)\n","  # print(\"the shaep of the baseline metrics is....: \")\n","  # print(baseline_metric_list.shape)\n","\n","  upper_y_limit = 0\n","  lower_y_limit = float(\"inf\")\n","  limits  = [(lower_y_limit, upper_y_limit)]*r \n","  all_bps = []\n","  n_list = []\n","  for i, data in enumerate([input_metrics, *baseline_metric_list]):\n","    try:\n","      transposed_data = data.transpose(2,1,0)\n","    except:\n","      transposed_data = np.array(data)[0].transpose(1,0,2)\n","\n","    n = transposed_data.shape[0]\n","    n_list.append(n)\n","    widths = 1/(n+1)\n","\n","    right_shift_const = np.linspace(0,widths*n,num=n, endpoint=True) - widths*n/2\n","    widths = 1/(n+2)\n","\n","    bps = gen_partition_column_boxplot(transposed_data, index_list[i], interest_cols, ax, i, [np.arange(transposed_data.shape[2]) + right_shift_const[j] for j in range(n)], limits, widths, colors, name=baseline_names[i])\n","    all_bps.extend(bps) #TODO add teh dash on off from here https://www.programcreek.com/python/?CodeExample=set+linestyle\n","    # limits = new_lim\n","\n","  slack = [(upper_y_limit - lower_y_limit)*.05 for lower_y_limit, upper_y_limit in limits]\n","  limits = [(lower_y_limit - slack[i], upper_y_limit + slack[i]) for i, (lower_y_limit, upper_y_limit) in enumerate(limits)]\n","\n","  def slice_per(source, step):\n","      return [source[i::step] for i in range(step)]\n","  # bp_ax_list = slice_per(all_bps, n)\n","  # print(bp_ax_list)\n","  # print(\"LIMITS: \")\n","  # print(limits)\n","  # print(len(bps))\n","  counter = 0\n","  lgd = None\n","  for i, a in (enumerate(ax) if isinstance(ax,  np.ndarray) else enumerate([ax])): # set each compared axis to be the same scale\n","    # n = n_list[i]\n","    n = len(num_points_per_epoch)\n","    a.set_ylim(limits[i% r])\n","    a.yaxis.grid(True, which='both')\n","    a.xaxis.grid(True, which='both')\n","    if len(all_bps) > 0:\n","      # print(\"trying to update legend\")\n","      lgd = a.legend([bp[\"boxes\"][0] for bp in all_bps[counter:counter + n]], num_points_per_epoch)\n","    else:\n","      lgd = a.legend()\n","    a.set_xlabel(\"Percentage of Total Data Set\")\n","    a.set_ylabel(metric_to_unit_mapping(interest_cols[0]))\n","    a.set_title(snake_to_readable(interest_cols[0]) +f\" {baseline_names[i]}\")\n","    a.set_xticks(np.arange(len(index_list[i])))\n","    a.set_xticklabels(f\"{x*100:.2f}%\"for x in index_list[i])\n","    counter += n\n","    \n","\n","  fig.show()\n","  if save_images:\n","    plt.savefig(filename+ \".png\", bbox_extra_artists=(lgd,), bbox_inches='tight')\n","  plt.pause(0.1)\n","\n","\n","\n","\n","def gen_partition_column_boxplot(metrics, index_list, interest_cols, ax, ax_no, right_shift_list, limits, width, color, name=None):\n","  bps = []\n","  new_lims = []\n","  if not isinstance(ax, np.ndarray):\n","    ax = [ax]\n","\n","  for i, data in enumerate(metrics):\n","\n","    lower_y_limit, upper_y_limit = limits[ax_no]\n","    max_y = np.max(data)\n","    min_y = np.min(data)\n","    upper_y_limit = max_y if max_y >= upper_y_limit else upper_y_limit\n","    lower_y_limit = min_y if min_y <= lower_y_limit else lower_y_limit\n","    # new_lims.append((lower_y_limit, upper_y_limit))\n","    limits[ax_no]= (lower_y_limit, upper_y_limit)\n","\n","    skip_boxplot = len(data.shape) < 2\n","\n","    if not skip_boxplot:\n","      bp = ax[ax_no].boxplot(data, positions = right_shift_list[i], notch=True, patch_artist=True, \n","                    boxprops=dict(\n","                        color=color[i],\n","                        facecolor=color[i],\n","                        alpha=.5\n","                    ),\n","                    widths = width\n","                    )\n","      bps.append(bp)\n","    medians = np.median(data, axis=0) if not skip_boxplot else data\n","    ax[ax_no].plot(list(range(len(index_list))), medians, color=color[i], alpha=.8 if skip_boxplot else .63, label = name if skip_boxplot else None)\n","\n","\n","  return bps\n","\n","\n","\n","  \n","def gen_aggregate_column_boxplot(input_metrics, baseline_metric_list, indices, baseline_names, interest_cols, figure_title, filename=\"temp\", save_images=False):\n","  \"\"\"\n","  input_metrics: list of metrics of shape (n_iterations, len(indices)) that should match the order of interest cols (and be same length)\n","  baseline_metric_list: list of list metrics of same format as input metrics, one for each baseline\n","  indices: the x axis labels (number of items in the given test\n","  baseline_names: list of strings for baseline names\n","  interest_cols: list of metric type names (ie raw (l1) err, l2 err, rmse)\n","  figure title:  string listing experiment conditions nad params. \n","  \"\"\"\n","\n","  colors = ['lightblue', 'lightgreen', 'pink', 'tomato', 'mediumpurple','darkcyan', 'crimson', 'coral', 'gold', 'darkcyan', 'dodgerblue',  'tan', 'slategrey']\n","\n","  r = len(input_metrics)\n","  n = 1 + len(baseline_metric_list)\n","  fig, ax = plt.subplots(nrows=r)\n","  fig.suptitle(figure_title, fontsize=15)\n","  fig.set_size_inches(15, 10*r)\n","  # print(\"the shaep of the input metrics is....: \")\n","  # print(input_metrics.shape)\n","  # print(\"the shaep of the baseline metrics is....: \")\n","  # print(baseline_metric_list.shape)\n","\n","\n","  upper_y_limit = 0\n","  lower_y_limit = float(\"inf\")\n","  limits  = [(lower_y_limit, upper_y_limit)]*r\n","  all_bps = []\n","  widths = 1/(n+1)\n","  for i, data in enumerate([input_metrics, *baseline_metric_list]):\n","\n","    # print(data)\n","    # print(data.shape)\n","    # print(indices.shape)\n","    right_shift_const = np.linspace(0,widths*n,num=n, endpoint=True) - widths*n/2\n","    widths = 1/(n+2)\n","\n","    bps, new_lim = gen_column_boxplot(data,indices, interest_cols, ax, np.arange(len(indices)) + right_shift_const[i], limits, widths, colors[i], name=baseline_names[i])\n","    all_bps.extend(bps) #TODO add teh dash on off from here https://www.programcreek.com/python/?CodeExample=set+linestyle\n","    limits = new_lim\n","\n","  slack = [(upper_y_limit - lower_y_limit)*.05 for lower_y_limit, upper_y_limit in limits]\n","  limits = [(lower_y_limit - slack[i], upper_y_limit + slack[i]) for i, (lower_y_limit, upper_y_limit) in enumerate(limits)]\n","\n","  def slice_per(source, step):\n","      return [source[i::step] for i in range(step)]\n","  bp_ax_list = slice_per(all_bps, r)\n","  # print(\"LIMITS: \")\n","  # print(limits)\n","  lgd = None\n","  for i, a in (enumerate(ax) if isinstance(ax,  np.ndarray) else enumerate([ax])): # set each compared axis to be the same scale\n","    a.set_ylim(limits[i% r])\n","    a.yaxis.grid(True, which='both')\n","    a.xaxis.grid(True, which='both')\n","    if len(all_bps) > 0:\n","      lgd = a.legend([bp[\"boxes\"][0] for bp in bp_ax_list[i]], baseline_names,loc='upper right', bbox_to_anchor=(BBOX_ANCHOR,1))\n","    else:\n","      lgd = a.legend(loc='upper right', bbox_to_anchor=(BBOX_ANCHOR,1))\n","    a.set_xlabel(\"Number of Samples\")\n","    a.set_ylabel(f\"{metric_to_unit_mapping(interest_cols[i])}\")\n","    a.set_title(snake_to_readable(interest_cols[i]))\n","    a.set_xticks(np.arange(len(indices)))\n","    a.set_xticklabels(indices)\n","    \n","\n","  fig.show()\n","  if save_images:\n","    plt.savefig(filename+ \".png\", bbox_extra_artists=(lgd,), bbox_inches='tight')\n","  plt.pause(0.1)\n","\n","\n","\n","\n","def gen_column_boxplot(metrics, indices, interest_cols, ax, right_shift, limits, width, color, name=None):\n","  \"\"\"\n","  metrics: list of metrics of shape (n_iterations, len(indices)) that should match the order of interest cols (and be same length)\n","  indices: the x axis labels (number of items in the given test\n","  interest_cols: list of metric type names (ie raw (l1) err, l2 err, rmse)\n","  ax: a list of axis to plot on 9one for each interest col\n","  right_shift: to prevent boxx and whiskers from overlapping.\n","  \"\"\"\n","\n","  bps = []\n","  new_lims = []\n","  if not isinstance(ax, np.ndarray):\n","    ax = [ax]\n","\n","  for i, data in enumerate(metrics):\n","    # print(data.shape)\n","    lower_y_limit, upper_y_limit = limits[i]\n","    max_y = np.max(data)\n","    min_y = np.min(data)\n","    upper_y_limit = max_y if max_y >= upper_y_limit else upper_y_limit\n","    lower_y_limit = min_y if min_y <= lower_y_limit else lower_y_limit\n","    new_lims.append((lower_y_limit, upper_y_limit))\n","\n","    skip_boxplot = len(data.shape) < 2\n","\n","    if not skip_boxplot:\n","      bp = ax[i].boxplot(data, positions = right_shift, notch=True, patch_artist=True, \n","                    boxprops=dict(\n","                        color=color,\n","                        facecolor=color,\n","                        alpha=.5\n","                    ),\n","                    widths = width\n","                    )\n","      bps.append(bp)\n","    medians = np.median(data, axis=0) if not skip_boxplot else data\n","    ax[i].plot(list(range(len(indices))), medians, color=color, alpha=.8 if skip_boxplot else .63, label = name if skip_boxplot else None)\n","\n","\n","  return bps, new_lims\n","\n","\n","\n","\n","\n","\n","\n","\n","def generate_plot_for_metrics(inputs, baselines, indices, plot_labels, figure_title=\"temp plot\", filename=\"tempplt.png\", save_images=True, output_counts=False, fig=None, ax=None):\n","\n","    if fig == None:\n","      fig, ax = plt.subplots(nrows=len(inputs), ncols=1) \n","    \n","    fig.set_size_inches(8.5, 10)\n","    fig.suptitle(figure_title, fontsize=16)\n","    for i, metric in enumerate(inputs):\n","        baseline_metric = baselines[i]\n","        percent_errors = generate_percent_err(baseline_metric, metric)\n","        \n","        print(\"v\"*50)\n","        print(percent_errors)\n","        # print(step_sz)\n","        widths = np.insert(indices[:-1],0,0)\n","        widths = indices - widths\n","        print(indices)\n","        try:\n","          indices = indices.to_list()\n","        except:\n","          pass\n","        print(len(indices))\n","        print(len(percent_errors))\n","        print(\"^\"*30)\n","        barlist = ax[i].bar(indices, percent_errors, width=widths*0.75) # this many out of total number of points\n","        for j, percent in enumerate(percent_errors):\n","          if percent < 0:\n","            barlist[j].set_color('r')\n","          else:\n","            barlist[j].set_color('g')\n","        ax[i].set_title(plot_labels[i]) # this many out of total number of points\n","        ax[i].set_xlabel(\"Number of Samples\")\n","        ax[i].set_ylabel(\"Avg Percent Improvement (0 is baseline perf)\")\n","        ax[i].set_ylim((-50, 50))\n","        ax[i].set_xticks(indices)\n","        rects = ax[i].patches\n","        print(metric - baseline_metric)\n","        bar_labels = [f\"{float(i):.1f}\" for i in (abs(metric - baseline_metric).flatten() if output_counts else percent_errors)]\n","        for rect, label in zip(rects, bar_labels):\n","            height = rect.get_height()\n","            x_value = rect.get_x() + rect.get_width() / 2\n","\n","            # Number of points between bar and label. Change to your liking.\n","            space = 5\n","            # Vertical alignment for positive values\n","            va = 'bottom'\n","\n","            # If value of bar is negative: Place label below bar\n","            if height < 0:\n","                # Invert space to place label below\n","                space *= -1\n","                # Vertically align label at top\n","                va = 'top'\n","            ax[i].annotate(label, (x_value, height), xytext=(0, space), textcoords=\"offset points\", ha='center', va=va)\n","        # ax[i].legend()\n","    fig.show()\n","    if save_images:\n","        plt.savefig(filename+ \".png\")\n","    plt.pause(0.1)\n","\n","\n","def generate_plot_for_baselines(inputs, indices, plot_labels, figure_title=\"temp plot\", filename=\"tempplt.png\"):\n","    print(inputs)\n","    fig, ax = plt.subplots(nrows=len(inputs[0]), ncols=1) \n","    fig.set_size_inches(8.5, 10)\n","    fig.suptitle(figure_title, fontsize=16)\n","    colors = ['r', 'y', 'g']\n","    legend = ['HR', \"RSHR\", \"PSHR\"]\n","    for k, metric_group in enumerate(inputs):\n","        # print(indices)\n","        # print(\"v\"*50)\n","        # print(percent_errors)\n","        # print(step_sz)\n","        widths = np.insert(indices[:-1],0,0)\n","        widths = indices - widths\n","        # print(indices)\n","        # print(widths)\n","        try:\n","          indices = indices.to_list()\n","        except:\n","          pass\n","        # print(len(indices))\n","        # print(len(percent_errors))\n","        # print(\"^\"*30)\n","        for i, metric in enumerate(metric_group):\n","          # print((indices +widths*.25*k).astype(int), metric, (widths*0.25).astype(int))\n","          # barlist = ax[i].bar(indices +widths*.25*k, metric.flatten(), width=widths*0.25, color=colors[k]) \n","          barlist = ax[i].plot(indices, metric.flatten(), color=colors[k], label=legend[k]) \n","          # for j, percent in enumerate(metric.flatten()):\n","          #   if percent < 0:\n","          #     barlist[j].set_color('r')\n","          #   else:\n","          #     barlist[j].set_color('g')\n","          ax[i].set_title(plot_labels[i]) # this many out of total number of points\n","          ax[i].set_xlabel(\"Number of Samples\")\n","          ax[i].set_ylabel(\"Error\")\n","          # ax[i].set_ylim((-50, 50))\n","          ax[i].set_xticks(indices)\n","          ax[i].legend()\n","          # rects = ax[i].patches\n","          # print(metric - baseline_metric)\n","          # bar_labels = [f\"{float(i)*100:.1f}\"  if len(inputs) == 2 else f\"{float(i):.1f}\" for i in (metric).flatten()]\n","          # for rect, label in zip(rects, bar_labels):\n","          #     height = rect.get_height()\n","          #     x_value = rect.get_x() + rect.get_width() / 2\n","\n","          #     # Number of points between bar and label. Change to your liking.\n","          #     space = 5\n","          #     # Vertical alignment for positive values\n","          #     va = 'bottom'\n","\n","          #     # If value of bar is negative: Place label below bar\n","          #     if height < 0:\n","          #         # Invert space to place label below\n","          #         space *= -1\n","          #         # Vertically align label at top\n","          #         va = 'top'\n","          #     ax[i].annotate(label, (x_value, height), xytext=(0, space), textcoords=\"offset points\", ha='center', va=va)\n","        # ax[i].legend()\n","    fig.show()\n","    plt.savefig(filename+ \".png\")\n","    plt.pause(0.1)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### eval"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import timeit\n","import scipy.io as io\n","import datetime\n","from dictances.chi_square import chi_square\n","import numpy as np\n","import random\n","import math\n","import matplotlib.pyplot as plt\n","import scipy.io as io\n","from typing import List, Dict, Any\n","from collections import Counter\n","from itertools import chain\n","from RR_RAPPOR import RAPPOR\n","from IDLDP.pg1 import IDLDP, LDP_MECHANISMS\n","from scipy.stats import entropy\n","calibrate_plt(plt)\n","\n","# LDP_MECHANISMS = [ \"hadamard\", \"oue_basic\", \"rappor_basic\", \"idldp_opt0\", \"rappor_idldp_opt1\", \"oue_idldp_opt_0\"]\n","\n","def generate_dist(output_distribution_list, in_list_sizes, element_map, alphabet_size,  verbose=True):\n","    # print(output_distribution_list)\n","    if len(output_distribution_list) == 1:\n","      # print(\"short circuiting generate dist\")\n","      # print(output_distribution_list)\n","      if len(output_distribution_list[0].shape) >1:\n","        # print([o.shape for o in output_distribution_list])\n","        output_distribution_list = [output_distribution_list[0][0]]\n","      return output_distribution_list[0]\n","    merged_pred = np.zeros(alphabet_size)\n","    for i, output_list in enumerate(output_distribution_list):\n","      if verbose  :\n","          print(f\"distribution number {i}\")\n","          print(f\"output_distributions: {output_list}\")\n","          print(f\"element_map: {element_map[str(i)]}\")\n","          print(f\"in list size: {in_list_sizes[i]}\")\n","      merged_pred[element_map[str(i)]] += np.nan_to_num(output_list)*in_list_sizes[i]\n","\n","    merged_pred = probability_normalize(merged_pred)\n","    return merged_pred\n","\n","def evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size, k=None, eps=None,                           epochs=None,  init=None, encode_acc = 1, encode_mode = 0,                          tiers=2, relaxation_mode=0, score_mode=0, growth_mode=0,                           privacy_budget=None, epoch_subdivisions=None, split_percentiles=None,                          num_repititions=10, baseline_arg_map=None, save_images=True, output_counts = True, seed=0,\n","                          idldp_config = None, ldp_mechanism=\"hadamard\", extra_metric_list=None,\\\n","                          obfuscate_heavy_hitters=True):\n","    # fix alphabet size and privacy levl, get the error plot with respect to sample size\n","    function_kwargs = locals()\n","    k = alphabet_size\n","    verbose = False\n","    topk_mode=True\n","    # output_counts = False # set to true for distribution over counts instead of counts\n","    # print(f\"evaluate_ground_truth kwargs: {function_kwargs}\")\n","    #Args:\n","    # k : alphabet size,  eps: privacy level, rep: repetition times to compute a point\n","    # epochs: number of points to compute\n","    \n","    # encode_acc : control whether to use fast encoding for hadamard responce\n","    #           recommended and default: 1 use fast encoding when k < 10000\n","    #                                    if memory use is high, disable this\n","    \n","    # mode: control encoding method for rappor and subset selection\n","    #       0 for standard, which is fast but memory intensive\n","    #       1 for light, which is relatively slow but not memory intensive\n","    #       2 for compress, where we compress the output of rappor and subsetselection into locations of ones.\n","    #       recommended and default: 0 when k <= 5000 n <= 1000000\n","    #                               if memory use is high, use light mode\n","    #       you can also create other modes by modifying the code\n","    # print('Alphabet size:', k)\n","    # print('Privacy level:', eps)\n","    if seed:\n","      np.random.seed(seed)\n","    baseline_flag = False\n","    if not epoch_subdivisions:\n","        epoch_subdivisions = epochs\n","    elif epoch_subdivisions == -1:\n","      epoch_subdivisions = 1\n","      baseline_flag = True\n","      if verbose:\n","        print(\"IN THE BASELINE EVALUATION\\n\")\n","    assert relaxation_mode in [0,1],\" only 2 relaxation modes implemented\"\n","    # indices = range(1, epoch_subdivisions + 1)\n","    indices = []\n","    prob_est_flag = False\n","\n","    extra_metric_list = ['kl_divergence', 'chi_square_distance', 'time', 'partition_weighted_rmse', 'weighted_rmse', 'inv_weighted_rmse', 'prob_est', 'variance', 'partition_inv_weighted_rmse']\n","\n","\n","    assert ldp_mechanism in LDP_MECHANISMS, f'ldp mechanism must be one of {LDP_MECHANISMS} but got {ldp_mechanism}'\n","    if LDP_MECHANISMS.index(ldp_mechanism) > 0: #if we in an IDLDP config\n","      assert idldp_config is not None, \"must have a valid idldp config\"\n","\n","    #TODO init the privacy budget if none?\n","    privacy_budget = init_privacy_budget(tiers, obfuscate_heavy_hitters=obfuscate_heavy_hitters) if privacy_budget is None else privacy_budget\n","\n","      # print(privacy_budget))\n","#     print(privacy_budget)\n","\n","    if baseline_flag:\n","      eps = max(privacy_budget)*eps\n","\n","    tot_mae = []\n","    tot_counts = []\n","    tot_count_mae = []\n","    tot_mse = []\n","    tot_count_mse = []\n","    tot_count_rmse = []\n","    tot_time = []\n","    partition_indices = []\n","\n","    extra_metrics = {} if extra_metric_list is None else {metric: [] for metric in extra_metric_list}\n","\n","\n","\n","\n","    epoch_growth_rate = get_growth_rate(df, epochs, mode=DEFAULT_EPOCH_GROWTH_RATE)\n","    # print(\"=\"*50)\n","\n","    # print(\"EPOCH GROWTH RATE\")\n","    # print(epoch_growth_rate)\n","    for epoch in range(epochs): # remember that this is for each set of final point sizes\n","        interim_metrics = {} if extra_metric_list is None else {metric: [] for metric in extra_metric_list}\n","        if verbose:\n","          print(f\"epoch number {epoch}\")\n","        subdiv_growth_rate = None\n","        epoch_df = copy.copy(df[:epoch_growth_rate[epoch]]) if epoch < epochs - 1 else df\n","        indices.append(len(epoch_df))\n","        if epoch_subdivisions > 1:\n","        # epoch_df = epoch_df.reset_index()\n","          subdiv_growth_rate = get_growth_rate(epoch_df, epoch_subdivisions, mode=growth_mode)\n","          # print(f\"epoch subdivision split points: {subdiv_growth_rate}\")\n","          epoch_subdivision_dfs = np.split(epoch_df, subdiv_growth_rate)\n","          # print(len(epoch_subdivision_dfs))\n","          # print(epoch_subdivisions)\n","\n","          assert len(epoch_subdivision_dfs) == epoch_subdivisions, \" should have the same numebr fo segments as subdivisions\"\n","        elif epoch_subdivisions == 1:\n","          epoch_subdivision_dfs = [epoch_df]\n","        else:\n","          assert False, \" epoch subdivision should be greater than 0\"\n","        subdiv_sizes = [len(e) for e in epoch_subdivision_dfs]\n","        # print(subdiv_sizes)\n","\n","        \n","        # TODO shuffle rows? subsection list, np.split, make that the in list....\n","        # print(\"=\"*50)\n","        # print(\"subdivisions\")\n","        # print(subdiv_growth_rate if subdiv_growth_rate is not None else \"all\")\n","        # # print(\"=\"*50)\n","        # print(len(epoch_subdivision_dfs))\n","\n","        # chunks = np.split(range(k), tiers)\n","        # element_map = {str(i): chunks[i] for i in range(tiers)}\n","        element_map = {str(i): list(range(k)) if i == 0 else [] for i in range(tiers)}\n","\n","        # to store l1 errors for each method\n","        l1_1 = [0]*epoch_subdivisions\n","        l1_2 = [0]*epoch_subdivisions\n","        l1_3 = [0]*epoch_subdivisions\n","        l1_4 = [0]*epoch_subdivisions\n","        \n","        # to store l2 errors for each method\n","        l2_1 = [0]*epoch_subdivisions\n","        l2_2 = [0]*epoch_subdivisions\n","        l2_3 = [0]*epoch_subdivisions\n","        l2_4 = [0]*epoch_subdivisions\n","        l2_5 = [0]*epoch_subdivisions\n","\n","        # to store decodign time for each method\n","        t1_1 = [0]*epoch_subdivisions\n","        t1_2 = [0]*epoch_subdivisions\n","        t1_3 = [0]*epoch_subdivisions\n","        t1_4 = [0]*epoch_subdivisions\n","        past_sizes = []\n","\n","        split_percentile_scores = np.zeros(alphabet_size)\n","        past_split_scores = []\n","\n","        for iter in range(epoch_subdivisions): # iterate with in the final point size over rthe number of points within\n","        # doesn't have to match the number of epochs but currently does\n","            if verbose:\n","              print(f\"Computing epoch subdivision {iter}\")\n","            in_list = epoch_subdivision_dfs[iter][col].to_numpy()\n","            # print(in_list)\n","            n = len(in_list)\n","            past_sizes.append(n)\n","\n","            if baseline_flag and tiers > 1: # bootstrap the ele tiers appropriately\n","              if verbose: \n","                print(\"bootstrapping tiers for baseline\")\n","              raw_counter = Counter(in_list)\n","              counter_with_zeros = {key: raw_counter[key] or 0 for key in element_map[\"0\"]}\n","              prob_est_topk_4 = counter_to_dist(counter_with_zeros)\n","\n","              if score_mode == 0:\n","                ranked_distribution_splits = get_random_element_split(prob_est_topk_4, split_percentiles, seed=seed)\n","              elif score_mode == 1:\n","                ranked_distribution_splits, split_percentile_scores = get_percentile_indices(prob_est_topk_4, split_percentiles)\n","              else:\n","                assert False, \"score mode not implemented for Baseline\"\n","              element_map = {str(i): ranked_distribution_splits[i] if i < len(ranked_distribution_splits) else [] for i in range(tiers)}\n","              # print(element_map)\n","\n","\n","            # print(f\"in list for iter {iter} with n {n} \\n \")\n","            # print(past_sizes)\n","            # print(np.sum(past_sizes))\n","            # print(f\"out of {len(epoch_df)}\")\n","\n","            #get subsectino of indices. \n","            count4 = 0\n","            t4 = 0\n","\n","            ele_domain_sizes = [len(element_map[v]) for v in element_map.keys()]\n","            if verbose:\n","              print(\"ele tiers \", ele_domain_sizes)\n","            enc = 1 if encode_acc == 1 else 0\n","            count_table = Counter()\n","\n","            index_list = [np.concatenate([np.where(in_list == x) for x in domain_elements], axis = 1).flatten() for domain_elements in element_map.values() if len(domain_elements)]\n","            # print(index_list)\n","            in_list_sizes = [len(in_list) for in_list in index_list]\n","            # ldp_mechanism = \"Oue_Basic\"\n","            if LDP_MECHANISMS.index(ldp_mechanism) == 0:\n","              # non_topk_sample_ind = np.array(range(n))\n","              ldp_instance_list = [Hadamard_Rand_high_priv(q, eps*privacy_budget[i], enc) for i, q in enumerate(ele_domain_sizes) if q != 0]\n","              encoding_list = [ldp_instance_list[i].encode_string(in_list[domain_elements]) for i, domain_elements in enumerate(index_list)]\n","              #first generate actual subset of elements we want to split out from this sampling\n","            else:\n","              #we want to modify the ind_to_tier map based on the first round respoonse\n","              #for the first round response just use OUE_basic/rappor_basic? seems unfair to rappor but im sure people using rappor have areason for using it not OUE.\n","              #TODO\n","              if LDP_MECHANISMS.index(ldp_mechanism) < 3: #idldp opts\n","                if verbose:\n","                  print(\"oue / rappor generatio\")\n","                ldp_instance_list = [RAPPOR(len(v), eps*privacy_budget[i], config=idldp_config, input_map ={d:j for j,d in enumerate(v)}) for i, (k,v) in enumerate(element_map.items()) if len(v) != 0]\n","                encoding_list = [ldp_instance_list[i].encode_string(in_list[domain_elements]) for i, domain_elements in enumerate(index_list)]\n","              else:\n","                if iter == 0 and not baseline_flag:\n","                  if verbose:\n","                    print(\"Defaulting to OUE\")\n","                  ldp_instance_list = [RAPPOR(alphabet_size, eps)]#Default to OUE\n","                else:\n","                  if verbose:\n","                    print(\"idldp mechanisms\")\n","                  new_ind_to_tier = {}\n","                  for key, vals in element_map.items():\n","                    new_ind_to_tier.update({v: key for v in vals})\n","                  idldp_config[\"ind_to_tier\"] = new_ind_to_tier\n","                  ldp_instance_list = [RAPPOR(alphabet_size, 1.6969696969, config=idldp_config)]#note the epsilon set here does not affect flip probabilities in idldp so it doesnt' matter\n","                encoding_list = [ldp_instance_list[0].encode_string(in_list)] #for IDLDP methods just encode the entire string\n","\n","            #TODO pass all the params in and the ntest this tomorrow YAATEHR\n","\n","\n","            start_time = timeit.default_timer()\n","            output_distribution_list = []\n","            if relaxation_mode == -1 or iter == 0:\n","              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) for i, encoding in enumerate(encoding_list)]\n","            elif relaxation_mode == 0:\n","              raw_counter = Counter(in_list[index_list[0]])\n","              counter_with_zeros = {key: raw_counter[key] or 0 for key in element_map[\"0\"]}\n","              for i, key in enumerate(counter_with_zeros.keys()):\n","                  assert key == element_map[\"0\"][i], \"key and element key should match\"\n","              prob_est_topk_4 = counter_to_dist(counter_with_zeros)\n","              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) if i != 0 else prob_est_topk_4 for i, encoding in enumerate(encoding_list)]\n","\n","            elif relaxation_mode == 1:\n","              if verbose: \n","                print(\"relaxation mode 1 split relaxation\")\n","              output_distribution_list = [ldp_instance_list[i].decode_string(encoding) for i, encoding in enumerate(encoding_list)]\n","            else:\n","              assert False, \"should have a new defined relaxation mode\"\n","\n","            # print(in_list_sizes)\n","\n","            prob_est = generate_dist(output_distribution_list, in_list_sizes, element_map, alphabet_size, verbose=verbose)\n","            # print(prob_est)\n","            if not baseline_flag: #Compute ranked distribution splits with logic for making sure percentiles add up to 1\n","            #Note that whne we are in the baselines we just compute the ranks once\n","              if len(split_percentiles) == 1 and len(index_list) == 1:\n","                if verbose:\n","                  print(\"FIRST BRNACH OF PERCENTILE MDOS\")\n","                current_split_percentiles = split_percentiles\n","              elif len(split_percentiles) > len(index_list) -1 and iter==0:\n","                current_split_percentiles = split_percentiles\n","              elif len(split_percentiles) > len(index_list) -1:\n","                if verbose:\n","                  print(len(split_percentiles), len(index_list))\n","                  print(in_list_sizes)\n","                  print(ldp_instance_list)\n","                sheered_percentiles = split_percentiles[len(index_list) -1:]\n","                slack_percentile = np.sum(sheered_percentiles) / ( len(index_list) -1 )\n","                # print(\"updating slack percentile \", slack_percentile)\n","                assert slack_percentile > 0 and slack_percentile < 1, \"slakc percentile should be a fraction\"\n","                current_split_percentiles = split_percentiles[len(index_list)-1:] + slack_percentile\n","                #TODO TEST TEH DISTRIBUTION SPLITS\n","              elif len(split_percentiles) == len(index_list) -1: # ie bootstrap iteration\n","                if verbose:\n","                  print(\"LAST BRANCH OF PERCENTILE MODS\")\n","                current_split_percentiles = split_percentiles\n","              else:\n","                assert False, \"Will end up throwing away sketched results\"\n","\n","              if score_mode == 0:\n","                ranked_distribution_splits = get_random_element_split(prob_est, split_percentiles, seed=seed)\n","              elif score_mode == 1:\n","                ranked_distribution_splits, split_percentile_scores = get_percentile_indices(prob_est, current_split_percentiles,topk_mode=topk_mode)\n","              elif score_mode == 2:\n","                ranked_distribution_splits, split_percentile_scores = get_rank_order_splits(prob_est, current_split_percentiles, past_split_scores, topk_mode=topk_mode, weights=past_sizes)\n","                past_split_scores.append(split_percentile_scores)\n","              else:\n","                assert False, \"score mode not implemented\"\n","\n","              element_map = {str(i): ranked_distribution_splits[i] if i < len(ranked_distribution_splits) else [] for i in range(tiers)}\n","              # print(element_map)\n","\n","            t4 = t4 + timeit.default_timer() - start_time\n","            count4 = prob_est*n \n","            if verbose:\n","              print(f\"for iter {iter} the counts sum to {np.sum(prob_est*n)}\")\n","            l1_4[iter] = count4\n","            l2_4[iter] = prob_est\n","            subdiv_mse = None\n","            epoch_subdiv_counts, _ = np.histogram(in_list,range(alphabet_size+1))\n","            # print(epoch_subdiv_counts)\n","            epoch_subdiv_dist = probability_normalize(epoch_subdiv_counts)\n","            if 'time' in extra_metric_list:\n","              interim_metrics['time'].append(t4)\n","            if 'variance' in extra_metric_list:\n","              prob_est_flag = True\n","              # interim_metrics['variance'].append(variance) # this hsould be the vraiance of rh estimated dsitribution. The variance ofht eestimator we can predict using the output MSE's\n","            if 'partition_weighted_rmse' in extra_metric_list:\n","              subdiv_mse = (epoch_subdiv_dist - prob_est)**2\n","              partition_weighted_rmse = np.average(subdiv_mse, axis=0, weights = epoch_subdiv_dist)\n","              # if iter > 1:\n","              #   print(\"for iter: \", iter)\n","              #   print(partition_weighted_rmse)\n","              #   print(subdiv_mse)\n","              #   print(epoch_subdiv_dist)\n","              #   print(prob_est)\n","              #   print('')\n","              # assert not partition_weighted_rmse.isnan()\n","              interim_metrics['partition_weighted_rmse'].append(partition_weighted_rmse)\n","            if 'partition_inv_weighted_rmse' in extra_metric_list:\n","              if subdiv_mse is None:\n","                subdiv_mse = (epoch_subdiv_dist - prob_est)**2\n","\n","              partition_inv_weighted_rmse = np.average(subdiv_mse, axis=0, weights = 1 - epoch_subdiv_dist)\n","              interim_metrics['partition_inv_weighted_rmse'].append(partition_inv_weighted_rmse)\n","            if 'weighted_rmse' in extra_metric_list:\n","              prob_est_flag = True\n","            if 'inv_weighted_rmse' in extra_metric_list:\n","              prob_est_flag = True\n","            if 'prob_est' in extra_metric_list  or prob_est_flag:\n","              interim_metrics['prob_est'].append(prob_est)\n","\n","            \n","            t1_4[iter] = t4\n","        # print(f\"counts per iterations: {l1_4}\")\n","        \n","        tot_counts.append(np.sum(l1_4, axis=0))\n","        tot_count_mae.append(np.average(l2_4, axis=0, weights=past_sizes))\n","        # print(tot_counts)\n","        tot_time.append(np.sum(t1_4))\n","\n","        # print(interim_metrics['partition_weighted_rmse'])\n","        if 'time' in extra_metric_list:\n","          extra_metrics['time'].append(np.sum(interim_metrics['time']))\n","\n","        epoch_true_counts, _ = np.histogram(epoch_df[col].to_numpy(),range(alphabet_size + 1))\n","        epoch_true_dist = probability_normalize(epoch_true_counts)\n","        extra_metrics['epoch_true_dist'] = epoch_true_dist\n","\n","        if 'prob_est' in extra_metric_list or prob_est_flag:\n","          extra_metrics['prob_est'].append(np.average(interim_metrics['prob_est'], axis=0, weights = subdiv_sizes))\n","        if 'variance' in extra_metric_list:\n","          pass #TOD implement this after the 20 runs\n","        if 'partition_weighted_rmse' in extra_metric_list:\n","          extra_metrics['partition_weighted_rmse'].append(interim_metrics['partition_weighted_rmse'])\n","        if 'partition_inv_weighted_rmse' in extra_metric_list:\n","          extra_metrics['partition_inv_weighted_rmse'].append(interim_metrics['partition_inv_weighted_rmse'])\n","        if 'weighted_rmse' in extra_metric_list or 'partition_weighted_rmse' in extra_metric_list:\n","          if 'partition_weighted_rmse' in extra_metric_list:\n","            extra_metrics['weighted_rmse'].append(np.average(interim_metrics['partition_weighted_rmse'], axis=0, weights = subdiv_sizes))\n","          else:\n","            subdiv_mse = (epoch_true_dist - extra_metrics['prob_est'][-1])**2\n","            weighted_rmse = np.average(subdiv_mse, axis=0, weights = epoch_true_dist)\n","            extra_metrics['weighted_rmse'].append(weighted_rmse)\n","        if 'inv_weighted_rmse' in extra_metric_list or 'partition_inv_weighted_rmse' in extra_metric_list:\n","          if 'partition_inv_weighted_rmse' in extra_metric_list:\n","            extra_metrics['inv_weighted_rmse'].append(np.average(interim_metrics['partition_inv_weighted_rmse'], axis=0, weights = subdiv_sizes))\n","          else:\n","            subdiv_mse = (epoch_true_dist - extra_metrics['prob_est'][-1])**2\n","            weighted_rmse = np.average(subdiv_mse, axis=0, weights = 1- epoch_true_dist)\n","            extra_metrics['inv_weighted_rmse'].append(weighted_rmse)\n","        if 'topk_precision' in extra_metric_list:\n","          pass\n","          # extra_metrics['topk_precision'].append(interim_metrics['topk_precision']) # this one we don't want to average for the entire run... we could , but it may be insteresting to seee this as the epochhs go....\n","          #thus we could have a ton of line graphs, or we could just look at the experimentals (the baselines are random...) we couuld aggregate this for a cross experimetn one...\n","        if 'kl_divergence' in extra_metric_list in extra_metric_list:\n","          non_zero_prob_est = extra_metrics['prob_est'][-1]\n","          non_zero_prob_est_indices = non_zero_prob_est != 0\n","          extra_metrics['kl_divergence'].append(entropy(extra_metrics['prob_est'][-1][non_zero_prob_est_indices], epoch_true_dist[non_zero_prob_est_indices]))\n","            \n","        if 'chi_square_distance' in extra_metric_list:\n","          non_zero_true_dist_indices = epoch_true_dist != 0\n","          \n","\n","          extra_metrics['chi_square_distance'].append(chi_square(dict(zip(range(alphabet_size + 1), extra_metrics['prob_est'][-1])), dict(zip(range(alphabet_size + 1), epoch_true_dist))))\n","        partition_indices.append(np.cumsum(subdiv_sizes))# we have all subdiv dfs...\n","    # print(partition_indices)\n","    if verbose:\n","      print(\"eval computation loop over\\n\\n\")\n","    # COmpute all errors from counts\n","    ground_truth_counts = ground_truth_hist[col]\n","    ground_truth_val, ground_truth_key = ground_truth_counts.to_numpy(), ground_truth_counts.keys()\n","    total_number_of_elements = np.sum(ground_truth_val) #looks strange but these are one hot.\n","\n","    for i, eval_counts in enumerate(tot_counts):\n","      if verbose:\n","        print(\"enumerating thorugh tot_counts (eval counts)\")\n","        print(len(tot_counts))\n","        print(len(epoch_subdivision_dfs))\n","      eval_map = defaultdict(lambda: 0, zip(range(alphabet_size), eval_counts))\n","      comparison_map = defaultdict(lambda: 0, zip(ground_truth_key, (ground_truth_val if output_counts else ground_truth_val/total_number_of_elements)                                                   * (epoch_growth_rate[i]/total_number_of_elements if i < len(tot_counts)-1 and output_counts else 1)))\n","      differences = {k: abs((eval_map[k] or 0) - x ) for k, x in comparison_map.items()}\n","      if verbose:\n","        print(f\"for iter {i} the computed counts sum to {np.sum(list(eval_map.values()))}\")\n","        print(f\"for iter {i} the actual counts sum to {np.sum(list(comparison_map.values()))}\")\n","        print(\"percentage of counts for ground truth: \", epoch_growth_rate[i]/total_number_of_elements if i < len(tot_counts)-1 else 1)\n","        print(\"differences between eval and comparison[0]: \", list( differences.items())[0])\n","        print(eval_map[0], comparison_map[0])\n","        print(\"eval map[0] vs ground truth (comparison) map[0] ^\")\n","      # mae = np.linalg.norm(list(differences.values()), ord=1) / (epoch_growth_rate[i] if i < len(tot_counts)-1 else total_number_of_elements)\n","      rmse = np.linalg.norm(list(differences.values()), ord=2) / (epoch_growth_rate[i] if i < len(tot_counts)-1 else total_number_of_elements)\n","\n","      mse = rmse ** 2\n","      tot_mse.append(mse)\n","      tot_count_rmse.append(rmse)\n","      # tot_mae.append(mae)\n","\n","    output_counts=False\n","\n","    for i, eval_counts in enumerate(tot_count_mae):\n","      if verbose:\n","        print(\"enumerating thorugh tot_count_mae (distribution not counts)\")\n","        print(len(tot_count_mae))\n","        print(len(epoch_subdivision_dfs))\n","      eval_map = defaultdict(lambda: 0, zip(range(alphabet_size), eval_counts))\n","      comparison_map = defaultdict(lambda: 0, zip(ground_truth_key, (ground_truth_val if output_counts else ground_truth_val/total_number_of_elements)                                                   * (epoch_growth_rate[i]/total_number_of_elements if i < len(tot_count_mae)-1 and output_counts else 1)))\n","      differences = {k: abs((eval_map[k] or 0) - x ) for k, x in comparison_map.items()}\n","      if verbose:\n","        print(f\"for iter {i} the computed counts sum to {np.sum(list(eval_map.values()))}\")\n","        print(f\"for iter {i} the actual counts sum to {np.sum(list(comparison_map.values()))}\")\n","        print(\"percentage of counts for ground truth: \", epoch_growth_rate[i]/total_number_of_elements if i < len(tot_count_mae)-1 else 1)\n","        print(\"differences between eval and comparison[0]: \", list( differences.items())[0])\n","        print(eval_map[0], comparison_map[0])\n","        print(\"eval map[0] vs ground truth (comparison) map[0] ^\")\n","      # mae = np.linalg.norm(list(differences.values()), ord=1) / (epoch_growth_rate[i] if i < len(tot_count_mae)-1 else total_number_of_elements)\n","      rmse = np.linalg.norm(list(differences.values()), ord=2) / (epoch_growth_rate[i] if i < len(tot_count_mae)-1 else total_number_of_elements)\n","\n","      # mse = rmse ** 2\n","      # tot_mse.append(mse)\n","      # tot_count_rmse.append(rmse)\n","      tot_mae.append(rmse)\n","\n","    if verbose:\n","      print(\"indices: \", indices)\n","      print(\"total mae: \", tot_mae)\n","    plot_label = \"hr - ILDP rollup error\"\n","\n","\n","\n","    ################ BASELINE CALL #################\n","\n","    if baseline_arg_map:\n","      baseline_data = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size, **baseline_arg_map)\n","      b_data = {f\"baseline_{k}\": v for k,v in baseline_data.items()}\n","    else:\n","     b_data = {}\n","\n","\n","    #save all the data into a mat file with time stamp\n","    time = datetime.datetime.now().strftime(\"%m_%d_%H_%M\")\n","    indices = np.array(indices)\n","    data = {\n","        'time' : time,\n","        'absz' : k,\n","        'privacy' : eps,\n","        'indices' : indices, # indices of each point (number of samples)\n","        'hr_error': tot_mae,\n","        'hr_error_l2': tot_mse,\n","        'hr_count_error_rmse': tot_count_rmse,\n","        'hr_time': tot_time,\n","        'extra_metrics': extra_metrics,\n","        'extra_metric_list': extra_metric_list,\n","        'partition_indices': partition_indices,\n","        **b_data\n","    }\n","    para = 'k_{}_eps_{}_{}'.format(k,eps,privacy_budget)\n","\n","    baseline_metrics = []\n","    input_metrics = []\n","    if baseline_arg_map:\n","      baseline_l1 = np.array(baseline_data[\"hr_error\"])\n","      baseline_l2 = np.array(baseline_data[\"hr_error_l2\"])\n","      baseline_rmse = np.array(baseline_data[\"hr_error_l2\"])**.5\n","      data[\"baseline_hr_error\"] = baseline_l1\n","      data[\"baseline_hr_error_l2\"] = baseline_l2\n","      data[\"baseline_hr_count_error_rmse\"] = baseline_rmse\n","    return data\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"markdown","metadata":{},"source":[" #DATA Generation\n","\n"," ## Schema\n"," | Name | Type | Description | Example |\n"," | ---- | ---- | ----------- | ------- |\n"," | **key** | `string` | Unique string identifying the region | US_CA |\n"," | **country_region** | `string` | The name of the country in English. | United States |\n"," | **country_region_code** | `string` | The [ISO 3166-1](https://en.wikipedia.org/wiki/ISO_3166-1) code for the country. | US |\n"," | **sub_region_1** | `string` | The name of a region in the country. | California |\n"," | **sub_region_1_code** | `string` | A country-specific [ISO 3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code for the region. | US-CA |\n"," | **sub_region_2** | `string` | The name of a subdivision of the region above. For example, *Santa Clara County*. |  |\n"," | **sub_region_2_code** | `string` | For the US - The [FIPS code](https://en.wikipedia.org/wiki/FIPS_county_code) for a US county (or equivalent). | 06085 |\n"," | **date** | `string` | The day on which the searches took place. For weekly data, this is the first day of the 7-day weekly interval starting on Monday. For example, in the weekly data the row labeled *2020-07-13* represents the search activity for the week of July 13 to July 19, 2020, inclusive. Calendar days start and end at midnight, Pacific Standard Time. | 2020-07-13 |\n"," | **`${symptom name}`** | `double` `[0-100]` | Repeated for each symptom. Reflects the normalized search volume for this symptom, for the specified date and region. The field may be empty when data is not available. | 87.02 |\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().system('pip install fuzzywuzzy')\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","import urllib.request\n","from fuzzywuzzy import fuzz, process\n","\n","def find_related_cols(columns, keywords):\n","  output = list()\n","  # processor = lambda x: \" \".join(x.split(\"_\"))\n","  for keyword in keywords:\n","    choices = process.extractWithoutOrder(keyword, columns, score_cutoff=80)\n","    # print(f\"found the following choices for {keyword}: {choices}\")\n","    output.extend(choices)\n","\n","  return list(set(output))\n","\n","GENERIC_SYMPTOM_KEYWORDS = [\n","                    \"Fever\",\n","                    \"chills\",\n","                    \"breath\",\n","                    \"fatigue\",\n","                    # \"ache\",\n","                    \"taste\",\n","                    \"smell\",\n","                    \"throat\",\n","                    \"congestion\",\n","                    \"runny\",\n","                    \"nausea\",\n","                    \"vomiting\",\n","                    \"diarrhea\",\n","                    # \"muscle\"\n","]\n","\n","\n","pd.set_option('display.max_row', 500)\n","pd.set_option('display.max_columns', 100)\n","DATA_URL = \"https://storage.googleapis.com/covid19-open-data/v2/google-search-trends.csv\"\n","\n","\n","# Load CSV data directly from the URL with pandas, the options are needed to prevent\n","# reading of records with key \"NA\" (Namibia) as NaN\n","\n","class MockArgs(object):\n","    def __init__(self):\n","        pass\n","\n","class Dataset(object):\n","  \"\"\"\n","  class to load and compute dataset manipulations for symptom dgeneration\n","  \"\"\"\n","\n","  def __init__(self, data_url, args=None):\n","    self.args = args if args else MockArgs()\n","\n","    if not os.path.exists(DATA_FILE_PATH):\n","        if not os.path.exists(os.path.dirname(DATA_FILE_PATH)):\n","            os.makedirs(os.path.dirname(DATA_FILE_PATH), exist_ok=True)\n","        urllib.request.urlretrieve(DATA_URL, DATA_FILE_PATH)\n","\n","    self.df = self._prepare_dataframe(data_url)\n","\n","    \n","  def _prepare_dataframe(self, data_url: str) -> pd.DataFrame:\n","      \"\"\"\n","      Loads the dataset and cleans it up\n","      :param data_url: the url containing the original data\n","      :return: a Pandas DataFrame with the historical data\n","      \"\"\"\n","      symptom_data = pd.read_csv(\n","          data_url,\n","          keep_default_na=False,\n","          na_values=[\"\"],\n","      )\n","      print(symptom_data.columns)\n","      return symptom_data\n","\n","\n","# symptom_data = pd.read_csv(\n","#     DATA_FILE_PATH,\n","#     keep_default_na=False,\n","#     na_values=[\"\"],\n","# )\n","# symptom_data[\"date\"] = pd.to_datetime(symptom_data[\"date\"])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idldp = IDLDP()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Parallel eval pipeline\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from itertools import chain\n","import pickle\n","import copy\n","from IDLDP.pg1 import ldp_mechanism_helper, IDLDP\n","\n","def evaluate_dataset_parallel(df, target_col_groups,\n","                              metadata_cols=None,\n","                              score_mode =0,\n","                              growth_mode=0,\n","                              k=None,\n","                              eps=None,\n","                              split_percentiles=None,\n","                              epochs=None,\n","                              step_sz=None,\n","                              init=None,\n","                              dist=None,\n","                              encode_acc = 1,\n","                              encode_mode = 0,\n","                              tiers=2,\n","                              relaxation_mode=0,\n","                              privacy_budget=None,\n","                              epoch_subdivisions=None,\n","                              BASELINE_VERSION = 0,\n","                              num_repititions = 10,\n","                              output_counts=False,\n","                              save_images=False,\n","                              ldp_mechanism=\"hadamard\",\n","                              baseline_mechanism=\"hadamard\",\n","                              ):\n","\n","  #preprocess df to contain categories\n","  # selecting cols by name\n","  # print(f\"evaluate_dataset_parallel kwargs: {locals()}\")\n","  # locals().update(kwargs)\n","\n","\n","  arg_map = copy.deepcopy(locals())\n","  arg_map.pop('df', None)\n","  arg_map.pop('target_col_groups', None)\n","  arg_map.pop('metadata_cols', None)\n","  arg_map.pop('step_sz', None)\n","  arg_map.pop('dist', None)\n","  arg_map.pop('baseline_mechanism', None)\n","  arg_map.pop('BASELINE_VERSION', None)\n","  baseline_arg_map = copy.deepcopy(arg_map)\n","\n","\n","  #For output grahps (all are computed, not all are displayed)\n","  # Out of all possible: interest_cols = [\"hr_error\", \"hr_error_l2\", \"hr_count_error_rmse\"]\n","  # interest_cols = [\"l1_error\", \"l2_error\", \"count_rmse\"]\n","  # interest_cols = [\"hr_error\", \"hr_error_l2\", \"hr_count_error_rmse\"]\n","  interest_cols = [\"hr_error\", \"hr_count_error_rmse\"]\n","\n","\n","\n","  # HR Baseline.\n","  hr_baseline = { #zero shot HR (vanilla)\n","      'ldp_mechanism': baseline_mechanism,\n","      'epoch_subdivisions': -1,\n","      'score_mode': 0, # DOesn't actually matter, nveer used.\n","      'tiers': 1,\n","      'split_percentiles': None, #doesnt' matter, all in one run\n","      'privacy_budget': [1],\n","  }\n","\n","  #Random SPlit Hadamard Response.\n","\n","  random_split_baseline = {# Random prior HR\n","      'ldp_mechanism': baseline_mechanism,\n","      'epoch_subdivisions': -1,\n","      # 'split_percentiles': None, # becomes uniform by number of tiers\n","      'score_mode': 0, # random scores\n","      # 'tiers': 2,\n","  }\n","\n","  # Percentile split hadamard response.\n","  percentile_split_baseline = { #oracle prior HR\n","      'ldp_mechanism': baseline_mechanism,\n","      'epoch_subdivisions': -1, #TODO change to the number of epoch subdivisions we have? This triggers baseline flag logic (ie we start to recompute the scores)\n","      # 'split_percentiles': [.1],\n","      'score_mode': 1,\n","      # 'tiers': 2,\n","  }\n","\n","  oue_baseline = {\n","    'ldp_mechanism': 'Oue_Basic',\n","    'epoch_subdivisions': -1,\n","    'score_mode': 0, # DOesn't actually matter, nveer used.\n","    'tiers': 1,\n","    'split_percentiles': None, #doesnt' matter, all in one run\n","    'privacy_budget': [1],\n","}\n","  zero_shot_hadamard = {# IF THE EXPERIMENTAL MECHANISM IS HADAMARD THIS IS THE SAME.....\n","    'ldp_mechanism': 'Hadamard',\n","    'epoch_subdivisions': -1,\n","    'score_mode': 0, # DOesn't actually matter, nveer used.\n","    'tiers': 1,\n","    'split_percentiles': None, #doesnt' matter, all in one run\n","    'privacy_budget': [1],\n","}\n","  random_split_iterative_baseline = {\n","      'ldp_mechanism': baseline_mechanism,\n","      # 'epoch_subdivisions': -1,\n","      # 'split_percentiles': split_percentiles, # becomes uniform by number of tiers\n","      'score_mode': 0, # random scores\n","      # 'tiers': 2,\n","  }\n","\n","  # Percentile split hadamard response.\n","  percentile_split_iterative_baseline = {\n","      'ldp_mechanism': ldp_mechanism,\n","      # 'epoch_subdivisions': -1, #TODO there is a bug here. in the percentile split. random with variable epochs and iters works fine...\n","      # 'split_percentiles': [.1],\n","      'score_mode': 1,\n","      # 'tiers': 2,\n","  }\n","\n","  other_experimentals = [{\"ldp_mechanism\": x} for x in LDP_MECHANISMS]\n","  # other_experimentals = [{\"ldp_mechanism\": x} for x in LDP_MECHANISMS]\n","\n","  baselines = [hr_baseline, random_split_baseline, percentile_split_baseline, oue_baseline, zero_shot_hadamard, random_split_iterative_baseline, percentile_split_iterative_baseline]\n","  baselines += other_experimentals\n","\n","\n","  # baseline_arg_map[\"split_percentiles\"] = None\n","  # baseline_arg_map[\"tiers\"] = 1\n","  # # baseline_arg_map[\"point_growth_rate\"]=0\n","  # baseline_arg_map[\"score_mode\"] = 0\n","  print(\"experimental_args: \", arg_map)\n","  print(\"\\n\")\n","  # print(\"baseline_args: \", baseline_arg_map)\n","  # print(\"\\n\\n\")\n","\n","\n","\n","\n","\n","  metadata_cols = [] if not metadata_cols else metadata_cols\n","  g = df.columns.to_series().groupby(df.dtypes).groups\n","  type_to_col = {k.name: v for k, v in g.items()}\n","  string_col_codes_and_labels = {}\n","  num_elements = len(df)\n","  ground_truth_hist = {}\n","  results= {}\n","\n","  if \"object\" in type_to_col.keys():\n","    for col in type_to_col[\"object\"]:\n","      if col in metadata_cols:\n","        continue\n","      cat = pd.Categorical(df[col])\n","      string_col_codes_and_labels[col] = (cat.codes, cat.categories)\n","      df[col] = cat.codes\n","  \n","  summed_results = np.zeros((6, epochs))\n","  listed_results = []\n","\n","  eval_data_dicts = []\n","  experimental_results = []\n","  experimental_extra_metrics = []\n","  baseline_results = []\n","  baseline_extra_metrics = []\n","\n","  subset_partition_indices = []\n","  indices = []\n","\n","  total_alphabet_size = 0\n","  config_cache = {f\"eval_{i}\": None for i in range(len(target_col_groups))}\n","  if not isinstance(BASELINE_VERSION, list):\n","    BASELINE_VERSION = [BASELINE_VERSION]\n","  for b in BASELINE_VERSION:\n","    config_cache.update({f\"baseline_{b}_{i}\": None for i in range(len(target_col_groups))})\n","\n","  for rep_no in range(num_repititions):\n","      \n","    df = df.sample(frac=1).reset_index(drop=True)\n","    total_alphabet_size = 0\n","\n","    for i, col_group in enumerate(target_col_groups):\n","      col = f\"col_group_{i}\"\n","      df[col] = df[col_group].astype(str).apply(''.join, axis=1)\n","      cat = pd.Categorical(df[col])\n","      string_col_codes_and_labels[col] = (cat.codes, cat.categories)\n","      df[col] = cat.codes\n","      alphabet_size = len(df[col].unique())\n","      total_alphabet_size += alphabet_size\n","\n","      for c in chain(col_group, [col]):\n","        #make histogram\n","        col_histogram = df[c].value_counts(sort=False)\n","        # print(col_histogram)\n","        # ground_truth_hist[c] = (col_histogram.to_numpy() / num_elements, col_historgram.index.to_list())\n","        ground_truth_hist[c] = col_histogram\n","      \n","\n","      opt_mode = ldp_mechanism_helper(ldp_mechanism)\n","      # split_percentiles = np.cumsum(np.array([1/tiers]*(tiers-1))) if not split_percentiles else split_percentiles\n","      assert len(split_percentiles) == tiers -1, f\"split percentilse must match tiers but got {len(split_percentiles)} and {tiers -1}\"\n","\n","      #convert np.split points to a distribution over high to low priv elements\n","\n","      def split_percentiles_to_tier_splits(split_points:List[float]):\n","        # cum_split = np.cumsum(split_points).tolist()\n","        # cum_split.insert(0, 0)\n","        # cum_split.pop(-1)\n","        # temp = np.subtract(cum_split,)\n","        tier_splits = np.concatenate([split_points, [1- np.sum(split_points)]]).tolist()\n","        return tier_splits\n","\n","\n","      if opt_mode != -1 and config_cache[f\"eval_{i}\"] is None:\n","        tier_split_percentages = split_percentiles_to_tier_splits(split_percentiles)\n","        print(\"first config generation\")\n","        _, _, idldp_config = idldp.gen_perturbation_probs(\n","          epsilon=eps,\n","          privacy_budget=privacy_budget, #\n","          tier_split_percentages=tier_split_percentages,\n","          domain_size=alphabet_size,\n","          total_records= None,#Doesn't actually affect anything....\n","          opt_mode=opt_mode,\n","        )\n","        config_cache[f\"eval_{i}\"] = idldp_config\n","\n","\n","      col_group_results = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size,\n","        score_mode=score_mode, k=alphabet_size, eps=eps, epochs=epochs, init=init, encode_acc = encode_acc,\n","        encode_mode = encode_mode, tiers=tiers, relaxation_mode=relaxation_mode,\n","        privacy_budget=privacy_budget, epoch_subdivisions=epoch_subdivisions,\n","        split_percentiles=split_percentiles, baseline_arg_map=None,\n","        save_images=save_images, output_counts = output_counts,\n","        ldp_mechanism=ldp_mechanism,\n","        )\n","      \n","      results[col] = [col_group_results]\n","\n","      for b in BASELINE_VERSION:\n","        baseline_arg_map = copy.deepcopy(arg_map)\n","        baseline_arg_map.update(baselines[b])\n","\n","\n","        baseline_arg_map[\"split_percentiles\"] = np.cumsum(np.array([1/baseline_arg_map[\"tiers\"]]*(baseline_arg_map[\"tiers\"]-1))) if (\"split_percentiles\" not in baseline_arg_map.keys() or baseline_arg_map[\"split_percentiles\"] is None) else baseline_arg_map[\"split_percentiles\"]\n","        assert baseline_arg_map[\"tiers\"] == 1 or len(baseline_arg_map[\"split_percentiles\"]) == baseline_arg_map[\"tiers\"] -1, f\"split percentilse must match tiers but got {len(baseline_arg_map['split_percentiles'])} and {baseline_arg_map['tiers'] -1}\"\n","        \n","        opt_mode = ldp_mechanism_helper(baseline_arg_map[\"ldp_mechanism\"])\n","\n","        if opt_mode != -1 and config_cache[f\"baseline_{b}_{i}\"] is None:\n","          tier_split_percentages = split_percentiles_to_tier_splits(split_percentiles)\n","          _, _, idldp_config = idldp.gen_perturbation_probs(\n","            epsilon=eps,\n","            privacy_budget=privacy_budget, #\n","            tier_split_percentages=tier_split_percentages,\n","            domain_size=alphabet_size,\n","            total_records = None,#Doesn't actually affect anything....\n","            opt_mode=opt_mode,\n","          )\n","          config_cache[f\"baseline_{b}_{i}\"] = idldp_config\n","\n","\n","        baseline_arg_map[\"idldp_config\"] = config_cache[f\"baseline_{b}_{i}\"] \n","\n","\n","        if rep_no == 0:\n","          print(\"baseline_args: \", baseline_arg_map)\n","          print(\"\\n\\n\")\n","\n","        baseline_data = evaluate_ground_truth(ground_truth_hist, df, col, alphabet_size,\n","          **baseline_arg_map)\n","        results[col].append(baseline_data)\n","        \n","\n","    for col in [f\"col_group_{i}\" for i in range(len(target_col_groups))]:\n","      r = results[col] # we have a list of true, + variable baselines\n","\n","      eval_data_dicts.extend([l for l in r])\n","\n","      experimental_results.append([r[0][interest] for interest in interest_cols])\n","      experimental_extra_metrics.append(r[0]['extra_metrics'])\n","      baseline_results.append([[l[interest] for interest in interest_cols] for i,l in enumerate(r) if i > 0])\n","      baseline_extra_metrics.append([l['extra_metrics'] for i,l in enumerate(r) if i > 0])\n","      if rep_no == 0:\n","        # subset_partition_indices.extend([l['partition_indices'][0] for l in r])\n","        subset_partition_indices.extend([l['partition_indices'][-1] for l in r])\n","\n","\n","\n","      #to get the relative results for each baseline we will need to stack each group\n","\n","      # stack = np.vstack((r[c] for c in chain(interest_cols, [f\"baseline_{x}\" for x in interest_cols])))\n","      # print(stack.shape)\n","      # print(summed_results.shape)\n","      # summed_results += stack\n","      # listed_results.append(stack)\n","\n","  # summed_results/= num_repititions\n","  # total_alphabet_size /= num_repititions\n","  # listed_results = np.array(listed_results)\n","  # scatter_results = [listed_results[:,i,:] for i in range(6)]\n","  # print(scatter_results)\n","  # print(f\"scatter results of len {len(scatter_results)} shape is: \")\n","  # print(scatter_results[0].shape)\n","  extra_metrics_list = list(experimental_extra_metrics[0].keys())\n","  transpose_metrics = ['partition_inv_weighted_rmse', 'partition_weighted_rmse']\n","  unnested_metrics = {key: np.array([e[key] for e in experimental_extra_metrics]) for key in extra_metrics_list}\n","  unnested_baseline_metrics = {key: np.array([[l[key] for l in e] for e in baseline_extra_metrics]) if key not in transpose_metrics else np.array([[l[key] for l in e] for e in baseline_extra_metrics], ndmin=4) for key in extra_metrics_list}\n","  for key in extra_metrics_list:\n","    if key in transpose_metrics:\n","      unnested_metrics[key] = unnested_metrics[key].transpose(2,0,1)\n","      # print(unnested_baseline_metrics[key].shape)\n","      try:\n","        unnested_baseline_metrics[key] = unnested_baseline_metrics[key].transpose(2, 0,1,3)\n","        # print(unnested_baseline_metrics[key])\n","        # print(\"first methdo successful\")\n","      except:\n","        # print(\"FIRST METHOD FAILED\")\n","        # print(\"\\n\"*10)\n","        # print(unnested_baseline_metrics.shape)\n","        # unnested_baseline_metrics[key] = np.expand_dims(unnested_baseline_metrics[key], -1).transpose(1,3,0,2)\n","        unnested_baseline_metrics[key] = unnested_baseline_metrics[key].transpose(2,0,1)\n","      #   print(unnested_baseline_metrics[key])\n","      # print(unnested_metrics[key])\n","    else:\n","      if len(unnested_baseline_metrics[key].shape) == 3:\n","        unnested_baseline_metrics[key] = np.expand_dims(unnested_baseline_metrics[key].transpose(1,0,2), axis=1)\n","      elif len(unnested_baseline_metrics[key].shape) == 4:\n","        unnested_baseline_metrics[key] = np.expand_dims(unnested_baseline_metrics[key].transpose(1, 0,2,3), axis=1)\n","\n","    # elif len(unnested_metrics[key].shape) == 2:\n","      # unnested_metrics[key] = unnested_metrics[key].transpose(1,0)\n","    #TODO prob est and variance are still not really updated....\n","      \n","    # print(key)\n","    # print(\"has experimetnal shape\")\n","    # print(unnested_metrics[key].shape)\n","    # print(\"has baseline shape\")\n","    # print(unnested_baseline_metrics[key].shape)\n","\n","\n","  if 'variance' in extra_metrics_list:\n","    print(f\"prob est shape {unnested_metrics['prob_est'].shape}\")\n","    print(f\"ind shape {eval_data_dicts[0]}\")\n","    print(f\"baseline prob est shape {unnested_baseline_metrics['prob_est'].shape}\")\n","    \n","    \n","    unnested_metrics['variance'] = np.sum(np.var(unnested_metrics['prob_est'], axis=0), axis=1)\n","    unnested_baseline_metrics['variance'] = np.sum(np.var(unnested_baseline_metrics['prob_est'], axis=2), axis=3)\n","\n","\n","\n","    # unnested_metrics['variance'] = np.sum(np.var(np.multiply(unnested_metrics['prob_est'].transpose(0,2,1), eval_data_dicts[0]['indices']).transpose(0,2,1), axis=0), axis=1)\n","    # unnested_baseline_metrics['variance'] = np.sum(np.var(np.multiply(unnested_baseline_metrics['prob_est'].transpose(0,1, 2,4,3), eval_data_dicts[0]['indices']).transpose(0,1, 2, 4, 3), axis=2), axis=3)\n","\n","\n","  experimental_results = np.array(experimental_results, ndmin=3)\n","  baseline_results = np.array(baseline_results, ndmin=4)\n","  # print(experimental_results.shape)\n","  # print(baseline_results.shape)\n","  experimental_results = experimental_results.transpose(1,0,2)\n","  baseline_results = baseline_results.transpose(1,2,0,3)\n","  # print(experimental_results)\n","  # print(baseline_results)\n","  # baseline_results = [b.tolist() for b in baseline_results]\n","\n","\n","  para = f'epss_{eps}_{[f\"{x:.2f}\" for x in privacy_budget]}'\n","  time = datetime.datetime.now().strftime(\"%m_%d_%H-%M\")\n","  baseline_readable = snake_to_readable(baseline_mechanism)\n","  legend = generate_legend_baseline(baseline_readable)\n","  legend = finalize_legend(legend)\n","\n","\n","  # print(total_alphabet_size)\n","  \n","  RELAXATIONS = [\"Oracle\", \"Split Domain\"]\n","  plot_dir = get_absolute_path(f\"plots_V{VERSION_NUM.split('.')[0]}/{dist}/s{score_mode}g{growth_mode}/{ldp_mechanism}_rmode_{relaxation_mode}_smode_{score_mode}_gmode_{growth_mode}_t_{tiers}_{para}\".replace(\" \", \"_\"))\n","  if not os.path.exists(plot_dir):\n","    os.makedirs(plot_dir)\n","  filename = os.path.join(plot_dir, f\"{dist}_{ldp_mechanism}_esub{epoch_subdivisions}_rel_{relaxation_mode}_sc_{score_mode}_gr_{growth_mode}_t_{tiers}_{para}\") #'data_' + dist + '_'+ relaxation_mode + topk + para + time\n","  kstr = f\"Tiers={tiers}, split={split_percentiles}, k={total_alphabet_size}, g={growth_mode}, s={score_mode}, e_subd={epoch_subdivisions}, rep={num_repititions}\"\n","  boxplot_title_string = f\"{ldp_mechanism} {dist}, {RELAXATIONS[relaxation_mode]} HR, \\u03B5=({eps}, {[f'{x*100:.0f}%' for x in privacy_budget]}), {kstr}\"\n","\n","  # print(results)\n","  legend_labels = [(\"Iter. \" if (ldp_mechanism_helper(ldp_mechanism) >2 or ldp_mechanism_helper(ldp_mechanism) == -1) else \"\")  + snake_to_readable(ldp_mechanism) + \" (Our Method)\"] + [legend[i] for i in BASELINE_VERSION]\n","  if save_images:\n","    pickle.dump((experimental_results, baseline_results, unnested_metrics, unnested_baseline_metrics, arg_map, filename, kstr, boxplot_title_string, results, subset_partition_indices, legend_labels, interest_cols, legend, plot_dir ), open(f\"{filename}.pkl\", 'wb'))\n","  \n","  gen_aggregate_column_boxplot(experimental_results, baseline_results, results[col][0][\"indices\"], legend_labels, interest_cols, boxplot_title_string, filename=filename + \"agg_cols\" , save_images=save_images)  \n","\n","  for metric in ['weighted_rmse', 'variance', 'time', 'kl_divergence', 'chi_square_distance', 'inv_weighted_rmse']: # for 'partition_weighted_rmse', 'prob_est'\n","    try:\n","      gen_aggregate_column_boxplot(np.array([unnested_metrics[metric]]), unnested_baseline_metrics[metric], results[col][0][\"indices\"], [\"Iter. \" + ldp_mechanism + \" (Our Method)\"] + [legend[i] for i in BASELINE_VERSION], [metric], boxplot_title_string, filename=filename + f\"extra_metric_{metric}\", save_images=save_images)\n","    except Exception as e:\n","      plt.close()\n","      print(e)\n","      print(\"failed to print as a chart metric: \", metric)\n","      print(unnested_metrics[metric])\n","\n","  for metric in ['partition_weighted_rmse', 'partition_inv_weighted_rmse']:\n","    # try:\n","      # print(\"unnested metric:: :: : : : :\", unnested_baseline_metrics[metric])\n","    gen_partition_aggregate_column_boxplot(unnested_metrics[metric], unnested_baseline_metrics[metric], subset_partition_indices, [ldp_mechanism + \" experimental\"] + [legend[i] for i in BASELINE_VERSION], [metric], results[col][0][\"indices\"], boxplot_title_string, filename=filename + f\"extra_metric_{metric}\", save_images=save_images)\n","    # except:\n","    #   try:\n","    #     print(\"unnested metric:: :: : : : :\", unnested_baseline_metrics[metric][0])\n","    #     gen_partition_aggregate_column_boxplot(unnested_metrics[metric], unnested_baseline_metrics[metric][0], subset_partition_indices, [ldp_mechanism + \" experimental\"] + [legend[i] for i in BASELINE_VERSION], [metric], results[col][0][\"indices\"], boxplot_title_string, filename=filename + f\"extra_metric_{metric}\", save_images=save_images)\n","    #   except:\n","    #     # try:\n","    #     print(\"unnested metric:: :: : : : :\", [np.array(e[0]) for e in unnested_baseline_metrics[metric]])\n","\n","    # gen_partition_aggregate_column_boxplot(unnested_metrics[metric],  [np.array(e[0]) for e in unnested_baseline_metrics[metric]], subset_partition_indices, [ldp_mechanism + \" experimental\"] + [legend[i] for i in BASELINE_VERSION], [metric], results[col][0][\"indices\"], boxplot_title_string, filename=filename + f\"extra_metric_{metric}\", save_images=save_images)\n","        # except:\n","        #   gen_partition_aggregate_column_boxplot(unnested_metrics[metric], [e[0][0] for e in unnested_baseline_metrics[metric]], subset_partition_indices, [ldp_mechanism + \" experimental\"] + [legend[i] for i in BASELINE_VERSION], [metric], results[col][0][\"indices\"], boxplot_title_string, filename=filename + f\"extra_metric_{metric}\", save_images=save_images)\n","\n","  for metric in ['prob_est']:\n","    # gen_reg_plot(\n","    pass\n","\n","    #use seaborn (regplot (ax=ax setting, do we have to turn data into df?) https://seaborn.pydata.org/generated/seaborn.regplot.html\n","  # interest_cols.extend([f\"baseline_{x}\" for x in interest_cols])\n","\n","  return results, ground_truth_hist, string_col_codes_and_labels\n","\n","# function_kwargs = {'k': 1000, 'score_mode' : 0, 'eps': 2, 'tiers': 2, 'split_percentiles': [.7], 'epochs': 10, 'encode_acc': 0, 'encode_mode': 0, 'point_growth_rate': 1, 'relaxation_mode': 1, 'basegroup_size': 0.01, 'eps_relaxation': 1, 'epoch_subdivisions': None, 'init': 1, 'step_sz': 50000}\n","# evaluate_dataset_parallel(github_sickness_data, [github_sickness_data_cols], **function_kwargs)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Cancer data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(covid_symptom_data, cancer_data, github_sickness_data) = pickle.load(open(get_absolute_path('datasets.pkl'), 'rb'))\n","\n","COVID_SYMPTOM_COLS, scores = zip(*find_related_cols(covid_symptom_data.columns.tolist(), GENERIC_SYMPTOM_KEYWORDS))\n","COVID_SYMPTOM_COLS = list(COVID_SYMPTOM_COLS)\n","# COVID_SYMPTOM_COLS.remove(\"search_trends_diarrhea\")\n","\n","for c in COVID_SYMPTOM_COLS:\n","  print(c)\n","\n","github_sickness_data_cols, scores = zip(*find_related_cols(github_sickness_data.columns.tolist(), GENERIC_SYMPTOM_KEYWORDS))\n","github_sickness_data_cols = list(github_sickness_data_cols)\n","print(github_sickness_data_cols)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NUM_GENERATED_DIST_SAMPLES = 10000\n","NUM_UNIQUE_ELES = 10\n","number_of_columns = 1\n","unique_elements = np.arange(NUM_UNIQUE_ELES)\n","prob = generate_Zipf_distribution(NUM_UNIQUE_ELES, 1.1)#TODO this should be greater than 1\n","#TODO you should also check what the actual power law constant for this distirbution is...\n","\n","assert len(unique_elements) == len(prob), f\"length of {len(unique_elements)} should be equal to the length of the probability dist {len(prob)}\"\n","\n","\n","power_law_dict = {col: np.random.choice(unique_elements, NUM_GENERATED_DIST_SAMPLES, p=prob) for col in range(number_of_columns)}\n","\n","power_law_10 = pd.DataFrame(power_law_dict)\n","power_law_10_cols = list(range(number_of_columns))\n","\n","# power_law_data.describe()\n","NUM_UNIQUE_ELES = 100\n","number_of_columns = 1\n","unique_elements = np.arange(NUM_UNIQUE_ELES)\n","prob = generate_Zipf_distribution(NUM_UNIQUE_ELES, 1.0)\n","\n","assert len(unique_elements) == len(prob), f\"length of {len(unique_elements)} should be equal to the length of the probability dist {len(prob)}\"\n","\n","\n","power_law_dict = {col: np.random.choice(unique_elements, NUM_GENERATED_DIST_SAMPLES, p=prob) for col in range(number_of_columns)}\n","\n","power_law_100 = pd.DataFrame(power_law_dict)\n","power_law_100_cols = list(range(number_of_columns))\n","\n","# power_law_data.describe()\n","\n","NUM_UNIQUE_ELES = 10\n","unique_elements = np.arange(NUM_UNIQUE_ELES)\n","\n","\n","dir_alpha = .5\n","zipf_s = 2.3\n","\n","prob1 = generate_uniform_distribution(NUM_UNIQUE_ELES)\n","prob2 = generate_two_steps_distribution(NUM_UNIQUE_ELES)\n","prob3 = generate_Zipf_distribution(NUM_UNIQUE_ELES,zipf_s)\n","prob4 = generate_Dirichlet_distribution(NUM_UNIQUE_ELES, dir_alpha)\n","# prob5 = generate_geometric_distribution(k,0.8)\n","# element_map = {'normal': list(range(k)), 'topk': []}\n","\n","prob_list = {\n","    'Zipf' : prob3,\n","    'Dirchlet' : prob4,\n","    'Two_steps' : prob2,\n","    'Uniform' : prob1,\n","    # 'Geometric' : prob5, \n","    }\n","\n","\n","\n","\n","prob_dict_list = [{col:np.random.choice(unique_elements, NUM_GENERATED_DIST_SAMPLES,p=p_dist) for col in range(number_of_columns)} for p_dist in prob_list.values()]\n","distribution_dfs = [pd.DataFrame(d) for d in prob_dict_list]\n","distribution_cols_list = [[[list(range(number_of_columns))]] for _ in prob_list.values()]\n","\n","# NUM_UNIQUE_ELES = 100\n","# unique_elements = np.arange(NUM_UNIQUE_ELES)\n","\n","\n","# prob1 = generate_uniform_distribution(NUM_UNIQUE_ELES)\n","# prob2 = generate_two_steps_distribution(NUM_UNIQUE_ELES)\n","# prob3 = generate_Zipf_distribution(NUM_UNIQUE_ELES,zipf_s)\n","# prob4 = generate_Dirichlet_distribution(NUM_UNIQUE_ELES,dir_alpha)\n","# # prob5 = generate_geometric_distribution(k,0.8)\n","# # element_map = {'normal': list(range(k)), 'topk': []}\n","\n","# prob_list = {\n","#     'Uniform' : prob1,\n","#     'Two_steps' : prob2,\n","#     'Zipf' : prob3,\n","#     'Dirchlet' : prob4,\n","#     # 'Geometric' : prob5, \n","#     }\n","# prob_dict_list = [{col:np.random.choice(unique_elements, NUM_GENERATED_DIST_SAMPLES,p=p_dist) for col in range(number_of_columns)} for p_dist in prob_list.values()]\n","# distribution_dfs.extend([pd.DataFrame(d) for d in prob_dict_list])\n","# distribution_cols_list.extend([[[list(range(number_of_columns))]] for _ in prob_list.values()])\n","from IDLDP.pg1 import LDP_MECHANISMS\n","LDP_MECHANISMS\n","# prob_dict_list\n",""]},{"cell_type":"markdown","metadata":{},"source":[" #Real Data main"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","from IDLDP.pg1 import LDP_MECHANISMS\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# idldp = IDLDP()\n","calibrate_plt(plt)\n","\n","# %matplotlib inlineo\n","\n","\n","#TODO plot the distributions and save in a version directory\n","\n","\n","\n","VERSION_NUM=\"test\"\n"," #TODO FIX THIS\n","dataset_dfs = [\n","  # cancer_data,\n","  # github_sickness_data,\n","  power_law_10,\n","  # power_law_100,\n","  *distribution_dfs,\n","  covid_symptom_data,\n","]\n","\n","DATASET_READABLE_NAMES = [\n","# \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","\"d=10 Zipf 1.1\",\n","# \"d=100 Zipf 1.1\",\n","f\"d=10 Zipf {zipf_s}\",\n","f'd=10 Dirichlet {dir_alpha}',\n","'d=10 Two Step',\n","'d=10 Uniform',\n","# 'd=100 Uniform',\n","# 'd=100 Two Step',\n","# f\"d=100 Zipf {zipf_s}\",\n","# f'd=100 Dirichlet {dir_alpha}',\n","\"Synthetic COVID Symptom config.\", \n","]\n","\n","\n","DATASET_NAMES = [\n","# \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","\"10 ele zipf 1.1\",\n","# \"100 ele zipf 1.1\",\n","f\"10 ele zipf {zipf_s}\",\n","f'10 ele dirchlet {dir_alpha}',\n","'10 ele two step',\n","'10 ele uniform',\n","# '100 ele uniform',\n","# '100 ele two step',\n","# f\"100 ele zipf {zipf_s}\",\n","# f'100 ele dirchlet {dir_alpha}',\n","\"Synthetic COVID Symptoms\", \n","]\n","\n","n = 2 #must be graeteer than or equal to 2\n","gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n","dataset_col_group_settings = [\n","  # [[[\"stage\", \"grid_index\"]]],\n","  # [[github_sickness_data_cols]],\n","  [[power_law_10_cols]],\n","  # [[power_law_100_cols]],\n","*distribution_cols_list,\n","  [[COVID_SYMPTOM_COLS]],\n","]\n","relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n","\n","failed_runs = []\n","for relaxation_mode in [1]:\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  for i, dataset_df in enumerate(dataset_dfs):\n","    for j, col_group in enumerate(dataset_col_group_settings[i]):\n","      temp = dataset_df[col_group[0]].astype(str).apply(''.join, axis=1)\n","      cat = pd.Categorical(temp)\n","      # codes = cat.codes\n","      alphabet_size = len(temp.unique())\n","      col_histogram = temp.value_counts(sort=True)\n","      # print(alphabet_size)\n","      # print(col_histogram)\n","\n","\n","      # summed_data = col_histogram.sum(axis=0, skipna=True)\n","      dist = probability_normalize(col_histogram.values)\n","      plot_dir = get_absolute_path(f\"plots_V{VERSION_NUM.split('.')[0]}/distributions/\".replace(\" \", \"_\"))\n","      if not os.path.exists(plot_dir):\n","        os.makedirs(plot_dir)\n","      \n","      sns_plot = sns.lineplot(range(alphabet_size), dist)        .set_title(f'{DATASET_READABLE_NAMES[i]} (sorted)')\n","        # .set_(f'{DATASET_NAMES[i].title()} Distribution (sorted)')\\\n","        # .set_title(f'{DATASET_NAMES[i].title()} Distribution (sorted)')\\\n","      plt.xlabel(\"Element #\")\n","      plt.ylabel(\"% Composition\")\n","      plt.ylim((0,.35))\n","      plt.subplots_adjust(left=.14, bottom=.16)\n","\n","      plt.pause(0.1)\n","      fig = sns_plot.get_figure()\n","      fig.savefig(os.path.join(plot_dir, DATASET_NAMES[i] + \".png\"))\n","      fig.show()\n","      plt.show()\n","\n","\n","\n","      for ldp_ind, ldp_mechanism in enumerate(LDP_MECHANISMS):\n","        if ldp_ind >0:\n","          continue #combined comp plot\n","\n","        # for baseline_version in range(3):\n","        for tier_ind, tiers in enumerate([2,5]):\n","          if 'COVID' in DATASET_NAMES[i] and tier_ind == 0:\n","            continue\n","\n","          for split_percentiles in [[.01*i]*(tiers-1) for i in [5, 10, 20]] if '10 ' not in DATASET_NAMES[i] else [[.1]*(tiers-1)]:\n","            for eps_ind, eps in enumerate([.1, 1]):\n","              # if ldp_ind in [5]:\n","              #   if growth_ind in [1]:\n","              #     continue\n","              for growth_ind, growth_mode in enumerate([0,1,2]):\n","                for score_ind, score_mode in enumerate([1,2]):\n","                  # relaxation_mode = 1\n","                  epochs = 5\n","                  # growth_mode = 0 #growth mode 1 is broken?\n","                  # score_mode = 2\n","                  privacy_budget = init_privacy_budget(tiers, obfuscate_heavy_hitters=False, max_relaxation=.2)\n","                  baseline_version = [0,2,5,8,9,10,11,12] # 4 for zero shot hadamard (2nd one)\n","                  # print(privacy_budget)\n","                  # epoch_subdivisions = 5\n","                  num_repititions = 3\n","                  # for score_mode in range(1,3):\n","                  # for growth_mode in [1]:\n","                  for epoch_subdivisions in range(4,10, 3):\n","                    # if ldp_ind in [0,1,2,3,4]:\n","                    #   if growth_ind in [0,1]:\n","                    #     continue\n","                    # if ldp_ind in [5]:\n","                    #   if growth_ind in [1]:\n","                    #     continue\n","                    \n","                    # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n","                        print(datetime.datetime.now())\n","                        print(DATASET_NAMES[i])\n","                        print(col_group)\n","                        print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                        function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n","                        'growth_mode': growth_mode,\n","                        'k': 1000,\n","                        'score_mode' : score_mode,\n","                        'eps': eps,\n","                        'tiers': tiers,\n","                        'split_percentiles': split_percentiles,\n","                        'epochs': epochs,\n","                        'encode_acc': 0,\n","                        'encode_mode': 0,\n","                        'relaxation_mode': relaxation_mode,\n","                        'privacy_budget': privacy_budget,\n","                        'epoch_subdivisions': epoch_subdivisions,\n","                        'init': 1,\n","                        'step_sz': 50000,\n","                        'num_repititions': num_repititions,\n","                        'output_counts': True,\n","                        'BASELINE_VERSION': baseline_version,\n","                        'ldp_mechanism': ldp_mechanism,\n","                        'baseline_mechanism': \"Hadamard\",\n","                        'save_images': True\n","    #                       'idldp': idldp,\n","                        }\n","                        # try:\n","                          # print(\"passing kwargs from main looop\")\n","                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                          # raise Exception(\"over\")\n","                        # except Exception as e:\n","                          # print(e)\n","                          # plt.close()\n","                          # failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs, e))\n","                          # break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# VERSION_NUM=\"12ObfuscateHighScore2.0-hadamard\"\n","import pprint\n","print(len(failed_runs))\n","pprint.pprint(failed_runs)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# VERSION_NUM=\"12ObfuscateHighScore2.0-hadamard\"\n","# import pprint\n","# print(len(failed_runs))\n","# pprint.pprint(failed_runs)\n","# score v2 failed runs below\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Main 2 (all zero shot baselines)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","from IDLDP.pg1 import LDP_MECHANISMS\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# idldp = IDLDP()\n","calibrate_plt(plt)\n","\n","# %matplotlib inlineo\n","\n","\n","#TODO plot the distributions and save in a version directory\n","\n","\n","\n","VERSION_NUM=\"31ObfuscateHigh\"\n","\n","dataset_dfs = [\n","  # covid_symptom_data,\n","  # cancer_data,\n","  # github_sickness_data,\n","  # power_law_10,\n","  # power_law_100,\n","  *distribution_dfs\n","]\n","\n","# DATASET_NAMES = [\n","# \"Synthetic COVID Symptom config.\", \n","# # \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"d=10 Zipf 1.1\",\n","# \"d=100 Zipf 1.1\",\n","# 'd=10 Uniform',\n","# 'd=10 Two Step',\n","# 'd=10 Dirichlet',\n","# 'd=100 Uniform',\n","# 'd=100 Two Step',\n","# 'd=100 Dirichlet',\n","# ]\n","\n","\n","DATASET_NAMES = [\n","# \"Synthetic COVID Symptoms\", \n","# \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"10 ele zipf 1.1\",\n","# \"100 ele zipf 1.1\",\n","# '10 ele uniform',\n","'10 ele two step',\n","'10 ele dirchlet',\n","# '100 ele uniform',\n","'100 ele two step',\n","'100 ele dirchlet',\n","]\n","\n","n = 2 #must be graeteer than or equal to 2\n","gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n","dataset_col_group_settings = [\n","  # [[COVID_SYMPTOM_COLS]],\n","  # [[[\"stage\", \"grid_index\"]]],\n","  # [[github_sickness_data_cols]],\n","  # [[power_law_10_cols]],\n","  # [[power_law_100_cols]],\n","*distribution_cols_list\n","]\n","relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n","\n","failed_runs = []\n","for relaxation_mode in [1]:\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  for i, dataset_df in enumerate(dataset_dfs):\n","    for j, col_group in enumerate(dataset_col_group_settings[i]):\n","\n","      for ldp_ind, ldp_mechanism in enumerate(LDP_MECHANISMS):\n","        # if ldp_ind <1:\n","        #   continue #skip hadamard\n","\n","        # for baseline_version in range(3):\n","        for tiers in [2,5]:\n","          for split_percentiles in [[.01*i]*(tiers-1) for i in [5, 10, 20]] if '10 ' not in DATASET_NAMES[i] else [[.1]*(tiers-1)]:\n","            for eps_ind, eps in enumerate([0.01, .1, 1, 10]):\n","              if ldp_ind in [0]:\n","                if eps_ind in [0,1]:\n","                  continue\n","              if ldp_ind in [1,3,4]:\n","                if eps_ind in [0]:\n","                  continue\n","              # if ldp_ind in [5]:\n","              #   if growth_ind in [1]:\n","              #     continue\n","              for growth_ind, growth_mode in enumerate([0,1,2]):\n","                if ldp_ind in [1,2,3,4]:\n","                  if growth_ind in [0,1]:\n","                    continue\n","                if ldp_ind in [0,5]:\n","                  if growth_ind in [0,2]:\n","                    continue\n","                for score_ind, score_mode in enumerate([1,2]):\n","                  if ldp_ind in [0,1,2,3,4,5]:\n","                    if score_ind in [1]:\n","                      continue\n","                  # relaxation_mode = 1\n","                  epochs = 5\n","                  # growth_mode = 0 #growth mode 1 is broken?\n","                  # score_mode = 2\n","                  privacy_budget = init_privacy_budget(tiers, obfuscate_heavy_hitters=True, max_relaxation=.2)\n","                  baseline_version = [0,1,2,5] # 4 for zero shot hadamard (2nd one)\n","                  # print(privacy_budget)\n","                  # epoch_subdivisions = 5\n","                  num_repititions = 12\n","                  # for score_mode in range(1,3):\n","                  # for growth_mode in [1]:\n","                  for epoch_subdivisions in range(4,10, 3):\n","                    # if ldp_ind in [0,1,2,3,4]:\n","                    #   if growth_ind in [0,1]:\n","                    #     continue\n","                    # if ldp_ind in [5]:\n","                    #   if growth_ind in [1]:\n","                    #     continue\n","                    \n","                    # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n","                        print(datetime.datetime.now())\n","                        print(DATASET_NAMES[i])\n","                        print(col_group)\n","                        print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                        function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n","                        'growth_mode': growth_mode,\n","                        'k': 1000,\n","                        'score_mode' : score_mode,\n","                        'eps': eps,\n","                        'tiers': tiers,\n","                        'split_percentiles': split_percentiles,\n","                        'epochs': epochs,\n","                        'encode_acc': 0,\n","                        'encode_mode': 0,\n","                        'relaxation_mode': relaxation_mode,\n","                        'privacy_budget': privacy_budget,\n","                        'epoch_subdivisions': epoch_subdivisions,\n","                        'init': 1,\n","                        'step_sz': 50000,\n","                        'num_repititions': num_repititions,\n","                        'output_counts': True,\n","                        'BASELINE_VERSION': baseline_version,\n","                        'ldp_mechanism': ldp_mechanism,\n","                        'baseline_mechanism': \"Hadamard\",\n","                        'save_images': True\n","    #                       'idldp': idldp,\n","                        }\n","                        try:\n","                          # print(\"passing kwargs from main looop\")\n","                          evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                          # raise Exception(\"over\")\n","                        except Exception as e:\n","                          print(e)\n","                          plt.close()\n","                          failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs, e))\n","                          # break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","pprint.pprint(failed_runs)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# VERSION_NUM=\"11ObfuscateLowRappor.0-rappor\"\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","from IDLDP.pg1 import LDP_MECHANISMS\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# idldp = IDLDP()\n","calibrate_plt(plt)\n","\n","# %matplotlib inlineo\n","\n","\n","#TODO plot the distributions and save in a version directory\n","\n","\n","\n","VERSION_NUM=\"31ObfuscateLow\"\n","\n","dataset_dfs = [\n","  covid_symptom_data,\n","  # cancer_data,\n","  # github_sickness_data,\n","  # power_law_10,\n","  # power_law_100,\n","  # *distribution_dfs\n","]\n","\n","# DATASET_NAMES = [\n","# \"Synthetic COVID Symptom config.\", \n","# # \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"d=10 Zipf 1.1\",\n","# \"d=100 Zipf 1.1\",\n","# 'd=10 Uniform',\n","# 'd=10 Two Step',\n","# 'd=10 Dirichlet',\n","# 'd=100 Uniform',\n","# 'd=100 Two Step',\n","# 'd=100 Dirichlet',\n","# ]\n","\n","\n","DATASET_NAMES = [\n","\"Synthetic COVID Symptoms\", \n","# \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"10 ele zipf 1.1\",\n","# \"100 ele zipf 1.1\",\n","# '10 ele uniform',\n","# '10 ele two step',\n","# '10 ele dirchlet',\n","# '100 ele uniform',\n","# '100 ele two step',\n","# '100 ele dirchlet',\n","]\n","\n","n = 2 #must be graeteer than or equal to 2\n","gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n","dataset_col_group_settings = [\n","  [[COVID_SYMPTOM_COLS]],\n","  # [[[\"stage\", \"grid_index\"]]],\n","  # [[github_sickness_data_cols]],\n","  # [[power_law_10_cols]],\n","  # [[power_law_100_cols]],\n","# *distribution_cols_list\n","]\n","relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n","\n","failed_runs = []\n","for relaxation_mode in [1]:\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  for i, dataset_df in enumerate(dataset_dfs):\n","    for j, col_group in enumerate(dataset_col_group_settings[i]):\n","\n","      for ldp_ind, ldp_mechanism in enumerate(LDP_MECHANISMS):\n","        # if ldp_ind <1:\n","        #   continue #skip hadamard\n","\n","        # for baseline_version in range(3):\n","        for tiers in [2,5]:\n","          for split_percentiles in [[.01*i]*(tiers-1) for i in [5, 10, 20]] if '10 ' not in DATASET_NAMES[i] else [[.1]*(tiers-1)]:\n","            for eps_ind, eps in enumerate([0.01, .1, 1, 10]):\n","              if ldp_ind in [0]:\n","                if eps_ind in [0]:\n","                  continue\n","              if ldp_ind in [1,3,4]:\n","                if eps_ind in [0]:\n","                  continue\n","              # if ldp_ind in [5]:\n","              #   if growth_ind in [1]:\n","              #     continue\n","              for growth_ind, growth_mode in enumerate([0,1,2]):\n","                if ldp_ind in [1,2,5,4]:\n","                  if growth_ind in [0,2]:\n","                    continue\n","                if ldp_ind in [3]:\n","                  if growth_ind in [0]:\n","                    continue\n","                for score_ind, score_mode in enumerate([1,2]):\n","                  if ldp_ind in [0,1,2,3,4,5]:\n","                    if score_ind in [1]:\n","                      continue\n","                  # relaxation_mode = 1\n","                  epochs = 5\n","                  # growth_mode = 0 #growth mode 1 is broken?\n","                  # score_mode = 2\n","                  privacy_budget = init_privacy_budget(tiers, obfuscate_heavy_hitters=False, max_relaxation=.2)\n","                  baseline_version = [0,1,2,5] # 4 for zero shot hadamard (2nd one)\n","                  # print(privacy_budget)\n","                  # epoch_subdivisions = 5\n","                  num_repititions = 12\n","                  # for score_mode in range(1,3):\n","                  # for growth_mode in [1]:\n","                  for epoch_subdivisions in range(4,10, 3):\n","                    # if ldp_ind in [0,1,2,3,4]:\n","                    #   if growth_ind in [0,1]:\n","                    #     continue\n","                    # if ldp_ind in [5]:\n","                    #   if growth_ind in [1]:\n","                    #     continue\n","                    \n","                    # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n","                        print(datetime.datetime.now())\n","                        print(DATASET_NAMES[i])\n","                        print(col_group)\n","                        print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                        function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n","                        'growth_mode': growth_mode,\n","                        'k': 1000,\n","                        'score_mode' : score_mode,\n","                        'eps': eps,\n","                        'tiers': tiers,\n","                        'split_percentiles': split_percentiles,\n","                        'epochs': epochs,\n","                        'encode_acc': 0,\n","                        'encode_mode': 0,\n","                        'relaxation_mode': relaxation_mode,\n","                        'privacy_budget': privacy_budget,\n","                        'epoch_subdivisions': epoch_subdivisions,\n","                        'init': 1,\n","                        'step_sz': 50000,\n","                        'num_repititions': num_repititions,\n","                        'output_counts': True,\n","                        'BASELINE_VERSION': baseline_version,\n","                        'ldp_mechanism': ldp_mechanism,\n","                        'baseline_mechanism': \"Hadamard\",\n","                        'save_images': True\n","    #                       'idldp': idldp,\n","                        }\n","                        try:\n","                          # print(\"passing kwargs from main looop\")\n","                          evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                          # raise Exception(\"over\")\n","                        except Exception as e:\n","                          print(e)\n","                          plt.close()\n","                          failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs, e))\n","                          # break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","pprint.pprint(failed_runs)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","from IDLDP.pg1 import LDP_MECHANISMS\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# idldp = IDLDP()\n","calibrate_plt(plt)\n","\n","# %matplotlib inlineo\n","\n","\n","#TODO plot the distributions and save in a version directory\n","\n","\n","\n","VERSION_NUM=\"31ObfuscateLow\"\n","\n","dataset_dfs = [\n","  # covid_symptom_data,\n","  # cancer_data,\n","  # github_sickness_data,\n","  # power_law_10,\n","  # power_law_100,\n","  *distribution_dfs\n","]\n","\n","# DATASET_NAMES = [\n","# \"Synthetic COVID Symptom config.\", \n","# # \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"d=10 Zipf 1.1\",\n","# \"d=100 Zipf 1.1\",\n","# 'd=10 Uniform',\n","# 'd=10 Two Step',\n","# 'd=10 Dirichlet',\n","# 'd=100 Uniform',\n","# 'd=100 Two Step',\n","# 'd=100 Dirichlet',\n","# ]\n","\n","\n","DATASET_NAMES = [\n","# \"Synthetic COVID Symptoms\", \n","# \"Cancer Data\", \n","# \"Github Disease Symptoms\",\n","# \"10 ele zipf 1.1\",\n","# \"100 ele zipf 1.1\",\n","# '10 ele uniform',\n","'10 ele two step',\n","'10 ele dirchlet',\n","# '100 ele uniform',\n","'100 ele two step',\n","'100 ele dirchlet',\n","]\n","\n","n = 2 #must be graeteer than or equal to 2\n","gen_col_group_split = lambda n, symtom_cols : [np.split(symtom_cols, [len(symtom_cols)//n*i for i in range(1,n)])]\n","dataset_col_group_settings = [\n","  # [[COVID_SYMPTOM_COLS]],\n","  # [[[\"stage\", \"grid_index\"]]],\n","  # [[github_sickness_data_cols]],\n","  # [[power_law_10_cols]],\n","  # [[power_law_100_cols]],\n","*distribution_cols_list\n","]\n","relaxation_modes = [\"oracle mode\", \"split relaxation\"]\n","\n","failed_runs = []\n","for relaxation_mode in [1]:\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  print(\"=\"*100 + \"\\n\\n\")\n","  for i, dataset_df in enumerate(dataset_dfs):\n","    for j, col_group in enumerate(dataset_col_group_settings[i]):\n","\n","      for ldp_ind, ldp_mechanism in enumerate(LDP_MECHANISMS):\n","        # if ldp_ind <1:\n","        #   continue #skip hadamard\n","\n","        # for baseline_version in range(3):\n","        for tiers in [2,5]:\n","          for split_percentiles in [[.01*i]*(tiers-1) for i in [5, 10, 20]] if '10 ' not in DATASET_NAMES[i] else [[.1]*(tiers-1)]:\n","            for eps_ind, eps in enumerate([0.01, .1, 1, 10]):\n","              if ldp_ind in [0]:\n","                if eps_ind in [0,1]:\n","                  continue\n","              if ldp_ind in [1,3,4]:\n","                if eps_ind in [0]:\n","                  continue\n","              # if ldp_ind in [5]:\n","              #   if growth_ind in [1]:\n","              #     continue\n","              for growth_ind, growth_mode in enumerate([0,1,2]):\n","                if ldp_ind in [1,2,3,4]:\n","                  if growth_ind in [0,1]:\n","                    continue\n","                if ldp_ind in [0,5]:\n","                  if growth_ind in [0,2]:\n","                    continue\n","                for score_ind, score_mode in enumerate([1,2]):\n","                  if ldp_ind in [0,1,2,3,4,5]:\n","                    if score_ind in [1]:\n","                      continue\n","                  # relaxation_mode = 1\n","                  epochs = 5\n","                  # growth_mode = 0 #growth mode 1 is broken?\n","                  # score_mode = 2\n","                  privacy_budget = init_privacy_budget(tiers, obfuscate_heavy_hitters=False, max_relaxation=.2)\n","                  baseline_version = [0,1,2,5] # 4 for zero shot hadamard (2nd one)\n","                  # print(privacy_budget)\n","                  # epoch_subdivisions = 5\n","                  num_repititions = 12\n","                  # for score_mode in range(1,3):\n","                  # for growth_mode in [1]:\n","                  for epoch_subdivisions in range(4,10, 3):\n","                    # if ldp_ind in [0,1,2,3,4]:\n","                    #   if growth_ind in [0,1]:\n","                    #     continue\n","                    # if ldp_ind in [5]:\n","                    #   if growth_ind in [1]:\n","                    #     continue\n","                    \n","                    # for basegroup_size in list(np.linspace(.01, .25, num=3, endpoint=True)):\n","                        print(datetime.datetime.now())\n","                        print(DATASET_NAMES[i])\n","                        print(col_group)\n","                        print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                        function_kwargs = {'dist': DATASET_NAMES[i] + f\"_{j}\",\n","                        'growth_mode': growth_mode,\n","                        'k': 1000,\n","                        'score_mode' : score_mode,\n","                        'eps': eps,\n","                        'tiers': tiers,\n","                        'split_percentiles': split_percentiles,\n","                        'epochs': epochs,\n","                        'encode_acc': 0,\n","                        'encode_mode': 0,\n","                        'relaxation_mode': relaxation_mode,\n","                        'privacy_budget': privacy_budget,\n","                        'epoch_subdivisions': epoch_subdivisions,\n","                        'init': 1,\n","                        'step_sz': 50000,\n","                        'num_repititions': num_repititions,\n","                        'output_counts': True,\n","                        'BASELINE_VERSION': baseline_version,\n","                        'ldp_mechanism': ldp_mechanism,\n","                        'baseline_mechanism': \"Hadamard\",\n","                        'save_images': True\n","    #                       'idldp': idldp,\n","                        }\n","                        try:\n","                          # print(\"passing kwargs from main looop\")\n","                          evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                          # raise Exception(\"over\")\n","                        except Exception as e:\n","                          print(e)\n","                          plt.close()\n","                          failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs, e))\n","                          # break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","pprint.pprint(failed_runs)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Main 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, dataset_df in enumerate(dataset_dfs):\n","  for j, col_group in enumerate(dataset_col_group_settings[i]):\n","    for baseline_version in range(3):\n","      for tiers in [2]:\n","        for split_percentiles in [[i*.1] for i in range(1, 10)]:\n","          eps = 2\n","          relaxation_mode = 1\n","          epochs = 5\n","          growth_mode=1\n","          score_mode=0\n","          eps_relaxation= 1\n","          # for eps_relaxation in [1, 4]:\n","\n","          for eps_relaxation in [1, 4]:\n","                      print(datetime.datetime.now())\n","                      print(DATASET_NAMES[i])\n","                      print(col_group)\n","                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n","                      try:\n","                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                      except Exception as e:\n","                        print(e)\n","                        plt.close()\n","                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n","                        break\n","          for score_mode in range(0,3):\n","                      print(datetime.datetime.now())\n","                      print(DATASET_NAMES[i])\n","                      print(col_group)\n","                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n","                      try:\n","                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                      except Exception as e:\n","                        print(e)\n","                        plt.close()\n","                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n","                        break\n","          for growth_mode in range(3):\n","                      print(datetime.datetime.now())\n","                      print(DATASET_NAMES[i])\n","                      print(col_group)\n","                      print(f\"relaxation_mode: {relaxation_modes[relaxation_mode]}\")\n","                      function_kwargs = {'dist': DATASET_NAMES[i] + f\"-iso_{j}\", 'growth_mode': growth_mode, 'k': 1000, 'score_mode' : score_mode, 'eps': eps, 'tiers': tiers, 'split_percentiles': split_percentiles, 'epochs': epochs, 'encode_acc': 0, 'encode_mode': 0, 'relaxation_mode': relaxation_mode, 'eps_relaxation': eps_relaxation, 'epoch_subdivisions': 10, 'init': 1, 'step_sz': 50000, 'BASELINE_VERSION': baseline_version}\n","                      try:\n","                        evaluate_dataset_parallel(dataset_df, col_group, **function_kwargs)\n","                      except Exception as e:\n","                        print(e)\n","                        plt.close()\n","                        failed_runs.append((DATASET_NAMES[i], col_group, function_kwargs))\n","                        break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","pprint.pprint(failed_runs)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Loader and evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PRIVACY_ML_ROOT = \"/Users/yaatehr/Documents/course6/PrivacyML/plots_V32combplotsobfLow\"\n","\n","pickle_path_dict = {}\n","for (dirpath, dirnames, filenames) in os.walk(PRIVACY_ML_ROOT):\n","    for filename in filenames:\n","        if filename.endswith('.pkl'): \n","            pickle_path_dict[filename] = os.sep.join([dirpath, filename])\n","\n","# print(pickle_path_dict)\n","\n","pickle_data_dict = {}\n","for k,v in pickle_path_dict.items():\n","  pickle_data_dict[k] = pickle.load(open(v, 'rb'))\n","\n","\n","max_mean_value_dict = {}\n","min_mean_value_dict = {}\n","\n","\n","\n","def unpack_metric_list(metric_dict, axis, f, target_ind, baseline_mode=False,\n","  index_list = None,\n","  baseline_names=None,\n","):\n","  \"\"\"\n","  return a dictionary of type mechanism: RMSE\n","  this will be loaded into the top level mechanism: RMSE: args\n","  CONVENTIONAL USAGE, make the target_ind a sinlge metric at a time. call multiple times. build separate structure for each of the 4 metrics.\n","  \"\"\"\n","\n","\n","\n","\n","  #TODO it seems like the partition_weighted_rmse, variance, and weighted_rmse all have different shapes. have to treat each of them separately in the func below.\n","\n","  #Experimental shapes\n","  #variacne is (5) (good) we wantto target these for the baselines shapes....\n","  #weighted RMSE is (12,5) good\n","  #partition is: (7, 12, 5) -> 12,5\n","\n","\n","\n","\n","  #baseline shapes  \n","  #variance is #baselines x unpack list (5)\n","  #partition is looking likea list of the triplets (normal triple, may need some transpose)\n","  # seems like we need to unpack this for each one though.... (4 lists), alsmost the same treatmetns.... (1,12,5,7) seems like we may want to just cast these to ndarrays instead of dealing with the list bullshit.....\n","  #wegihed RMSE is an unpack then 12,5 i'd assume\n","  # \n","\n","\n","  mechanism_to_rmse_dict = {}\n","  assert len(target_ind) == 1\n","\n","  for k,v in metric_dict.items():\n","    if k not in target_ind:\n","      continue\n","\n","    if not baseline_mode:\n","      try:\n","        # print(\"EXPERIMENTALS\")\n","        # if 'partition_weighted_rmse' in target_ind:\n","        #   print(\"PARTITION\")\n","        # if 'weighted_rmse' in target_ind:\n","        #   print(\"WEIGHTEDRMSE\")\n","        # if 'variance' in target_ind:\n","        #   print(\"VARIACNAE\")\n","        # print(v.shape)\n","        mechanism_to_rmse_dict[baseline_names[0]] = f(v, axis=axis)\n","      except Exception as e:\n","        print(e)\n","        print(k)\n","        # print(v)\n","        raise e\n","    else:\n","      baseline_metric_list = v\n","      index_locs = [i for i, percentile_list in enumerate(index_list) if len(percentile_list) > 1]\n","      baseline_metric_list = np.array(baseline_metric_list)[[i - 1 for i in index_locs[1:]]].tolist()\n","      index_list = np.array(index_list, dtype=object)[index_locs].tolist()\n","      # print(index_list)\n","      print(baseline_names)\n","      baseline_names = np.array(baseline_names)[index_locs].tolist()\n","      index_labels = copy.copy(index_list)\n","      index_list = [inds / inds[-1] for inds in index_list]\n","\n","      if len(index_list) == 0 or 0 not in index_locs: #filter out list here (for all input lists, baseline names, metrics, etc)\n","        print(\"Skipping single partition chart\")\n","        return\n","      print(index_locs)\n","      for i, ind in enumerate(index_locs[1:]):\n","        try:\n","          print(\"BASELNIES\")\n","          if 'partition_weighted_rmse' in target_ind:\n","            print(\"PARTITION\")\n","          if 'weighted_rmse' in target_ind:\n","            print(\"WEIGHTEDRMSE\")\n","          if 'variance' in target_ind:\n","            print(\"VARIACNAE\")\n","          print(np.array(baseline_metric_list[i]).shape)\n","          mechanism_to_rmse_dict[baseline_names[i]] = f(baseline_metric_list[i])\n","          # else:\n","            # mechanism_to_rmse_dict[baseline_names[i]] = f(baseline_metric_list[i], axis=axis)\n","        except Exception as e:\n","          print(e)\n","          print(k)\n","          # print(v)\n","          raise e\n","        break\n","\n","\n","\n","  return mechanism_to_rmse_dict \n","\n","def get_average_vals(pickle, target_ind=None):\n","    (experimental_results, baseline_results, unnested_metrics, unnested_baseline_metrics, arg_map, filename, kstr, boxplot_title_string, results, subset_partition_indices, legend_labels, interest_cols, legend, plot_dir ) = pickle\n","\n","    # print(arg_map)\n","    # unnested metrics is KEY: 3(num reps) 5(number of epochs) 10(number of elements)\n","    # baselines is 1, # baselines eval for this metric, then the same\n","    #we wanttot avg or take the median across the 1st / 3rd axis\n","\n","    # if not target_ind:\n","      # target_ind = [\"partition_weighted_rmse\", \"weighted_rmse\", \"variance\"]\n","    # print()\n","\n","    # print(legend_labels)\n","    print()\n","    print(boxplot_title_string)\n","    print(\"generating unpacked for eexperimental vals\")\n","\n","    # print(unnested_metrics.keys())\n","  \n","\n","    unnested_avgs = unpack_metric_list(unnested_metrics, 0, np.average, target_ind, baseline_names = legend_labels)\n","    print(\"generating unpacked vals BASELINE\")\n","    # print(unnested_baseline_metrics)\n","    unnested_baseline_avgs = unpack_metric_list(unnested_baseline_metrics, 2, np.average, target_ind, baseline_mode=True, index_list=subset_partition_indices, baseline_names = legend_labels)\n","\n","\n","\n","\n","    print(target_ind)\n","    print(list(unnested_avgs.values())[0].shape)\n","    print(list(unnested_baseline_avgs.values())[0])\n","    return None, None\n","# Getting RMSE stats raw\n","\n","# baseline_metric_list: list of list metrics of same format as input metrics, one for each baselin\n","\n","\n","\n","# Dict structure: for each of the 4 targets we build a separate data struct (make helper). We want to make this hashable by the arguments...\n","# We want to dind whichset of arguments has the best results so we want to be able to sort all the keys....\n","# Maybe this should be invereted, some rmse -> args that made it....\n","# take the highest 5 for each type and look at those....\n","\n","\n","\n","#There are also 6. different tpyes of LDP mechanisms. How do we sort though that? Treat each mechanisms differentyl.\n","# So for each one of these dicts there are 6 subdicts, one for each meechanism. Then we can expand as we please.... \n","# That means unforling the results Also they aren't necessarily ordered. There are lists that just have an associated list with the label for the position. Defineitely better to just make hella dictionaries......\n","\n","\n","COMPLETE_LEGEND = list(chain(*[generate_legend_baseline(snake_to_readable(x)) for x in LDP_MECHANISMS]))\n","COMPLETE_LEGEND = finalize_legend(COMPLETE_LEGEND)\n","# print(COMPLETE_LEGEND)\n","\n","\n","def build_target_metric_results_dict(\n","    pickle_data_dict: object,\n","    target_stat_string: str,\n","\n","):\n","    print(f\"finding: {target_stat_string}\")\n","\n","    #target stat strign to index helper?\n","\n","    mechanism_level_dict = {mechanism: {} for mechanism in COMPLETE_LEGEND} # 6 subdicts, one for each mech\n","    # RMSE -> ARGS for each mechanism\n","\n","    #search for best RMSE helper. We have an inverted struct, we have\n","    #Pickle for arg\n","    # mechanisms\n","    # we want to search over all of these and build the max RMSE per mechanism so we need\n","    # iterate over,ddon't even sort them just record all the RMSE per a mechanism\n","    # target_ind = [\"partition_weighted_rmse\", \"weighted_rmse\", \"variance\"]\n","    target_ind = [\"partition_weighted_rmse\"]\n","\n","    for mechanism in COMPLETE_LEGEND:\n","        old_dict = mechanism_level_dict[mechanism]\n","        for path, p in pickle_data_dict.items():\n","            # print(p)\n","\n","            [get_average_vals(p, [x]) for x in target_ind] # note you forgot about the baseliens... there are actually baselines for each one of these arg runs....\n","            #TODO expand the mechanism level dict to include the baselines too...\n","\n","\n","\n","\n","\n","\n","build_target_metric_results_dict(pickle_data_dict, \"RMSE\")\n","\n","\n","#Getting the metric sttaes\n","\n","\n","\n","#weighted RMSE\n","\n","\n","\n","\n","\n","#variance\n","\n","\n","\n","\n","\n","#partition weighted RMSE\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Test IDLDP\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","from RR_RAPPOR import RAPPOR\n","import numpy as np\n","from IDLDP.pg1 import IDLDP\n","\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","k = 1000 #absz\n","n = 10000\n","elements = range(0,k) #ab\n","lbd = 0.8 #parameter for geometric dist\n","eps = 1 # privacy_para\n","prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n","#prob = [1/float(k)] * k\n","in_list = np.random.choice(elements, n, p=prob) #input symbols\n","\n","\n","# idldp params\n","privacy_budget = [1, 1.2, 2]\n","tier_split_percentages = [.05, .05, .9]\n","domain_size = k\n","total_records = n\n","opt_mode = 1\n","idldp = IDLDP()\n","\n","\n","\n","fig, ax = plt.subplots(nrows=5, ncols=1) \n","fig.set_size_inches(8.5, 10)\n","\n","\n","for opt_mode in range(5):\n","  print(f\"checking opt mode {opt_mode}\\n\\n\")\n","  X, pred_MSE, config = idldp.gen_perturbation_probs(\n","    eps,\n","    privacy_budget,\n","    tier_split_percentages=tier_split_percentages,\n","    domain_size=k,\n","    total_records = n,\n","    opt_mode = opt_mode,\n","  )\n","\n","\n","  rappor = RAPPOR(k,eps, config=config)\n","  out_list = rappor.id_ldp_perturb(in_list, **config)\n","  outp,temp = np.histogram(out_list,range(k+1))\n","  prob_est = rappor.decode_counts(outp,n) # estimate the original underlying distribution\n","  print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n","  print (\"prob_sum: \", str(sum(prob_est)))\n","  prob_est = rappor.decode_counts(outp,n,1) # estimate the original underlying distribution\n","  ax[opt_mode].plot(elements,prob)\n","  ax[opt_mode].plot(elements,prob_est)\n","  #plt.plot(prob_est)\n","  print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n","  print (\"prob_sum: \", str(sum(prob_est)))\n","  # plt.pause(0.1)\n","# plt.close()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}